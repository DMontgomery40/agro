{"id":0,"text":"from fastapi import FastAPI, Query, HTTPException\nfrom pydantic import BaseModel\nfrom typing import Optional, Dict, Any, List\nfrom pathlib import Path\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.responses import FileResponse, JSONResponse\nfrom langgraph_app import build_graph\nfrom hybrid_search import search_routed_multi\nfrom config_loader import load_repos\nfrom index_stats import get_index_stats as _get_index_stats\nimport os, json\nfrom typing import Any, Dict\n\napp = FastAPI(title=\"AGRO RAG + GUI\")\n\n_graph = Noneget_graph():\n    global _graph\n    if _graph is None:\n        _graph = build_graph()\n    return _graph\n\nCFG = {\"configurable\": {\"thread_id\": \"http\"}}"}
{"id":1,"text":"Answer(BaseModel):\n    answer: str\n\nROOT = Path(__file__).resolve().parent\nGUI_DIR = ROOT / \"gui\"\nDOCS_DIR = ROOT / \"docs\"\n\n# Serve static GUI assets\nif GUI_DIR.exists():\n    app.mount(\"/gui\", StaticFiles(directory=str(GUI_DIR), html=True), name=\"gui\")\n\n# Serve local docs and repo files for in-GUI links\nif DOCS_DIR.exists():\n    app.mount(\"/docs\", StaticFiles(directory=str(DOCS_DIR), html=True), name=\"docs\")\napp.mount(\"/files\", StaticFiles(directory=str(ROOT), html=True), name=\"files\")\n\n@app.get(\"/\", include_in_schema=False)serve_index():\n    idx = GUI_DIR / \"index.html\"\n    if idx.exists():\n        return FileResponse(str(idx))\n    return {\"ok\": True, \"message\": \"GUI assets not found; use /health, /search, /answer\"}\n\n@app.get(\"/health\")health():\n    try:\n        g = get_graph()\n        return {\"status\": \"healthy\", \"graph_loaded\": g is not None, \"ts\": __import__('datetime').datetime.utcnow().isoformat() + 'Z'}\n    except Exception as e:\n        return {\"status\": \"error\", \"detail\": str(e)}\n\n@app.get(\"/answer\", response_model=Answer)"}
{"id":2,"text":"answer(\n    q: str = Query(..., description=\"Question\"),\n    repo: Optional[str] = Query(None, description=\"Repository override: project|project\")\n):\n    \"\"\"Answer a question using strict per-repo routing.\n\n    If `repo` is provided, retrieval and the answer header will use that repo.\n    Otherwise, a lightweight router selects the repo from the query content.\n    \"\"\"\n    g = get_graph()\n    state = {\"question\": q, \"documents\": [], \"generation\":\"\", \"iteration\":0, \"confidence\":0.0, \"repo\": (repo.strip() if repo else None)}\n    res = g.invoke(state, CFG)\n    return {\"answer\": res[\"generation\"]}\n\n@app.get(\"/search\")"}
{"id":3,"text":"search(\n    q: str = Query(..., description=\"Question\"),\n    repo: Optional[str] = Query(None, description=\"Repository override: project|project\"),\n    top_k: int = Query(10, description=\"Number of results to return\")\n):\n    \"\"\"Search for relevant code locations without generation.\n\n    Returns file paths, line ranges, and rerank scores for the most relevant code chunks.\n    \"\"\"\n    docs = search_routed_multi(q, repo_override=repo, m=4, final_k=top_k)\n    results = [\n        {\n            \"file_path\": d.get(\"file_path\", \"\"),\n            \"start_line\": d.get(\"start_line\", 0),\n            \"end_line\": d.get(\"end_line\", 0),\n            \"language\": d.get(\"language\", \"\"),\n            \"rerank_score\": float(d.get(\"rerank_score\", 0.0) or 0.0),\n            \"repo\": d.get(\"repo\", repo),\n        }\n        for d in docs\n    ]\n    return {\"results\": results, \"repo\": repo, \"count\": len(results)}\n\n# ---------------- Minimal GUI API stubs ----------------_read_json(path: Path, default: Any) -> Any:\n    if path.exists():\n        try:\n            return json.loads(path.read_text())\n        except Exception:\n            return default\n    return default"}
{"id":4,"text":"_write_json(path: Path, data: Any) -> None:\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(json.dumps(data, indent=2))\n\n# ---- Prices helper for auto-profile"}
{"id":5,"text":"_default_prices() -> Dict[str, Any]:\n    return {\n        \"last_updated\": \"2025-10-10\",\n        \"currency\": \"USD\",\n        \"models\": [\n            {\"provider\": \"openai\", \"family\": \"gpt-4o-mini\", \"model\": \"gpt-4o-mini\",\n             \"unit\": \"1k_tokens\", \"input_per_1k\": 0.005, \"output_per_1k\": 0.015,\n             \"embed_per_1k\": 0.0001, \"rerank_per_1k\": 0.0, \"notes\": \"EXAMPLE\"},\n            {\"provider\": \"cohere\", \"family\": \"rerank-english-v3.0\", \"model\": \"rerank-english-v3.0\",\n             \"unit\": \"1k_tokens\", \"input_per_1k\": 0.0, \"output_per_1k\": 0.0,\n             \"embed_per_1k\": 0.0, \"rerank_per_1k\": 0.30, \"notes\": \"EXAMPLE\"},\n            {\"provider\": \"voyage\", \"family\": \"voyage-3-large\", \"model\": \"voyage-3-large\",\n             \"unit\": \"1k_tokens\", \"input_per_1k\": 0.0, \"output_per_1k\": 0.0,\n             \"embed_per_1k\": 0.12, \"rerank_per_1k\": 0.0, \"notes\": \"EXAMPLE\"},\n            {\"provider\": \"local\", \"family\": \"qwen3-coder\", \"model\": \"qwen3-coder:14b\",\n             \"unit\": \"request\", \"per_request\": 0.0, \"notes\": \"Local inference assumed $0; electricity optional\"}\n        ]\n    }"}
{"id":6,"text":"_read_prices() -> Dict[str, Any]:\n    data = _read_json(GUI_DIR / \"prices.json\", {\"models\": []})\n    if not data or not isinstance(data, dict) or not data.get(\"models\"):\n        return _default_prices()\n    return data\n\n@app.post(\"/api/env/reload\")api_env_reload() -> Dict[str, Any]:\n    try:\n        from dotenv import load_dotenv as _ld\n        _ld(override=False)\n    except Exception:\n        pass\n    return {\"ok\": True}\n\n@app.get(\"/api/config\")get_config() -> Dict[str, Any]:\n    cfg = load_repos()\n    # return a broad env snapshot for the GUI; rely on client to pick what it needs\n    env: Dict[str, Any] = {}\n    for k, v in os.environ.items():\n        # keep it simple; include strings only\n        env[k] = v\n    repos = cfg.get(\"repos\", [])\n    return {\n        \"env\": env,\n        \"default_repo\": cfg.get(\"default_repo\"),\n        \"repos\": repos,\n    }\n\n@app.post(\"/api/config\")"}
{"id":7,"text":"set_config(payload: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    Persist environment variables and repos.json edits coming from the GUI.\n\n    Shape: { env: {KEY: VALUE, ...}, repos: [{name, path, keywords, path_boosts, layer_bonuses}, ...] }\n\n    - Writes env keys to .env in repo root (idempotent upsert)\n    - Writes repos to repos.json\n    - Also applies env to current process so the running server reflects changes immediately\n    \"\"\"\n    root = ROOT\n    env_updates: Dict[str, Any] = dict(payload.get(\"env\") or {})\n    repos_updates: List[Dict[str, Any]] = list(payload.get(\"repos\") or [])\n\n    # 1) Upsert .env\n    env_path = root / \".env\"\n    existing: Dict[str, str] = {}\n    if env_path.exists():\n        for line in env_path.read_text().splitlines():\n            if not line.strip() or line.strip().startswith(\"#\") or \"=\" not in line:\n                continue\n            k, v = line.split(\"=\", 1)\n            existing[k.strip()] = v.strip()\n    for k, v in env_updates.items():\n        existing[str(k)] = str(v)\n        os.environ[str(k)] = str(v)\n    # Write back\n    lines = [f\"{k}={existing[k]}\" for k in sorted(existing.keys())]\n    env_path.write_text(\"\\n\".join(lines) + \"\\n\")\n\n    # 2) Upsert repos.json\n    repos_path = root / \"repos.json\"\n    cfg = _read_json(repos_path, {\"default_repo\": None, \"repos\": []})\n    # Keep default_repo if provided in env\n    default_repo = env_updates.get(\"REPO\") or cfg.get(\"default_repo\")\n    # Merge repos by name\n    by_name: Dict[str, Dict[str, Any]] = {str(r.get(\"name\")): r for r in cfg.get(\"repos\", []) if r.get(\"name\")}\n    for r in repos_updates:\n        name = str(r.get(\"name\") or \"\").strip()\n        if not name:\n            continue\n        cur = by_name.get(name, {\"name\": name})\n        # Only accept expected keys\n        if \"path\" in r:\n            cur[\"path\"] = r[\"path\"]\n        if \"keywords\" in r and isinstance(r[\"keywords\"], list):\n            cur[\"keywords\"] = [str(x) for x in r[\"keywords\"]]\n        if \"path_boosts\" in r and isinstance(r[\"path_boosts\"], list):\n            cur[\"path_boosts\"] = [str(x) for x in r[\"path_boosts\"]]\n        if \"layer_bonuses\" in r and isinstance(r[\"layer_bonuses\"], dict):\n            cur[\"layer_bonuses\"] = r[\"layer_bonuses\"]\n        by_name[name] = cur\n    new_cfg = {\n        \"default_repo\": default_repo,\n        \"repos\": sorted(by_name.values(), key=lambda x: str(x.get(\"name\")))\n    }\n    _write_json(repos_path, new_cfg)\n\n    return {\"status\": \"success\", \"applied_env_keys\": sorted(existing.keys()), \"repos_count\": len(new_cfg[\"repos\"]) }\n\n@app.get(\"/api/prices\")"}
{"id":8,"text":"get_prices():\n    prices_path = GUI_DIR / \"prices.json\"\n    data = _read_json(prices_path, _default_prices())\n    return JSONResponse(data)\n\n@app.post(\"/api/prices/upsert\")upsert_price(item: Dict[str, Any]) -> Dict[str, Any]:\n    prices_path = GUI_DIR / \"prices.json\"\n    data = _read_json(prices_path, {\"models\": []})\n    models: List[Dict[str, Any]] = list(data.get(\"models\", []))\n    key = (str(item.get(\"provider\")), str(item.get(\"model\")))\n    idx = next((i for i, m in enumerate(models) if (str(m.get(\"provider\")), str(m.get(\"model\"))) == key), None)\n    if idx is None:\n        models.append(item)\n    else:\n        models[idx].update(item)\n    data[\"models\"] = models\n    data[\"last_updated\"] = __import__('datetime').datetime.utcnow().strftime('%Y-%m-%d')\n    _write_json(prices_path, data)\n    return {\"ok\": True, \"count\": len(models)}\n\n@app.get(\"/api/keywords\")"}
{"id":9,"text":"get_keywords() -> Dict[str, Any]:\n    def extract_terms(obj: Any) -> List[str]:\n        out: List[str] = []\n        try:\n            if isinstance(obj, list):\n                for it in obj:\n                    if isinstance(it, str):\n                        out.append(it)\n                    elif isinstance(it, dict):\n                        # common shapes\n                        for key in (\"keyword\", \"term\", \"key\", \"name\"):\n                            if key in it and isinstance(it[key], str):\n                                out.append(it[key])\n                                break\n            elif isinstance(obj, dict):\n                # prefer \"agro\" or \"project\" buckets, else flatten all lists\n                for bucket in (\"agro\", \"project\"):\n                    if bucket in obj and isinstance(obj[bucket], list):\n                        out.extend(extract_terms(obj[bucket]))\n                        return out\n                for v in obj.values():\n                    out.extend(extract_terms(v))\n        except Exception:\n            pass\n        return out\n    discr_raw = _read_json(ROOT / \"discriminative_keywords.json\", {})\n    sema_raw = _read_json(ROOT / \"semantic_keywords.json\", {})\n    discr = extract_terms(discr_raw)\n    sema = extract_terms(sema_raw)\n    repos_cfg = load_repos()\n    repo_k = []\n    for r in repos_cfg.get(\"repos\", []):\n        for k in r.get(\"keywords\", []) or []:\n            if isinstance(k, str):\n                repo_k.append(k)\n    def uniq(xs: List[str]) -> List[str]:\n        seen = set(); out: List[str] = []\n        for k in xs:\n            k2 = str(k)\n            if k2 not in seen:\n                out.append(k2); seen.add(k2)\n        return out\n    discr = uniq(discr)\n    sema = uniq(sema)\n    repo_k = uniq(repo_k)\n    allk = uniq((discr or []) + (sema or []) + (repo_k or []))\n    return {\"discriminative\": discr, \"semantic\": sema, \"repos\": repo_k, \"keywords\": allk}\n\n@app.post(\"/api/scan-hw\")"}
{"id":10,"text":"scan_hw() -> Dict[str, Any]:\n    # Lightweight local scan without new deps\n    import platform, shutil\n    info = {\n        \"os\": platform.system(),\n        \"arch\": platform.machine(),\n        \"cpu_cores\": os.cpu_count() or 0,\n        \"mem_gb\": None,\n    }\n    # Try to get memory (Darwin via sysctl; Linux via /proc/meminfo)\n    try:\n        if info[\"os\"] == \"Darwin\":\n            import subprocess\n            out = subprocess.check_output([\"sysctl\", \"-n\", \"hw.memsize\"], text=True).strip()\n            info[\"mem_gb\"] = round(int(out) / (1024**3), 2)\n        elif Path(\"/proc/meminfo\").exists():\n            txt = Path(\"/proc/meminfo\").read_text()\n            for line in txt.splitlines():\n                if line.startswith(\"MemTotal:\"):\n                    kb = int(line.split()[1]); info[\"mem_gb\"] = round(kb/1024/1024, 2)\n                    break\n    except Exception:\n        pass\n    runtimes = {\n        \"ollama\": bool(os.getenv(\"OLLAMA_URL\") or shutil.which(\"ollama\")),\n        \"coreml\": info[\"os\"] == \"Darwin\",\n        \"cuda\": bool(shutil.which(\"nvidia-smi\")),\n        \"mps\": info[\"os\"] == \"Darwin\",\n    }\n    tools = {\"uvicorn\": bool(shutil.which(\"uvicorn\")), \"docker\": bool(shutil.which(\"docker\"))}\n    return {\"info\": info, \"runtimes\": runtimes, \"tools\": tools}"}
{"id":11,"text":"_find_price(provider: str, model: Optional[str]) -> Optional[Dict[str, Any]]:\n    data = _read_json(GUI_DIR / \"prices.json\", {\"models\": []})\n    models = data.get(\"models\", [])\n    # Prefer exact provider+model, else fallback to first matching provider\n    for m in models:\n        if m.get(\"provider\") == provider and (model is None or m.get(\"model\") == model):\n            return m\n    for m in models:\n        if m.get(\"provider\") == provider:\n            return m\n    return None"}
{"id":12,"text":"_estimate_cost(gen_provider: str, gen_model: Optional[str], tokens_in: int, tokens_out: int, embeds: int, reranks: int, requests_per_day: int,\n                   embed_provider: Optional[str] = None, embed_model: Optional[str] = None,\n                   rerank_provider: Optional[str] = None, rerank_model: Optional[str] = None) -> Dict[str, Any]:\n    # Generation\n    price_gen = _find_price(gen_provider, gen_model)\n    if not price_gen:\n        price_gen = {\"input_per_1k\": 0.0, \"output_per_1k\": 0.0, \"per_request\": 0.0}\n    per_1k_in = float(price_gen.get(\"input_per_1k\", 0.0))\n    per_1k_out = float(price_gen.get(\"output_per_1k\", 0.0))\n    per_req = float(price_gen.get(\"per_request\", 0.0))\n    daily = 0.0\n    daily += (tokens_in/1000.0) * per_1k_in * max(1, requests_per_day)\n    daily += (tokens_out/1000.0) * per_1k_out * max(1, requests_per_day)\n    daily += per_req * max(1, requests_per_day)\n\n    # Embeddings (separate provider/model)\n    if embeds > 0:\n        if embed_provider is None and gen_provider == 'openai':\n            embed_provider, embed_model = 'openai', (embed_model or 'text-embedding-3-small')\n        price_emb = _find_price(embed_provider or gen_provider, embed_model)\n        if price_emb:\n            daily += (embeds/1000.0) * float(price_emb.get(\"embed_per_1k\", 0.0))\n\n    # Rerank\n    if reranks > 0:\n        price_rr = _find_price(rerank_provider or 'cohere', rerank_model or 'rerank-3.5')\n        if price_rr:\n            daily += (reranks/1000.0) * float(price_rr.get(\"rerank_per_1k\", 0.0))\n\n    breakdown = {\n        \"generation\": price_gen,\n        \"embeddings\": _find_price(embed_provider or gen_provider, embed_model) if embeds>0 else None,\n        \"rerank\": _find_price(rerank_provider or 'cohere', rerank_model or 'rerank-3.5') if reranks>0 else None,\n    }\n    return {\"daily\": round(daily, 6), \"monthly\": round(daily*30.0, 4), \"breakdown\": breakdown}\n\n@app.post(\"/api/cost/estimate\")"}
{"id":13,"text":"cost_estimate(payload: Dict[str, Any]) -> Dict[str, Any]:\n    gen_provider = str(payload.get(\"gen_provider\") or payload.get(\"provider\") or \"openai\")\n    gen_model = payload.get(\"gen_model\")\n    tokens_in = int(payload.get(\"tokens_in\") or 0)\n    tokens_out = int(payload.get(\"tokens_out\") or 0)\n    embeds = int(payload.get(\"embeds\") or 0)\n    reranks = int(payload.get(\"reranks\") or 0)\n    rpd = int(payload.get(\"requests_per_day\") or 0)\n    emb_prov = payload.get(\"embed_provider\")\n    emb_model = payload.get(\"embed_model\")\n    rr_prov = payload.get(\"rerank_provider\")\n    rr_model = payload.get(\"rerank_model\")\n    return _estimate_cost(gen_provider, gen_model, tokens_in, tokens_out, embeds, reranks, rpd,\n                          embed_provider=emb_prov, embed_model=emb_model,\n                          rerank_provider=rr_prov, rerank_model=rr_model)\n\n@app.post(\"/api/cost/estimate_pipeline\")cost_estimate_pipeline(payload: Dict[str, Any]) -> Dict[str, Any]:\n    # same shape as estimate(), kept for compatibility\n    return cost_estimate(payload)\n\n@app.get(\"/api/profiles\")"}
{"id":14,"text":"profiles_list() -> Dict[str, Any]:\n    prof_dir = GUI_DIR / \"profiles\"\n    prof_dir.mkdir(parents=True, exist_ok=True)\n    names = []\n    for p in prof_dir.glob(\"*.json\"):\n        names.append(p.stem)\n    return {\"profiles\": sorted(names), \"default\": None}\n\n@app.post(\"/api/profiles/save\")profiles_save(payload: Dict[str, Any]) -> Dict[str, Any]:\n    name = str(payload.get(\"name\") or \"\").strip()\n    prof = payload.get(\"profile\") or {}\n    if not name:\n        raise HTTPException(status_code=400, detail=\"missing name\")\n    path = GUI_DIR / \"profiles\" / f\"{name}.json\"\n    _write_json(path, prof)\n    return {\"ok\": True, \"name\": name}\n\n@app.post(\"/api/profiles/apply\")profiles_apply(payload: Dict[str, Any]) -> Dict[str, Any]:\n    prof = payload.get(\"profile\") or {}\n    applied = []\n    for k, v in prof.items():\n        os.environ[str(k)] = str(v)\n        applied.append(str(k))\n    return {\"ok\": True, \"applied_keys\": applied}\n\n# --- Auto-profile v2\ntry:\n    from autoprofile import autoprofile as _ap_select\nexcept Exception:\n    _ap_select = None\n\n@app.post(\"/api/profile/autoselect\")"}
{"id":15,"text":"api_profile_autoselect(payload: Dict[str, Any]):\n    if _ap_select is None:\n        raise HTTPException(status_code=500, detail=\"autoprofile module not available\")\n    prices = _read_prices()\n    env, reason = _ap_select(payload, prices)\n    if not env:\n        raise HTTPException(status_code=422, detail=reason)\n    return {\"env\": env, \"reason\": reason}\n\n@app.post(\"/api/checkpoint/config\")checkpoint_config() -> Dict[str, Any]:\n    \"\"\"Write a timestamped checkpoint of current env + repos to gui/profiles.\"\"\"\n    cfg = get_config()\n    from datetime import datetime\n    ts = datetime.utcnow().strftime('%Y%m%d-%H%M%S')\n    out_dir = GUI_DIR / \"profiles\"\n    out_dir.mkdir(parents=True, exist_ok=True)\n    path = out_dir / f\"checkpoint-{ts}.json\"\n    _write_json(path, cfg)\n    return {\"ok\": True, \"path\": str(path)}\n\n# --- Index + Cards: comprehensive status with all metrics ---\n_INDEX_STATUS: List[str] = []\n_INDEX_METADATA: Dict[str, Any] = {}\n\n@app.post(\"/api/index/start\")"}
{"id":16,"text":"index_start() -> Dict[str, Any]:\n    \"\"\"Start indexing with real subprocess execution.\"\"\"\n    global _INDEX_STATUS, _INDEX_METADATA\n    import subprocess\n    import threading\n\n    _INDEX_STATUS = [\"Indexing started...\"]\n    _INDEX_METADATA = {}\n\n    def run_index():\n        global _INDEX_STATUS, _INDEX_METADATA\n        try:\n            repo = os.getenv(\"REPO\", \"agro\")\n            _INDEX_STATUS.append(f\"Indexing repository: {repo}\")\n\n            # Run the actual indexer\n            result = subprocess.run(\n                [\"python\", \"index_repo.py\"],\n                capture_output=True,\n                text=True,\n                cwd=ROOT,\n                env={**os.environ, \"REPO\": repo}\n            )\n\n            if result.returncode == 0:\n                _INDEX_STATUS.append(\"✓ Indexing completed successfully\")\n                _INDEX_METADATA = _get_index_stats()\n            else:\n                _INDEX_STATUS.append(f\"✗ Indexing failed: {result.stderr[:200]}\")\n        except Exception as e:\n            _INDEX_STATUS.append(f\"✗ Error: {str(e)}\")\n\n    # Run in background\n    thread = threading.Thread(target=run_index, daemon=True)\n    thread.start()\n\n    return {\"ok\": True, \"message\": \"Indexing started in background\"}\n\n@app.get(\"/api/index/status\")"}
{"id":17,"text":"index_status() -> Dict[str, Any]:\n    \"\"\"Return comprehensive indexing status with all metrics.\"\"\"\n    if not _INDEX_METADATA:\n        # Return basic status if no metadata yet\n        return {\n            \"lines\": _INDEX_STATUS,\n            \"running\": len(_INDEX_STATUS) > 0 and not any(\"completed\" in s or \"failed\" in s for s in _INDEX_STATUS),\n            \"metadata\": _get_index_stats()  # Always provide current stats\n        }\n\n    return {\n        \"lines\": _INDEX_STATUS,\n        \"running\": False,\n        \"metadata\": _INDEX_METADATA\n    }\n\n@app.post(\"/api/cards/build\")cards_build() -> Dict[str, Any]:\n    return {\"ok\": True}\n\n@app.get(\"/api/cards\")cards_list() -> Dict[str, Any]:\n    return {\"cards\": []}\n\n# ---------------- Git hooks helpers ----------------"}
{"id":18,"text":"_git_hooks_dir() -> Path:\n    # repo root assumed to be same as this file's parent\n    root = ROOT\n    return root / \".git\" / \"hooks\"\n\n_HOOK_POST_CHECKOUT = \"\"\"#!/usr/bin/env bash\n# Auto-index on branch changes when AUTO_INDEX=1\n[ \"${AUTO_INDEX:-0}\" != \"1\" ] && exit 0\nrepo_root=\"$(git rev-parse --show-toplevel)\"\ncd \"$repo_root\" || exit 0\nif [ -d .venv ]; then . .venv/bin/activate; fi\nexport REPO=agro EMBEDDING_TYPE=local SKIP_DENSE=1\nexport OUT_DIR_BASE=\"./out.noindex-shared\"\npython index_repo.py >/dev/null 2>&1 || true\n\"\"\"\n\n_HOOK_POST_COMMIT = \"\"\"#!/usr/bin/env bash\n# Auto-index on commit when AUTO_INDEX=1\n[ \"${AUTO_INDEX:-0}\" != \"1\" ] && exit 0\nrepo_root=\"$(git rev-parse --show-toplevel)\"\ncd \"$repo_root\" || exit 0\nif [ -d .venv ]; then . .venv/bin/activate; fi\nexport REPO=agro EMBEDDING_TYPE=local SKIP_DENSE=1\nexport OUT_DIR_BASE=\"./out.noindex-shared\"\npython index_repo.py >/dev/null 2>&1 || true\n\"\"\"\n\n@app.get(\"/api/git/hooks/status\")"}
{"id":19,"text":"git_hooks_status() -> Dict[str, Any]:\n    d = _git_hooks_dir()\n    pc = d / \"post-checkout\"\n    pm = d / \"post-commit\"\n    return {\n        \"dir\": str(d),\n        \"post_checkout\": pc.exists(),\n        \"post_commit\": pm.exists(),\n        \"enabled_hint\": \"export AUTO_INDEX=1\"\n    }\n\n@app.post(\"/api/git/hooks/install\")git_hooks_install() -> Dict[str, Any]:\n    d = _git_hooks_dir()\n    try:\n        d.mkdir(parents=True, exist_ok=True)\n        pc = d / \"post-checkout\"\n        pm = d / \"post-commit\"\n        pc.write_text(_HOOK_POST_CHECKOUT)\n        pm.write_text(_HOOK_POST_COMMIT)\n        os.chmod(pc, 0o755)\n        os.chmod(pm, 0o755)\n        return {\"ok\": True, \"message\": \"Installed git hooks. Enable with: export AUTO_INDEX=1\"}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))"}
{"id":20,"text":"import math\nimport os\nfrom typing import List, Dict\nfrom rerankers import Reranker\nfrom typing import Optional\n\n# Load .env early so config reads below see the right values\ntry:\n    from dotenv import load_dotenv\n    load_dotenv(override=False)\nexcept Exception:\n    pass\n\n_HF_PIPE = None  # optional transformers pipeline for models that require trust_remote_code\n\n_RERANKER = None\n\n# Default to a strong open-source cross-encoder; allow env override\nDEFAULT_MODEL = os.getenv('RERANKER_MODEL', 'BAAI/bge-reranker-v2-m3')\n# Default backend: local (set RERANK_BACKEND=cohere + COHERE_API_KEY to use Cohere API)\nRERANK_BACKEND = (os.getenv('RERANK_BACKEND', 'local') or 'local').lower()\n# Default Cohere model (override via COHERE_RERANK_MODEL). Accepts 'rerank-3.5' or 'rerank-2.5'.\nCOHERE_MODEL = os.getenv('COHERE_RERANK_MODEL', 'rerank-3.5')\n\n_sigmoid(x: float) -> float:\n    try:\n        return 1.0 / (1.0 + math.exp(-float(x)))\n    except Exception:\n        return 0.0"}
{"id":21,"text":"_normalize(score: float, model_name: str) -> float:\n    if any(k in model_name.lower() for k in ['bge-reranker', 'cross-encoder', 'mxbai', 'jina-reranker']):\n        return _sigmoid(score)\n    return float(score)\n\n_maybe_init_hf_pipeline(model_name: str) -> Optional[object]:\n    global _HF_PIPE\n    if _HF_PIPE is not None:\n        return _HF_PIPE\n    try:\n        if 'jinaai/jina-reranker' in model_name.lower():\n            # Use HF pipeline directly to guarantee trust_remote_code is honored\n            os.environ.setdefault('TRANSFORMERS_TRUST_REMOTE_CODE', '1')\n            from transformers import pipeline\n            _HF_PIPE = pipeline(\n                task='text-classification',\n                model=model_name,\n                tokenizer=model_name,\n                trust_remote_code=True,\n                device_map='auto'\n            )\n            return _HF_PIPE\n    except Exception:\n        _HF_PIPE = None\n    return _HF_PIPE"}
{"id":22,"text":"get_reranker() -> Reranker:\n    global _RERANKER\n    if _RERANKER is None:\n        model_name = DEFAULT_MODEL\n        # First try a direct HF pipeline for models with custom code\n        if _maybe_init_hf_pipeline(model_name):\n            return None  # Signal to use HF pipeline path\n        # Otherwise, fall back to rerankers\n        os.environ.setdefault('TRANSFORMERS_TRUST_REMOTE_CODE', '1')\n        _RERANKER = Reranker(model_name, model_type='cross-encoder', trust_remote_code=True)\n    return _RERANKER"}
{"id":23,"text":"rerank_results(query: str, results: List[Dict], top_k: int = 10) -> List[Dict]:\n    if not results:\n        return []\n    # Optional hard disable for environments without model/network\n    if RERANK_BACKEND in ('none', 'off', 'disabled'):\n        # Assign neutral scores based on original order\n        for i, r in enumerate(results):\n            r['rerank_score'] = float(1.0 - (i * 0.01))\n        return results[:top_k]\n    model_name = DEFAULT_MODEL\n    # Optional Cohere backend (remote API)\n    if RERANK_BACKEND == 'cohere':\n        try:\n            import cohere  # type: ignore\n            api_key = os.getenv('COHERE_API_KEY')\n            if not api_key:\n                raise RuntimeError('COHERE_API_KEY not set')\n            client = cohere.Client(api_key=api_key)\n            docs = []\n            for r in results:\n                file_ctx = r.get('file_path', '')\n                code_snip = (r.get('code') or r.get('text') or '')[:700]\n                docs.append(f\"{file_ctx}\\n\\n{code_snip}\")\n            rr = client.rerank(model=COHERE_MODEL, query=query, documents=docs, top_n=len(docs))\n            # Normalize scores into 0..1\n            scores = [getattr(x, 'relevance_score', 0.0) for x in rr.results]\n            max_s = max(scores) if scores else 1.0\n            for item in rr.results:\n                idx = int(getattr(item, 'index', 0))\n                score = float(getattr(item, 'relevance_score', 0.0))\n                results[idx]['rerank_score'] = (score / max_s) if max_s else 0.0\n            results.sort(key=lambda x: x.get('rerank_score', 0.0), reverse=True)\n            return results[:top_k]\n        except Exception:\n            # Fall back to local reranker paths below\n            pass\n    # HF pipeline path (e.g., Jina reranker)\n    pipe = _maybe_init_hf_pipeline(model_name)\n    if pipe is not None:\n        pairs = []\n        for r in results:\n            code_snip = (r.get('code') or r.get('text') or '')[:700]\n            pairs.append({'text': query, 'text_pair': code_snip})\n        try:\n            out = pipe(pairs, truncation=True)\n            raw = []\n            for i, o in enumerate(out):\n                score = float(o.get('score', 0.0))\n                s = _normalize(score, model_name)\n                results[i]['rerank_score'] = s\n                raw.append(s)\n            # Calibrate scores to 0..1 range per request\n            if raw:\n                mn, mx = min(raw), max(raw)\n                rng = (mx - mn)\n                if rng > 1e-9:\n                    for r in results:\n                        r['rerank_score'] = (float(r.get('rerank_score', 0.0)) - mn) / rng\n                elif mx != 0.0:\n                    for r in results:\n                        r['rerank_score'] = float(r.get('rerank_score', 0.0)) / abs(mx)\n            results.sort(key=lambda x: x.get('rerank_score', 0.0), reverse=True)\n            return results[:top_k]\n        except Exception:\n            # Fall back to rerankers path below\n            pass\n    # rerankers path\n    docs = []\n    for r in results:\n        file_ctx = r.get('file_path', '')\n        code_snip = (r.get('code') or r.get('text') or '')[:600]\n        docs.append(f\"{file_ctx}\\n\\n{code_snip}\")\n    rr = get_reranker()\n    if rr is None and _maybe_init_hf_pipeline(model_name) is not None:\n        # HF pipeline already used above; should not reach here\n        return results[:top_k]\n    ranked = rr.rank(query=query, docs=docs, doc_ids=list(range(len(docs))))\n    raw_scores = []\n    for res in ranked.results:\n        idx = res.document.doc_id\n        s = _normalize(res.score, model_name)\n        results[idx]['rerank_score'] = s\n        raw_scores.append(s)\n    # Calibrate to [0,1] per call for stable gating across backends\n    if raw_scores:\n        mn, mx = min(raw_scores), max(raw_scores)\n        rng = (mx - mn)\n        if rng > 1e-9:\n            for r in results:\n                r['rerank_score'] = (float(r.get('rerank_score', 0.0)) - mn) / rng\n        elif mx != 0.0:\n            for r in results:\n                r['rerank_score'] = float(r.get('rerank_score', 0.0)) / abs(mx)\n    results.sort(key=lambda x: x.get('rerank_score', 0.0), reverse=True)\n    return results[:top_k]"}
{"id":24,"text":"import os\nimport json\nfrom pathlib import Path\nfrom typing import Any, Dict, Optional, Tuple, List\n\nfrom config_loader import layer_bonuses as _layer_bonuses_cfg, path_boosts as _path_boosts_cfg\n\n\n_OVERRIDES: Dict[str, Any] | None = None\n\n_overrides_path() -> Path:\n    # Keep with UI assets for simplicity\n    return Path(__file__).parent / \"ui\" / \"runtime_overrides.json\"\n\n_load_overrides() -> Dict[str, Any]:\n    global _OVERRIDES\n    if _OVERRIDES is not None:\n        return _OVERRIDES\n    p = _overrides_path()\n    if p.exists():\n        try:\n            _OVERRIDES = json.loads(p.read_text())\n        except Exception:\n            _OVERRIDES = {}\n    else:\n        _OVERRIDES = {}\n    return _OVERRIDES\n\n_get_override(repo: Optional[str], key: str) -> Any:\n    ov = _load_overrides()\n    # Precedence: per-repo -> _global\n    if repo:\n        rp = ov.get(repo)\n        if isinstance(rp, dict) and key in rp:\n            return rp[key]\n    g = ov.get(\"_global\")\n    if isinstance(g, dict) and key in g:\n        return g[key]\n    return None"}
{"id":25,"text":"_coerce(value: Any, typ: str) -> Any:\n    if value is None:\n        return None\n    try:\n        if typ == \"int\":\n            return int(value)\n        if typ == \"float\":\n            return float(value)\n        if typ == \"bool\":\n            if isinstance(value, bool):\n                return value\n            s = str(value).strip().lower()\n            return s in {\"1\", \"true\", \"yes\", \"on\"}\n        if typ == \"str\":\n            return str(value)\n        if typ == \"list[str]\":\n            if isinstance(value, list):\n                return [str(x) for x in value]\n            return [x.strip() for x in str(value).split(',') if x.strip()]\n    except Exception:\n        return None\n    return value\n\nget_str(repo: Optional[str], key: str, env_key: Optional[str] = None, default: Optional[str] = None) -> Optional[str]:\n    v = _get_override(repo, key)\n    if v is None and env_key:\n        v = os.getenv(env_key)\n    return _coerce(v if v is not None else default, \"str\")\n\nget_int(repo: Optional[str], key: str, env_default: Optional[int] = None, default: Optional[int] = None) -> int:\n    v = _get_override(repo, key)\n    if v is None:\n        v = env_default\n    v = default if v is None else v\n    out = _coerce(v, \"int\")\n    return int(out) if out is not None else int(default or 0)"}
{"id":26,"text":"get_float(repo: Optional[str], key: str, env_default: Optional[float] = None, default: Optional[float] = None) -> float:\n    v = _get_override(repo, key)\n    if v is None:\n        v = env_default\n    v = default if v is None else v\n    out = _coerce(v, \"float\")\n    return float(out) if out is not None else float(default or 0.0)\n\nget_bool(repo: Optional[str], key: str, env_default: Optional[bool] = None, default: Optional[bool] = None) -> bool:\n    v = _get_override(repo, key)\n    if v is None:\n        v = env_default\n    v = default if v is None else v\n    out = _coerce(v, \"bool\")\n    return bool(out) if out is not None else bool(default or False)\n\n\n# High-level helpers\nget_conf_thresholds(repo: Optional[str]) -> Tuple[float, float, float]:\n    t1 = get_float(repo, \"CONF_TOP1\", float(os.getenv(\"CONF_TOP1\", \"0.62\")), 0.62)\n    a5 = get_float(repo, \"CONF_AVG5\", float(os.getenv(\"CONF_AVG5\", \"0.55\")), 0.55)\n    anyc = get_float(repo, \"CONF_ANY\", float(os.getenv(\"CONF_ANY\", \"0.55\")), 0.55)\n    return t1, a5, anyc"}
{"id":27,"text":"get_topk(repo: Optional[str]) -> Tuple[int, int, int]:\n    kd = get_int(repo, \"TOPK_DENSE\", int(os.getenv(\"TOPK_DENSE\", \"75\") or 75), 75)\n    ks = get_int(repo, \"TOPK_SPARSE\", int(os.getenv(\"TOPK_SPARSE\", \"75\") or 75), 75)\n    fk = get_int(repo, \"FINAL_K\", int(os.getenv(\"FINAL_K\", \"10\") or 10), 10)\n    return kd, ks, fk\n\nget_mq_rewrites(repo: Optional[str]) -> int:\n    return get_int(repo, \"MQ_REWRITES\", int(os.getenv(\"MQ_REWRITES\", \"2\") or 2), 2)\n\nget_reranker_config(repo: Optional[str]) -> Dict[str, str]:\n    return {\n        \"backend\": (get_str(repo, \"RERANK_BACKEND\", \"RERANK_BACKEND\", \"local\") or \"local\").lower(),\n        \"model\": get_str(repo, \"RERANKER_MODEL\", \"RERANKER_MODEL\", \"BAAI/bge-reranker-v2-m3\"),\n        \"cohere_model\": get_str(repo, \"COHERE_RERANK_MODEL\", \"COHERE_RERANK_MODEL\", \"rerank-3.5\"),\n    }"}
{"id":28,"text":"get_path_boosts(repo: Optional[str]) -> List[str]:\n    # Use repos.json, with optional env override per-repo (e.g., PROJECT_PATH_BOOSTS)\n    lst = _path_boosts_cfg(repo or \"\")\n    env_key = f\"{(repo or '').upper()}_PATH_BOOSTS\" if repo else None\n    if env_key:\n        env_val = os.getenv(env_key)\n        if not env_val and (repo or \"\").lower() == \"project\":\n            env_val = os.getenv(\"project_PATH_BOOSTS\")\n        if env_val:\n            lst.extend([t.strip() for t in env_val.split(',') if t.strip()])\n    # De-dup while preserving order\n    seen = set(); out = []\n    for t in lst:\n        tl = t.strip().lower()\n        if tl and tl not in seen:\n            seen.add(tl); out.append(tl)\n    return out\n\nget_layer_bonuses(repo: Optional[str]) -> Dict[str, Dict[str, float]]:\n    return _layer_bonuses_cfg(repo or \"\")"}
{"id":29,"text":"import os\nimport json\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Tuple\n\n\n_CACHE: Dict[str, Any] = {}\n\n_repos_file_path() -> Path:\n    # Allow override via env; default to repos.json in repo root\n    env_path = os.getenv(\"REPOS_FILE\")\n    if env_path:\n        return Path(env_path).expanduser().resolve()\n    return Path(__file__).resolve().parent / \"repos.json\""}
{"id":30,"text":"load_repos() -> Dict[str, Any]:\n    \"\"\"\n    Load repository configuration.\n\n    Format:\n      {\n        \"default_repo\": \"example-1\",\n        \"repos\": [\n          {\"name\": \"example-1\", \"path\": \"/abs/path/example-1\",\n           \"keywords\": [\"auth\", \"backend\"],\n           \"path_boosts\": [\"src/server/\", \"api/\"],\n           \"layer_bonuses\": {\"server\": {\"kernel\": 0.10}, \"ui\": {\"ui\": 0.12}}\n          },\n          {\"name\": \"example-2\", \"path\": [\"/abs/path/example-2\", \"/abs/alt\"]}\n        ]\n      }\n\n    Fallbacks (if repos.json missing):\n      - If REPO and REPO_PATH env set, synthesize a single-repo config.\n      - Else raise a clear error when a function requires config.\n    \"\"\"\n    global _CACHE\n    if \"config\" in _CACHE:\n        return _CACHE[\"config\"]\n\n    p = _repos_file_path()\n    if p.exists():\n        try:\n            data = json.loads(p.read_text())\n            if isinstance(data, dict) and isinstance(data.get(\"repos\"), list):\n                _CACHE[\"config\"] = data\n                return data\n        except Exception:\n            pass\n\n    # Fallback to environment-only single repo\n    env_repo = (os.getenv(\"REPO\") or \"default\").strip()\n    env_path = os.getenv(\"REPO_PATH\") or os.getenv(f\"REPO_{env_repo.upper()}_PATH\")\n    if env_path:\n        cfg = {\n            \"default_repo\": env_repo,\n            \"repos\": [\n                {\"name\": env_repo, \"path\": env_path}\n            ]\n        }\n        _CACHE[\"config\"] = cfg\n        return cfg\n\n    # Last resort minimal placeholder (no repos) to allow help text rendering\n    cfg = {\"default_repo\": None, \"repos\": []}\n    _CACHE[\"config\"] = cfg\n    return cfg"}
{"id":31,"text":"list_repos() -> List[str]:\n    cfg = load_repos()\n    return [str(r.get(\"name\")) for r in cfg.get(\"repos\", []) if r.get(\"name\")]\n\nget_default_repo() -> str:\n    cfg = load_repos()\n    # 1) explicit default\n    if cfg.get(\"default_repo\"):\n        return str(cfg[\"default_repo\"]).strip()\n    # 2) first repo\n    repos = cfg.get(\"repos\", [])\n    if repos:\n        return str(repos[0].get(\"name\"))\n    # 3) env REPO or sentinel\n    return (os.getenv(\"REPO\") or \"default\").strip()\n\n_find_repo(name: str) -> Optional[Dict[str, Any]]:\n    name_low = (name or \"\").strip().lower()\n    if not name_low:\n        return None\n    for r in load_repos().get(\"repos\", []):\n        if (r.get(\"name\") or \"\").strip().lower() == name_low:\n            return r\n    return None\n\nget_repo_paths(name: str) -> List[str]:\n    r = _find_repo(name)\n    if not r:\n        raise ValueError(f\"Unknown repo: {name}. Known: {', '.join(list_repos()) or '[]'}\")\n    p = r.get(\"path\")\n    if isinstance(p, list):\n        return [str(Path(x).expanduser()) for x in p]\n    if isinstance(p, str):\n        return [str(Path(p).expanduser())]\n    raise ValueError(f\"Repo `{name}` missing 'path' in repos.json\")"}
{"id":32,"text":"_out_base_dir() -> Path:\n    \"\"\"Resolve base output directory for indices.\n\n    Order of precedence:\n      1) ENV OUT_DIR_BASE or RAG_OUT_BASE (absolute or relative to repo)\n      2) ./out.noindex-shared (cross-branch shared)\n      3) ./out.noindex-gui\n      4) ./out.noindex-devclean\n      5) ./out.noindex\n      6) ./out\n    \"\"\"\n    root = Path(__file__).resolve().parent\n    env_base = os.getenv(\"OUT_DIR_BASE\") or os.getenv(\"RAG_OUT_BASE\")\n    if env_base:\n        p = Path(env_base).expanduser()\n        if not p.is_absolute():\n            p = (root / p)\n        return p\n    for cand in (\"out.noindex-shared\", \"out.noindex-gui\", \"out.noindex-devclean\", \"out.noindex\"):\n        if (root / cand).exists():\n            return root / cand\n    return root / \"out\"\n\nout_dir(name: str) -> str:\n    base = _out_base_dir() / name\n    return str(base)\n\nget_repo_keywords(name: str) -> List[str]:\n    r = _find_repo(name)\n    if not r:\n        return []\n    kws = r.get(\"keywords\") or []\n    return [str(k).lower() for k in kws if isinstance(k, str)]"}
{"id":33,"text":"path_boosts(name: str) -> List[str]:\n    r = _find_repo(name)\n    if not r:\n        return []\n    lst = r.get(\"path_boosts\") or []\n    return [str(x) for x in lst if isinstance(x, str)]\n\nlayer_bonuses(name: str) -> Dict[str, Dict[str, float]]:\n    r = _find_repo(name)\n    if not r:\n        return {}\n    lb = r.get(\"layer_bonuses\") or {}\n    # Normalize numeric values\n    out: Dict[str, Dict[str, float]] = {}\n    for intent, d in (lb.items() if isinstance(lb, dict) else []):\n        if not isinstance(d, dict):\n            continue\n        out[intent] = {k: float(v) for k, v in d.items() if isinstance(v, (int, float))}\n    return out"}
{"id":34,"text":"choose_repo_from_query(query: str, default: Optional[str] = None) -> str:\n    q = (query or \"\").lower().strip()\n    # Allow explicit prefix: \"<name>: question\"\n    if \":\" in q:\n        cand, _ = q.split(\":\", 1)\n        cand = cand.strip()\n        if cand in [r.lower() for r in list_repos()]:\n            return cand\n    # Keyword voting across repos\n    best = None\n    best_hits = 0\n    for name in list_repos():\n        hits = 0\n        for kw in get_repo_keywords(name):\n            if kw and kw in q:\n                hits += 1\n        if hits > best_hits:\n            best = name\n            best_hits = hits\n    if best:\n        return best\n    return (default or get_default_repo())"}
{"id":35,"text":"import os\nfrom dotenv import load_dotenv\n\nload_dotenv('env/project.env')\nfrom serve_rag import app\n\nif __name__ == '__main__':\n    import uvicorn\n    os.environ['COLLECTION_NAME'] = os.environ.get('COLLECTION_NAME', f\"{os.environ['REPO']}_{os.environ.get('COLLECTION_SUFFIX','default')}\")\n    port = int(os.environ.get('PORT', '8014'))\n    uvicorn.run(app, host='127.0.0.1', port=port)"}
{"id":36,"text":"import os\nimport json\nimport collections\nfrom typing import List, Dict\nfrom pathlib import Path\nfrom config_loader import choose_repo_from_query, get_default_repo, out_dir\nfrom dotenv import load_dotenv, find_dotenv\n# Load any existing env ASAP so downstream imports (e.g., rerank backend) see them\ntry:\n    load_dotenv(override=False)\nexcept Exception:\n    pass\nfrom qdrant_client import QdrantClient, models\nimport bm25s\nfrom bm25s.tokenization import Tokenizer\nfrom Stemmer import Stemmer\nfrom rerank import rerank_results as ce_rerank\nfrom env_model import generate_text\n\n# Query intent → layer preferences"}
{"id":37,"text":"_classify_query(q:str)->str:\n    ql=(q or '').lower()\n    if any(k in ql for k in ['ui','react','component','tsx','page','frontend','render','css']):\n        return 'ui'\n    if any(k in ql for k in ['notification','pushover','apprise','hubspot','provider','integration','adapter','webhook']):\n        return 'integration'\n    if any(k in ql for k in ['diagnostic','health','event log','phi','mask','hipaa','middleware','auth','token','oauth','hmac']):\n        return 'server'\n    if any(k in ql for k in ['sdk','client library','python sdk','node sdk']):\n        return 'sdk'\n    if any(k in ql for k in ['infra','asterisk','sip','t.38','ami','freeswitch','egress','cloudflared']):\n        return 'infra'\n    return 'server'"}
{"id":38,"text":"_project_layer_bonus(layer:str,intent:str)->float:\n    layer_lower=(layer or '').lower()\n    intent_lower=(intent or 'server').lower()\n    table={'server':{'kernel':0.10,'plugin':0.04,'ui':0.00,'docs':0.00,'tests':0.00,'infra':0.02},\n           'integration':{'integration':0.12,'kernel':0.04,'ui':0.00,'docs':0.00,'tests':0.00,'infra':0.00},\n           'ui':{'ui':0.12,'docs':0.06,'kernel':0.02,'plugin':0.02,'tests':0.00,'infra':0.00},\n           'sdk':{'kernel':0.04,'docs':0.02},\n           'infra':{'infra':0.12,'kernel':0.04}}\n    return table.get(intent_lower,{}).get(layer_lower,0.0)"}
{"id":39,"text":"_project_layer_bonus(layer:str,intent:str)->float:\n    layer_lower=(layer or '').lower()\n    intent_lower=(intent or 'server').lower()\n    table={'server':{'server':0.10,'integration':0.06,'fax':0.30,'admin console':0.10,'sdk':0.00,'infra':0.00,'docs':0.02},\n           'integration':{'provider':0.12,'traits':0.10,'server':0.06,'ui':0.00,'sdk':0.00,'infra':0.02,'docs':0.00},\n           'ui':{'ui':0.12,'docs':0.06,'server':0.02,'hipaa':0.20},\n           'sdk':{'sdk':0.12,'server':0.04,'docs':0.02},\n           'infra':{'infra':0.12,'server':0.04,'provider':0.04}}\n    return table.get(intent_lower,{}).get(layer_lower,0.0)\n_provider_plugin_hint(fp:str, code:str)->float:\n    fp=(fp or '').lower()\n    code=(code or '').lower()\n    keys=['provider','providers','integration','adapter','webhook','pushover','apprise','hubspot']\n    return 0.06 if any(k in fp or k in code for k in keys) else 0.0"}
{"id":40,"text":"_origin_bonus(origin:str, mode:str)->float:\n    origin = (origin or '').lower()\n    mode=(mode or 'prefer_first_party').lower()\n    if mode == 'prefer_first_party':\n        return 0.06 if origin=='first_party' else (-0.08 if origin=='vendor' else 0.0)\n    if mode == 'prefer_vendor':\n        return 0.06 if origin=='vendor' else 0.0\n    return 0.0\n_feature_bonus(query:str, fp:str, code:str)->float:\n    ql = (query or '').lower()\n    fp = (fp or '').lower()\n    code=(code or '').lower()\n    bumps = 0.0\n    if any(k in ql for k in ['diagnostic','health','event log','phi','hipaa']):\n        if ('diagnostic' in fp) or ('diagnostic' in code) or ('event' in fp and 'log' in fp):\n            bumps += 0.06\n    return bumps\n_card_bonus(chunk_id: str, card_chunk_ids: set) -> float:\n    \"\"\"Boost chunks that matched via card-based retrieval.\"\"\"\n    return 0.08 if str(chunk_id) in card_chunk_ids else 0.0\n\n# Path-aware bonus to tilt results toward likely server/auth code"}
{"id":41,"text":"_path_bonus(fp: str) -> float:\n    fp = (fp or '').lower()\n    bonus = 0.0\n    for sfx, b in [\n        ('/identity/', 0.12),\n        ('/auth/', 0.12),\n        ('/server', 0.10),\n        ('/backend', 0.10),\n        ('/api/', 0.08),\n    ]:\n        if sfx in fp:\n            bonus += b\n    return bonus\n\n# Additional PROJECT-only path boosts (env-tunable)"}
{"id":42,"text":"_project_path_boost(fp: str, repo_tag: str) -> float:\n    import os as _os\n    if (repo_tag or '').lower() != 'project':\n        return 0.0\n    cfg = _os.getenv('project_PATH_BOOSTS', 'app/,lib/,config/,scripts/,server/,api/,api/app,app/services,app/routers,api/admin_ui,app/plugins')\n    tokens = [t.strip().lower() for t in cfg.split(',') if t.strip()]\n    s = (fp or '').lower()\n    bonus = 0.0\n    for tok in tokens:\n        if tok and tok in s:\n            bonus += 0.06\n    return min(bonus, 0.18)\n\n# Load environment from repo root .env without hard-coded paths\ntry:\n    load_dotenv(override=False)\n    repo_root = Path(__file__).resolve().parent\n    env_path = repo_root / \".env\"\n    if env_path.exists():\n        load_dotenv(dotenv_path=env_path, override=False)\n    else:\n        alt = find_dotenv(usecwd=True)\n        if alt:\n            load_dotenv(dotenv_path=alt, override=False)\nexcept Exception:\n    pass\nQDRANT_URL = os.getenv('QDRANT_URL','http://127.0.0.1:6333')\nREPO = os.getenv('REPO','project')\nVENDOR_MODE = os.getenv('VENDOR_MODE','prefer_first_party')\n# Allow explicit collection override (for versioned collections per embedding config)\nCOLLECTION = os.getenv('COLLECTION_NAME', f'code_chunks_{REPO}')\n\n# --- Embeddings provider (openai | voyage | local) ---"}
{"id":43,"text":"_lazy_import_openai():\n    from openai import OpenAI\n    return OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n_lazy_import_voyage():\n    import voyageai\n    return voyageai.Client(api_key=os.getenv(\"VOYAGE_API_KEY\"))\n\n_local_embed_model = None"}
{"id":44,"text":"_get_embedding(text: str, kind: str = \"query\") -> list[float]:\n    et = (os.getenv(\"EMBEDDING_TYPE\", \"openai\") or \"openai\").lower()\n    if et == \"voyage\":\n        vo = _lazy_import_voyage()\n        out = vo.embed([text], model=\"voyage-code-3\", input_type=kind, output_dimension=512)\n        return out.embeddings[0]\n    if et == \"local\":\n        global _local_embed_model\n        if _local_embed_model is None:\n            from sentence_transformers import SentenceTransformer\n            _local_embed_model = SentenceTransformer('BAAI/bge-small-en-v1.5')\n        # Normalize embeddings for cosine distance\n        return _local_embed_model.encode([text], normalize_embeddings=True, show_progress_bar=False)[0].tolist()\n    # default openai\n    client = _lazy_import_openai()\n    resp = client.embeddings.create(input=text, model=\"text-embedding-3-large\")\n    return resp.data[0].embedding"}
{"id":45,"text":"rrf(\n    dense: list,\n    sparse: list,\n    k: int = 10,\n    kdiv: int = 60\n) -> list:\n    \"\"\"\n    Reciprocal Rank Fusion (RRF) for combining dense and sparse retrieval results.\n\n    Args:\n        dense (List): Ranked list of IDs from dense retrieval.\n        sparse (List): Ranked list of IDs from sparse retrieval.\n        k (int, optional): Number of top results to return. Defaults to 10.\n        kdiv (int, optional): RRF constant to dampen rank impact. Defaults to 60.\n\n    Returns:\n        List: Top-k fused IDs by RRF score.\n    \"\"\"\n    score: dict = collections.defaultdict(float)\n    for rank, pid in enumerate(dense, start=1):\n        score[pid] += 1.0 / (kdiv + rank)\n    for rank, pid in enumerate(sparse, start=1):\n        score[pid] += 1.0 / (kdiv + rank)\n    ranked = sorted(score.items(), key=lambda x: x[1], reverse=True)\n    return [pid for pid, _ in ranked[:k]]"}
{"id":46,"text":"_load_chunks(repo: str) -> List[Dict]:\n    \"\"\"Load minimal chunk metadata (omit code to reduce memory).\"\"\"\n    p = os.path.join(out_dir(repo), 'chunks.jsonl')\n    chunks: List[Dict] = []\n    if os.path.exists(p):\n        with open(p, 'r', encoding='utf-8') as f:\n            for line in f:\n                try:\n                    o = json.loads(line)\n                except Exception:\n                    continue\n                # Drop bulky fields to keep memory bounded\n                o.pop('code', None)\n                o.pop('summary', None)\n                o.pop('keywords', None)\n                chunks.append(o)\n    return chunks\n_load_bm25_map(idx_dir: str):\n    # Prefer point IDs (UUID strings) aligned with Qdrant\n    pid_json = os.path.join(idx_dir, 'bm25_point_ids.json')\n    if os.path.exists(pid_json):\n        m = json.load(open(pid_json))\n        return [m[str(i)] for i in range(len(m))]\n    # Fallback to chunk_ids.txt (string chunk IDs)\n    map_path = os.path.join(idx_dir, 'chunk_ids.txt')\n    if os.path.exists(map_path):\n        with open(map_path, 'r', encoding='utf-8') as f:\n            ids = [line.strip() for line in f if line.strip()]\n        return ids\n    return None"}
{"id":47,"text":"_load_cards_bm25(repo: str):\n    idx_dir = os.path.join(out_dir(repo), 'bm25_cards')\n    try:\n        import bm25s\n        retr = bm25s.BM25.load(idx_dir)\n        return retr\n    except Exception:\n        return None\n_load_cards_map(repo: str) -> Dict:\n    \"\"\"Load cards to get chunk ID mapping. Returns dict with card index -> chunk_id and chunk_id -> card data.\"\"\"\n    cards_file = os.path.join(out_dir(repo), 'cards.jsonl')\n    cards_by_idx = {}  # card corpus index -> chunk_id\n    cards_by_chunk_id = {}  # chunk_id -> card metadata\n    try:\n        with open(cards_file, 'r', encoding='utf-8') as f:\n            for idx, line in enumerate(f):\n                card = json.loads(line)\n                chunk_id = str(card.get('id', ''))\n                if chunk_id:\n                    cards_by_idx[idx] = chunk_id\n                    cards_by_chunk_id[chunk_id] = card\n        return {'by_idx': cards_by_idx, 'by_chunk_id': cards_by_chunk_id}\n    except Exception:\n        return {'by_idx': {}, 'by_chunk_id': {}}"}
{"id":48,"text":"search(query: str, repo: str, topk_dense: int = 75, topk_sparse: int = 75, final_k: int = 10) -> List[Dict]:\n    chunks = _load_chunks(repo)\n    if not chunks:\n        return []\n\n    # ---- Dense (Qdrant) ----\n    dense_pairs = []\n    qc = QdrantClient(url=QDRANT_URL)\n    coll = os.getenv('COLLECTION_NAME', f'code_chunks_{repo}')\n    try:\n        e = _get_embedding(query, kind=\"query\")\n    except Exception:\n        e = []\n    try:\n        dres = qc.query_points(\n            collection_name=coll,\n            query=e,\n            using='dense',\n            limit=topk_dense,\n            with_payload=models.PayloadSelectorInclude(include=['file_path','start_line','end_line','language','layer','repo','hash','id'])\n        )\n        points = getattr(dres, 'points', dres)\n        dense_pairs = [(str(p.id), dict(p.payload)) for p in points]  # type: ignore\n    except Exception:\n        dense_pairs = []\n\n    # ---- Sparse (BM25S) ----\n    idx_dir = os.path.join(out_dir(repo), 'bm25_index')\n    retriever = bm25s.BM25.load(idx_dir)\n    tokenizer = Tokenizer(stemmer=Stemmer('english'), stopwords='en')\n    tokens = tokenizer.tokenize([query])\n    ids, _ = retriever.retrieve(tokens, k=topk_sparse)\n    # ids shaped (1, k)\n    ids = ids.tolist()[0] if hasattr(ids, 'tolist') else list(ids[0])\n    id_map = _load_bm25_map(idx_dir)\n    by_chunk_id = {str(c['id']): c for c in chunks}\n    sparse_pairs = []\n    for i in ids:\n        if id_map is not None:\n            if 0 <= i < len(id_map):\n                pid_or_cid = id_map[i]\n                key = str(pid_or_cid)\n                if key in by_chunk_id:\n                    # id_map contained chunk id\n                    sparse_pairs.append((key, by_chunk_id[key]))\n                else:\n                    # Fallback to corpus order alignment\n                    if 0 <= i < len(chunks):\n                        sparse_pairs.append((str(chunks[i]['id']), chunks[i]))\n        else:\n            # fallback to corpus order alignment\n            if 0 <= i < len(chunks):\n                sparse_pairs.append((str(chunks[i]['id']), chunks[i]))\n\n    # Card-based BM25 boosting: retrieve cards and boost matching chunks\n    card_chunk_ids: set = set()\n    cards_retr = _load_cards_bm25(repo)\n    if cards_retr is not None:\n        try:\n            cards_map = _load_cards_map(repo)\n            tokens = tokenizer.tokenize([query])\n            c_ids, _ = cards_retr.retrieve(tokens, k=min(topk_sparse, 30))\n            # Map card indices to chunk IDs\n            c_ids_flat = c_ids[0] if hasattr(c_ids, '__getitem__') else c_ids\n            for card_idx in c_ids_flat:\n                chunk_id = cards_map['by_idx'].get(int(card_idx))\n                if chunk_id:\n                    card_chunk_ids.add(str(chunk_id))\n        except Exception:\n            pass\n\n    # Fuse\n    dense_ids = [pid for pid,_ in dense_pairs]\n    sparse_ids = [pid for pid,_ in sparse_pairs]\n    fused = rrf(dense_ids, sparse_ids, k=max(final_k, 2*final_k)) if dense_pairs else sparse_ids[:final_k]\n    by_id = {pid: p for pid,p in (dense_pairs + sparse_pairs)}\n    docs = [by_id[pid] for pid in fused if pid in by_id]\n    # Hydrate code bodies with a low-memory strategy (lazy, on-demand)\n    HYDRATION_MODE = (os.getenv('HYDRATION_MODE','lazy') or 'lazy').lower()\n    if HYDRATION_MODE != 'none':\n        _hydrate_docs_inplace(repo, docs)\n    docs = ce_rerank(query, docs, top_k=final_k)\n    # Apply path + layer intent + provider + feature + card + (optional) origin bonuses, then resort\n    intent = _classify_query(query)\n    for d in docs:\n        layer_bonus = _project_layer_bonus(d.get('layer',''), intent) if repo=='project' else _project_layer_bonus(d.get('layer',''), intent)\n        origin_bonus = _origin_bonus(d.get('origin',''), VENDOR_MODE) if 'VENDOR_MODE' in os.environ else 0.0\n        repo_tag = d.get('repo', repo)\n        chunk_id = str(d.get('id', ''))\n        d['rerank_score'] = float(\n            d.get('rerank_score', 0.0)\n            + _path_bonus(d.get('file_path', ''))\n            + _project_path_boost(d.get('file_path',''), repo_tag)\n            + layer_bonus\n            + _provider_plugin_hint(d.get('file_path', ''), d.get('code', '')[:1000])\n            + _feature_bonus(query, d.get('file_path',''), d.get('code','')[:800])\n            + _card_bonus(chunk_id, card_chunk_ids)\n            + origin_bonus\n        )\n    docs.sort(key=lambda x: x.get('rerank_score', 0.0), reverse=True)\n    return docs[:final_k]\n\n# Local code cache to hydrate code bodies from chunks.jsonl instead of Qdrant payloads\n_code_cache_by_repo: dict[str, dict] = {}"}
{"id":49,"text":"_load_code_cache(repo: str):\n    import json\n    if repo in _code_cache_by_repo:\n        return _code_cache_by_repo[repo]\n    jl = os.path.join(out_dir(repo), 'chunks.jsonl')\n    cache: dict[str, dict[str, str]] = {'by_hash': {}, 'by_id': {}}\n    try:\n        with open(jl, 'r', encoding='utf-8') as f:\n            for line in f:\n                try:\n                    o = json.loads(line)\n                except Exception:\n                    continue\n                h = o.get('hash')\n                cid = str(o.get('id', ''))\n                code = o.get('code', '')\n                if h:\n                    cache['by_hash'][h] = code\n                if cid:\n                    cache['by_id'][cid] = code\n    except FileNotFoundError:\n        pass\n    _code_cache_by_repo[repo] = cache\n    return cache"}
{"id":50,"text":"_hydrate_docs_inplace(repo: str, docs: List[Dict]) -> None:\n    \"\"\"Fill missing code for the selected docs by streaming chunks.jsonl once.\n\n    Avoids loading the entire repo into memory. Honors HYDRATION_MAX_CHARS to cap snippet size.\n    \"\"\"\n    needed_ids: set[str] = set()\n    needed_hashes: set[str] = set()\n    for d in docs:\n        if d.get('code'):\n            continue\n        cid = str(d.get('id','') or '')\n        h = d.get('hash')\n        if cid:\n            needed_ids.add(cid)\n        if h:\n            needed_hashes.add(h)\n    if not needed_ids and not needed_hashes:\n        return\n    jl = os.path.join(out_dir(repo), 'chunks.jsonl')\n    max_chars = int(os.getenv('HYDRATION_MAX_CHARS', '2000') or '2000')\n    found_by_id: dict[str, str] = {}\n    found_by_hash: dict[str, str] = {}\n    try:\n        with open(jl, 'r', encoding='utf-8') as f:\n            for line in f:\n                try:\n                    o = json.loads(line)\n                except Exception:\n                    continue\n                cid = str(o.get('id','') or '')\n                h = o.get('hash')\n                code = (o.get('code') or '')\n                if max_chars > 0 and code:\n                    code = code[:max_chars]\n                if cid and cid in needed_ids and cid not in found_by_id:\n                    found_by_id[cid] = code\n                if h and h in needed_hashes and h not in found_by_hash:\n                    found_by_hash[h] = code\n                if len(found_by_id) >= len(needed_ids) and len(found_by_hash) >= len(needed_hashes):\n                    break\n    except FileNotFoundError:\n        return\n    for d in docs:\n        if not d.get('code'):\n            cid = str(d.get('id','') or '')\n            h = d.get('hash')\n            d['code'] = found_by_id.get(cid) or (found_by_hash.get(h) if h else '') or ''\n\n# --- filename/path boosts applied post-rerank ---"}
{"id":51,"text":"_apply_filename_boosts(docs: list[dict], question: str) -> None:\n    terms = set((question or '').lower().replace('/', ' ').replace('-', ' ').split())\n    for d in docs:\n        fp = (d.get('file_path') or '').lower()\n        fn = os.path.basename(fp)\n        parts = fp.split('/')\n        score = float(d.get('rerank_score', 0.0) or 0.0)\n        if any(t and t in fn for t in terms):\n            score *= 1.5\n        if any(t and t in p for t in terms for p in parts):\n            score *= 1.2\n        d['rerank_score'] = score\n    docs.sort(key=lambda x: x.get('rerank_score', 0.0), reverse=True)\n\n# --- Strict per-repo routing helpers (no fusion) ---"}
{"id":52,"text":"route_repo(query: str, default_repo: str | None = None) -> str:\n    \"\"\"Route to a repo using repos.json config and lightweight prefixing.\n\n    - Supports explicit prefix: \"<name>: question\"\n    - Falls back to keyword voting as configured in repos.json\n    - Defaults to configured default_repo (repos.json) or env REPO\n    \"\"\"\n    try:\n        # Prefer config-driven choice (handles prefixes + keywords)\n        return choose_repo_from_query(query, default=(default_repo or get_default_repo()))\n    except Exception:\n        # Very safe fallback\n        q = (query or '').lower().strip()\n        if ':' in q:\n            cand, _ = q.split(':', 1)\n            cand = cand.strip()\n            if cand:\n                return cand\n        return (default_repo or os.getenv('REPO', 'project') or 'project').strip()\nsearch_routed(query: str, repo_override: str | None = None, final_k: int = 10):\n    repo = (repo_override or route_repo(query, default_repo=os.getenv('REPO', 'project')) or os.getenv('REPO', 'project')).strip()\n    return search(query, repo=repo, final_k=final_k)\n\n# Multi-query expansion (cheap) and routed search"}
{"id":53,"text":"expand_queries(query: str, m: int = 4) -> list[str]:\n    # Fast path: no expansion requested\n    if m <= 1:\n        return [query]\n    try:\n        sys = \"Rewrite a developer query into multiple search-friendly variants without changing meaning.\"\n        user = f\"Count: {m}\\nQuery: {query}\\nOutput one variant per line, no numbering.\"\n        text, _ = generate_text(user_input=user, system_instructions=sys, reasoning_effort=None)\n        lines = [ln.strip('- ').strip() for ln in (text or '').splitlines() if ln.strip()]\n        uniq = []\n        for ln in lines:\n            if ln and ln not in uniq:\n                uniq.append(ln)\n        return (uniq or [query])[:m]\n    except Exception:\n        return [query]"}
{"id":54,"text":"search_routed_multi(query: str, repo_override: str | None = None, m: int = 4, final_k: int = 10):\n    repo = (repo_override or route_repo(query) or os.getenv('REPO','project')).strip()\n    variants = expand_queries(query, m=m)\n    all_docs = []\n    for qv in variants:\n        docs = search(qv, repo=repo, final_k=final_k)\n        all_docs.extend(docs)\n    # Deduplicate by file_path + line span\n    seen = set()\n    uniq = []\n    for d in all_docs:\n        key = (d.get('file_path'), d.get('start_line'), d.get('end_line'))\n        if key in seen:\n            continue\n        seen.add(key)\n        uniq.append(d)\n    # Rerank union\n    try:\n        from rerank import rerank_results as ce_rerank\n        reranked = ce_rerank(query, uniq, top_k=final_k)\n        _apply_filename_boosts(reranked, query)\n        return reranked\n    except Exception:\n        return uniq[:final_k]"}
{"id":55,"text":"#!/usr/bin/env python3\n\"\"\"\nEnterprise Compatibility Watchdog (stub)\n\n- Reads compat_rules.json and prints a summary\n- Intended to be extended with collectors (GitHub issues, release notes) and emit rules/alerts\n\"\"\"\nfrom __future__ import annotations\nimport json\nfrom pathlib import Path\n\nROOT = Path(__file__).resolve().parent\nRULES = ROOT / \"compat_rules.json\"\n\nmain() -> int:\n    if RULES.exists():\n        try:\n            data = json.loads(RULES.read_text())\n        except Exception:\n            data = []\n    else:\n        data = []\n    print(f\"[watchdog] Loaded {len(data)} compat rule(s) from {RULES}\")\n    for i, r in enumerate(data[:10], start=1):\n        print(f\"  {i}. {r.get('id')} — {r.get('message')}\")\n    return 0\n\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())"}
{"id":56,"text":"# coding: utf-8\n\nPRUNE_DIRS = {\n    '.git', '.worktrees', '.venv', 'venv', 'env', '.venv_ci',\n    'node_modules', 'vendor', 'dist', 'build',\n    '.next', '.turbo', '.svelte-kit', 'coverage',\n    'site', '_site', '__pycache__', '.pytest_cache', '.mypy_cache', '.cache'\n}\n\nVALID_EXTS = (\n    '.py', '.ts', '.tsx', '.js', '.jsx', '.go', '.rs', '.java', '.c', '.cpp',\n    '.md', '.mdx', '.yaml', '.yml', '.toml', '.json'\n)\n\nSKIP_EXTS = ('.map', '.pyc', '.ds_store')\n_should_index_file(name):\n    n = name.lower()\n    if n.endswith(SKIP_EXTS):\n        return False\n    return n.endswith(VALID_EXTS)\n_prune_dirs_in_place(dirs):\n    # remove noisy dirs without descending into them\n    dirs[:] = [d for d in dirs if d not in PRUNE_DIRS]"}
{"id":57,"text":"import os\nimport json\nfrom typing import Optional, Dict, Any, Tuple\n\ntry:\n    from openai import OpenAI\nexcept Exception as e:\n    raise RuntimeError(\"openai>=1.x is required for Responses API\") from e\n\n# Model pin (Responses API): default to OpenAI gpt-4o-mini\n# Users can override with GEN_MODEL (e.g., gpt-4.1, o4-mini, gpt-4o)\n# Avoid local defaults; prefer OpenAI to reduce confusion.\n_DEFAULT_MODEL = os.getenv(\"GEN_MODEL\", os.getenv(\"ENRICH_MODEL\", \"gpt-4o-mini\"))\n\n_client = None\n\n# MLX backend (lazy-loaded for Apple Silicon)\n_mlx_model = None\n_mlx_tokenizer = None\n_get_mlx_model():\n    \"\"\"Lazy-load MLX model for Apple Silicon generation\"\"\"\n    global _mlx_model, _mlx_tokenizer\n    if _mlx_model is None:\n        from mlx_lm import load\n        model_name = os.getenv(\"GEN_MODEL\", \"mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit\")\n        _mlx_model, _mlx_tokenizer = load(model_name)\n    return _mlx_model, _mlx_tokenizer"}
{"id":58,"text":"client() -> OpenAI:\n    global _client\n    if _client is None:\n        # Let OPENAI_API_KEY and OPENAI_BASE_URL (if set) drive the target\n        # This allows using OpenAI-hosted models or an OpenAI-compatible server (e.g., vLLM/Ollama proxy)\n        _client = OpenAI()\n    return _client\n\n_extract_text(resp: Any) -> str:\n    # Prefer .output_text if present (library convenience), else parse the structure\n    txt = \"\"\n    if hasattr(resp, \"output_text\") and isinstance(getattr(resp, \"output_text\"), str):\n        txt = resp.output_text\n        if txt:\n            return txt\n    try:\n        # Fallback path: resp.output[0].content[0].text\n        out = getattr(resp, \"output\", None)\n        if out and len(out) > 0:\n            cont = getattr(out[0], \"content\", None)\n            if cont and len(cont) > 0 and hasattr(cont[0], \"text\"):\n                return cont[0].text or \"\"\n    except Exception:\n        pass\n    return txt or \"\""}
{"id":59,"text":"generate_text(\n    user_input: str,\n    *,\n    system_instructions: Optional[str] = None,\n    model: Optional[str] = None,\n    reasoning_effort: Optional[str] = None,  # e.g., \"low\" | \"medium\" | \"high\"\n    response_format: Optional[Dict[str, Any]] = None,  # e.g., {\"type\":\"json_object\"}\n    store: bool = False,\n    previous_response_id: Optional[str] = None,\n    extra: Optional[Dict[str, Any]] = None,\n) -> Tuple[str, Any]:\n    \"\"\"\n    Minimal wrapper over Responses API:\n      - Uses 'instructions' for system prompt and 'input' for user text\n      - Supports response_format (JSON mode / structured)\n      - Leaves tool-calling to upstream if needed (MCP/file_search handled elsewhere)\n    \"\"\"\n    mdl = model or _DEFAULT_MODEL\n    kwargs: Dict[str, Any] = {\n        \"model\": mdl,\n        \"input\": user_input,\n        \"store\": store,\n    }\n    if system_instructions:\n        kwargs[\"instructions\"] = system_instructions\n    if reasoning_effort:\n        kwargs[\"reasoning\"] = {\"effort\": reasoning_effort}\n    if response_format:\n        kwargs[\"response_format\"] = response_format\n    if previous_response_id:\n        kwargs[\"previous_response_id\"] = previous_response_id\n    if extra:\n        kwargs.update(extra)\n\n    # Check for MLX backend (Apple Silicon, highest priority for local models)\n    ENRICH_BACKEND = os.getenv(\"ENRICH_BACKEND\", \"\").lower()\n    is_mlx_model = mdl.startswith(\"mlx-community/\") if mdl else False\n    prefer_mlx = (ENRICH_BACKEND == \"mlx\") or is_mlx_model\n\n    if prefer_mlx:\n        try:\n            from mlx_lm import generate\n            model, tokenizer = _get_mlx_model()\n\n            # Build prompt with system instructions if provided\n            sys_text = (system_instructions or \"\").strip()\n            prompt = (f\"<system>{sys_text}</system>\\n\" if sys_text else \"\") + user_input\n\n            # Generate with MLX\n            text = generate(\n                model,\n                tokenizer,\n                prompt=prompt,\n                max_tokens=2048,  # More tokens for answer generation\n                verbose=False\n            )\n            return text, {\"response\": text, \"backend\": \"mlx\"}\n        except Exception as e:\n            # Fall through to Ollama/OpenAI on MLX failure\n            pass\n\n    # If using a local model server (Ollama-compatible), prefer its API first when configured\n    OLLAMA_URL = os.getenv(\"OLLAMA_URL\")\n    prefer_ollama = bool(OLLAMA_URL)\n    if prefer_ollama:\n        try:\n            import requests, json as _json, time\n            sys_text = (system_instructions or \"\").strip()\n            prompt = (f\"<system>{sys_text}</system>\\n\" if sys_text else \"\") + user_input\n            url = OLLAMA_URL.rstrip(\"/\") + \"/generate\"\n\n            # Retry configuration (production hardening)\n            max_retries = 2\n            chunk_timeout = 60  # 60s per chunk\n            total_timeout = 300  # 5min hard cap\n\n            for attempt in range(max_retries + 1):\n                start_time = time.time()\n                try:\n                    # Prefer streaming to avoid long blocking on large models\n                    with requests.post(url, json={\n                        \"model\": mdl,\n                        \"prompt\": prompt,\n                        \"stream\": True,\n                        \"options\": {\"temperature\": 0.2, \"num_ctx\": 8192},\n                    }, timeout=chunk_timeout, stream=True) as r:\n                        r.raise_for_status()\n                        buf = []\n                        last = None\n                        for line in r.iter_lines(decode_unicode=True):\n                            # Check total timeout\n                            if time.time() - start_time > total_timeout:\n                                # Return partial output on timeout\n                                partial = (\"\".join(buf) or \"\").strip()\n                                if partial:\n                                    return partial + \" [TIMEOUT]\", {\"response\": partial, \"timeout\": True}\n                                break\n\n                            if not line:\n                                continue\n                            try:\n                                obj = _json.loads(line)\n                            except Exception:\n                                continue\n                            if isinstance(obj, dict):\n                                seg = (obj.get(\"response\") or \"\")\n                                if seg:\n                                    buf.append(seg)\n                                last = obj\n                                if obj.get(\"done\") is True:\n                                    break\n\n                        text = (\"\".join(buf) or \"\").strip()\n                        if text:\n                            return text, (last or {\"response\": text})\n\n                    # Fallback to non-stream if streaming returned empty\n                    resp = requests.post(url, json={\n                        \"model\": mdl,\n                        \"prompt\": prompt,\n                        \"stream\": False,\n                        \"options\": {\"temperature\": 0.2, \"num_ctx\": 8192},\n                    }, timeout=total_timeout)\n                    resp.raise_for_status()\n                    data = resp.json()\n                    text = (data.get(\"response\") or \"\").strip()\n                    if text:\n                        return text, data\n\n                except (requests.Timeout, requests.ConnectionError) as e:\n                    # Retry on network errors\n                    if attempt < max_retries:\n                        backoff = 2 ** attempt  # Exponential backoff: 1s, 2s\n                        time.sleep(backoff)\n                        continue\n                    # Last attempt failed, fall through to OpenAI\n                    pass\n                except Exception:\n                    # Other errors, don't retry\n                    break\n        except Exception:\n            pass\n    # Try OpenAI Responses API\n    try:\n        resp = client().responses.create(**kwargs)\n        text = _extract_text(resp)\n        return text, resp\n    except Exception:\n        # Fallback to Chat Completions for OpenAI-compatible servers that don't expose Responses API\n        try:\n            messages = []\n            if system_instructions:\n                messages.append({\"role\": \"system\", \"content\": system_instructions})\n            messages.append({\"role\": \"user\", \"content\": user_input})\n            ckwargs: Dict[str, Any] = {\"model\": mdl, \"messages\": messages}\n            if response_format and isinstance(response_format, dict):\n                ckwargs[\"response_format\"] = response_format\n            cc = client().chat.completions.create(**ckwargs)\n            text = (cc.choices[0].message.content if getattr(cc, \"choices\", []) else \"\") or \"\"\n            return text, cc\n        except Exception as e:\n            raise RuntimeError(f\"Generation failed for model={mdl}: {e}\")"}
{"id":60,"text":"#!/usr/bin/env python3\n\"\"\"\nMinimal eval loop with regression tracking.\n\nUsage:\n  python eval_loop.py                    # Run once\n  python eval_loop.py --watch            # Run on file changes\n  python eval_loop.py --baseline         # Save current results as baseline\n  python eval_loop.py --compare          # Compare against baseline\n\"\"\"\nimport os\nimport sys\nimport json\nimport time\nimport argparse\nfrom pathlib import Path\nfrom typing import Dict, List, Any\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Import eval logic\nfrom eval_rag import main as run_eval, hit, GOLDEN_PATH, USE_MULTI, FINAL_K\nfrom hybrid_search import search_routed, search_routed_multi\n\n\nBASELINE_PATH = os.getenv('BASELINE_PATH', 'eval_baseline.json')"}
{"id":61,"text":"run_eval_with_results() -> Dict[str, Any]:\n    \"\"\"Run eval and return detailed results.\"\"\"\n    if not os.path.exists(GOLDEN_PATH):\n        return {\"error\": f\"No golden file at {GOLDEN_PATH}\"}\n\n    gold = json.load(open(GOLDEN_PATH))\n    total = len(gold)\n    hits_top1 = 0\n    hits_topk = 0\n    results = []\n\n    t0 = time.time()\n    for i, row in enumerate(gold, 1):\n        q = row['q']\n        repo = row.get('repo') or os.getenv('REPO', 'project')\n        expect = row.get('expect_paths') or []\n\n        if USE_MULTI:\n            docs = search_routed_multi(q, repo_override=repo, m=4, final_k=FINAL_K)\n        else:\n            docs = search_routed(q, repo_override=repo, final_k=FINAL_K)\n\n        paths = [d.get('file_path', '') for d in docs]\n        top1_hit = hit(paths[:1], expect) if paths else False\n        topk_hit = hit(paths, expect) if paths else False\n\n        if top1_hit:\n            hits_top1 += 1\n        if topk_hit:\n            hits_topk += 1\n\n        results.append({\n            \"question\": q,\n            \"repo\": repo,\n            \"expect_paths\": expect,\n            \"top1_path\": paths[:1],\n            \"top1_hit\": top1_hit,\n            \"topk_hit\": topk_hit,\n            \"top_paths\": paths[:FINAL_K]\n        })\n\n    dt = time.time() - t0\n\n    return {\n        \"total\": total,\n        \"top1_hits\": hits_top1,\n        \"topk_hits\": hits_topk,\n        \"top1_accuracy\": round(hits_top1 / max(1, total), 3),\n        \"topk_accuracy\": round(hits_topk / max(1, total), 3),\n        \"final_k\": FINAL_K,\n        \"use_multi\": USE_MULTI,\n        \"duration_secs\": round(dt, 2),\n        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n        \"results\": results\n    }"}
{"id":62,"text":"save_baseline(results: Dict[str, Any]):\n    \"\"\"Save current results as baseline.\"\"\"\n    with open(BASELINE_PATH, 'w') as f:\n        json.dump(results, f, indent=2)\n    print(f\"✓ Baseline saved to {BASELINE_PATH}\")"}
{"id":63,"text":"compare_with_baseline(current: Dict[str, Any]):\n    \"\"\"Compare current results with baseline.\"\"\"\n    if not os.path.exists(BASELINE_PATH):\n        print(f\"⚠ No baseline found at {BASELINE_PATH}\")\n        print(f\"  Run with --baseline to create one\")\n        return\n\n    with open(BASELINE_PATH) as f:\n        baseline = json.load(f)\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"REGRESSION CHECK: Current vs Baseline\")\n    print(\"=\"*60)\n\n    curr_top1 = current[\"top1_accuracy\"]\n    base_top1 = baseline[\"top1_accuracy\"]\n    curr_topk = current[\"topk_accuracy\"]\n    base_topk = baseline[\"topk_accuracy\"]\n\n    delta_top1 = curr_top1 - base_top1\n    delta_topk = curr_topk - base_topk\n\n    print(f\"\\nTop-1 Accuracy:\")\n    print(f\"  Baseline: {base_top1:.3f}\")\n    print(f\"  Current:  {curr_top1:.3f}\")\n    print(f\"  Delta:    {delta_top1:+.3f} {'✓' if delta_top1 >= 0 else '✗'}\")\n\n    print(f\"\\nTop-{FINAL_K} Accuracy:\")\n    print(f\"  Baseline: {base_topk:.3f}\")\n    print(f\"  Current:  {curr_topk:.3f}\")\n    print(f\"  Delta:    {delta_topk:+.3f} {'✓' if delta_topk >= 0 else '✗'}\")\n\n    # Check for regressions per-question\n    regressions = []\n    improvements = []\n\n    for i, (curr_res, base_res) in enumerate(zip(current[\"results\"], baseline[\"results\"])):\n        if curr_res[\"question\"] != base_res[\"question\"]:\n            continue  # skip if questions don't align\n\n        if base_res[\"top1_hit\"] and not curr_res[\"top1_hit\"]:\n            regressions.append((i+1, curr_res[\"question\"], curr_res[\"repo\"]))\n        elif not base_res[\"top1_hit\"] and curr_res[\"top1_hit\"]:\n            improvements.append((i+1, curr_res[\"question\"], curr_res[\"repo\"]))\n\n    if regressions:\n        print(f\"\\n⚠ REGRESSIONS ({len(regressions)} questions):\")\n        for idx, q, repo in regressions:\n            print(f\"  [{idx}] {repo}: {q}\")\n\n    if improvements:\n        print(f\"\\n✓ IMPROVEMENTS ({len(improvements)} questions):\")\n        for idx, q, repo in improvements:\n            print(f\"  [{idx}] {repo}: {q}\")\n\n    if not regressions and delta_top1 >= -0.05 and delta_topk >= -0.05:\n        print(\"\\n✓ No significant regressions detected\")\n        return True\n    else:\n        print(\"\\n✗ Regressions detected!\")\n        return False"}
{"id":64,"text":"watch_mode():\n    \"\"\"Watch for file changes and re-run eval.\"\"\"\n    print(\"⏱ Watch mode: monitoring for changes...\")\n    print(f\"   Watching: {GOLDEN_PATH}, hybrid_search.py, langgraph_app.py\")\n\n    files_to_watch = [\n        GOLDEN_PATH,\n        \"hybrid_search.py\",\n        \"langgraph_app.py\",\n        \"index_repo.py\",\n        \"rerank.py\"\n    ]\n\n    last_mtimes = {}\n    for fp in files_to_watch:\n        if os.path.exists(fp):\n            last_mtimes[fp] = os.path.getmtime(fp)\n\n    while True:\n        time.sleep(5)\n        changed = False\n        for fp in files_to_watch:\n            if not os.path.exists(fp):\n                continue\n            mtime = os.path.getmtime(fp)\n            if fp not in last_mtimes or mtime > last_mtimes[fp]:\n                print(f\"\\n🔄 Change detected: {fp}\")\n                last_mtimes[fp] = mtime\n                changed = True\n\n        if changed:\n            print(\"\\n\" + \"=\"*60)\n            print(\"Running eval...\")\n            print(\"=\"*60)\n            results = run_eval_with_results()\n            if \"error\" in results:\n                print(f\"Error: {results['error']}\")\n            else:\n                print(json.dumps({\n                    \"top1_accuracy\": results[\"top1_accuracy\"],\n                    \"topk_accuracy\": results[\"topk_accuracy\"],\n                    \"total\": results[\"total\"],\n                    \"duration_secs\": results[\"duration_secs\"]\n                }, indent=2))"}
{"id":65,"text":"main():\n    parser = argparse.ArgumentParser(description=\"RAG eval loop with regression tracking\")\n    parser.add_argument(\"--baseline\", action=\"store_true\", help=\"Save current results as baseline\")\n    parser.add_argument(\"--compare\", action=\"store_true\", help=\"Compare current results with baseline\")\n    parser.add_argument(\"--watch\", action=\"store_true\", help=\"Watch for file changes and re-run\")\n    parser.add_argument(\"--json\", action=\"store_true\", help=\"Output results as JSON\")\n\n    args = parser.parse_args()\n\n    if args.watch:\n        watch_mode()\n        return\n\n    print(\"Running eval...\")\n    results = run_eval_with_results()\n\n    if \"error\" in results:\n        print(f\"Error: {results['error']}\", file=sys.stderr)\n        sys.exit(1)\n\n    if args.json:\n        print(json.dumps(results, indent=2))\n    else:\n        print(\"\\n\" + \"=\"*60)\n        print(\"EVAL RESULTS\")\n        print(\"=\"*60)\n        print(f\"Total questions: {results['total']}\")\n        print(f\"Top-1 accuracy:  {results['top1_accuracy']:.1%} ({results['top1_hits']}/{results['total']})\")\n        print(f\"Top-{FINAL_K} accuracy: {results['topk_accuracy']:.1%} ({results['topk_hits']}/{results['total']})\")\n        print(f\"Duration:        {results['duration_secs']}s\")\n        print(f\"Timestamp:       {results['timestamp']}\")\n\n        # Show failures\n        failures = [r for r in results[\"results\"] if not r[\"topk_hit\"]]\n        if failures:\n            print(f\"\\n⚠ Failures ({len(failures)}):\")\n            for r in failures:\n                print(f\"  [{r['repo']}] {r['question']}\")\n                print(f\"    Expected: {r['expect_paths']}\")\n                print(f\"    Got: {r['top_paths'][:3]}\")\n\n    if args.baseline:\n        save_baseline(results)\n    elif args.compare:\n        compare_with_baseline(results)\n\n\nif __name__ == \"__main__\":\n    main()"}
{"id":66,"text":"import os, json\nfrom typing import Dict\nfrom dotenv import load_dotenv\nfrom env_model import generate_text\n\nload_dotenv()\nREPO = os.getenv('REPO','project').strip()\nMAX_CHUNKS = int(os.getenv('CARDS_MAX','0') or '0')\nBASE = os.path.join(os.path.dirname(__file__), 'out.noindex', REPO)\nCHUNKS = os.path.join(BASE, 'chunks.jsonl')\nCARDS = os.path.join(BASE, 'cards.jsonl')\nCARDS_TXT = os.path.join(BASE, 'cards.txt')\nINDEX_DIR = os.path.join(BASE, 'bm25_cards')\n\nPROMPT = (\n    \"Summarize this code chunk for retrieval as a JSON object with keys: \"\n    \"symbols (array of names: functions/classes/components/routes), purpose (short sentence), \"\n    \"routes (array of route paths if any). Respond with only the JSON.\\n\\n\"\n)\niter_chunks():\n    with open(CHUNKS, 'r', encoding='utf-8') as f:\n        for line in f:\n            o = json.loads(line)\n            yield o"}
{"id":67,"text":"main():\n    os.makedirs(BASE, exist_ok=True)\n    # Responses API via env_model.generate_text\n    n = 0\n    with open(CARDS, 'w', encoding='utf-8') as out_json, open(CARDS_TXT, 'w', encoding='utf-8') as out_txt:\n        for ch in iter_chunks():\n            code = ch.get('code','')\n            fp = ch.get('file_path','')\n            snippet = code[:2000]\n            msg = PROMPT + snippet\n            try:\n                text, _ = generate_text(user_input=msg, system_instructions=None, reasoning_effort=None, response_format={\"type\": \"json_object\"})\n                content = (text or '').strip()\n                card: Dict = json.loads(content) if content else {\"symbols\": [], \"purpose\": \"\", \"routes\": []}\n            except Exception:\n                card = {\"symbols\": [], \"purpose\": \"\", \"routes\": []}\n            card['file_path'] = fp\n            card['id'] = ch.get('id')\n            out_json.write(json.dumps(card, ensure_ascii=False) + '\\n')\n            # Text for BM25\n            text = ' '.join(card.get('symbols', [])) + '\\n' + card.get('purpose','') + '\\n' + ' '.join(card.get('routes', [])) + '\\n' + fp\n            out_txt.write(text.replace('\\n',' ') + '\\n')\n            n += 1\n            if MAX_CHUNKS and n >= MAX_CHUNKS:\n                break\n    # Build BM25 over cards text\n    try:\n        import bm25s\n        from bm25s.tokenization import Tokenizer\n        from Stemmer import Stemmer\n        stemmer = Stemmer('english'); tok = Tokenizer(stemmer=stemmer, stopwords='en')\n        with open(CARDS_TXT,'r',encoding='utf-8') as f:\n            docs = [line.strip() for line in f if line.strip()]\n        tokens = tok.tokenize(docs)\n        retriever = bm25s.BM25(method='lucene', k1=1.2, b=0.65)\n        retriever.index(tokens)\n        # Workaround: ensure JSON-serializable vocab keys\n        try:\n            retriever.vocab_dict = {str(k): v for k, v in retriever.vocab_dict.items()}\n        except Exception:\n            pass\n        os.makedirs(INDEX_DIR, exist_ok=True)\n        retriever.save(INDEX_DIR, corpus=docs)\n        tok.save_vocab(save_dir=INDEX_DIR)\n        tok.save_stopwords(save_dir=INDEX_DIR)\n        print(f\"Built cards BM25 index with {len(docs)} docs at {INDEX_DIR}\")\n    except Exception as e:\n        print('BM25 build failed:', e)\n\nif __name__ == '__main__':\n    main()"}
{"id":68,"text":"import os\nimport json\nfrom pathlib import Path\nfrom typing import Any, Dict, List\n\n\nROOT = Path(__file__).resolve().parent\n\n_read_json(path: Path, default: Any) -> Any:\n    if path.exists():\n        try:\n            return json.loads(path.read_text())\n        except Exception:\n            return default\n    return default"}
{"id":69,"text":"_last_index_timestamp_for_repo(base: Path, repo_name: str) -> str | None:\n    \"\"\"Return the best-effort last index timestamp for a single repo under a base dir.\n\n    Preference order:\n    1) base/<repo>/last_index.json[\"timestamp\"]\n    2) mtime of base/<repo>/chunks.jsonl\n    3) mtime of base/<repo>/bm25_index directory\n    \"\"\"\n    repo_dir = base / repo_name\n    if not repo_dir.exists():\n        return None\n\n    # 1) Explicit metadata file\n    meta = _read_json(repo_dir / \"last_index.json\", {})\n    ts = str(meta.get(\"timestamp\") or \"\").strip()\n    if ts:\n        return ts\n\n    # 2) chunks.jsonl mtime\n    chunks = repo_dir / \"chunks.jsonl\"\n    if chunks.exists():\n        try:\n            return __import__('datetime').datetime.utcfromtimestamp(chunks.stat().st_mtime).isoformat() + 'Z'\n        except Exception:\n            pass\n\n    # 3) bm25_index dir mtime\n    bm25 = repo_dir / \"bm25_index\"\n    if bm25.exists():\n        try:\n            return __import__('datetime').datetime.utcfromtimestamp(bm25.stat().st_mtime).isoformat() + 'Z'\n        except Exception:\n            pass\n    return None"}
{"id":70,"text":"get_index_stats() -> Dict[str, Any]:\n    \"\"\"Gather comprehensive indexing statistics with storage calculator integration.\n\n    This mirrors the server's previous _get_index_stats, but prefers a persisted\n    last_index.json timestamp if present, falling back to file mtimes, then now().\n    \"\"\"\n    import subprocess\n    from datetime import datetime\n\n    # Get embedding configuration\n    embedding_type = os.getenv(\"EMBEDDING_TYPE\", \"openai\").lower()\n    embedding_dim = int(os.getenv(\"EMBEDDING_DIM\", \"3072\" if embedding_type == \"openai\" else \"512\"))\n\n    stats: Dict[str, Any] = {\n        \"timestamp\": datetime.utcnow().isoformat() + 'Z',  # will be replaced below if we detect a better one\n        \"repos\": [],\n        \"total_storage\": 0,\n        \"embedding_config\": {\n            \"provider\": embedding_type,\n            \"model\": \"text-embedding-3-large\" if embedding_type == \"openai\" else f\"local-{embedding_type}\",\n            \"dimensions\": embedding_dim,\n            \"precision\": \"float32\",\n        },\n        \"keywords_count\": 0,\n        \"storage_breakdown\": {\n            \"chunks_json\": 0,\n            \"bm25_index\": 0,\n            \"cards\": 0,\n            \"embeddings_raw\": 0,\n            \"qdrant_overhead\": 0,\n            \"reranker_cache\": 0,\n            \"redis\": 419430400,  # 400 MiB default\n        },\n        \"costs\": {\n            \"total_tokens\": 0,\n            \"embedding_cost\": 0.0,\n        },\n    }\n\n    # Current repo + branch\n    try:\n        repo = os.getenv(\"REPO\", \"agro\")\n        branch_result = subprocess.run([\"git\", \"branch\", \"--show-current\"], capture_output=True, text=True, cwd=ROOT)\n        branch = branch_result.stdout.strip() if branch_result.returncode == 0 else \"unknown\"\n        stats[\"current_repo\"] = repo\n        stats[\"current_branch\"] = branch\n    except Exception:\n        stats[\"current_repo\"] = os.getenv(\"REPO\", \"agro\")\n        stats[\"current_branch\"] = \"unknown\"\n\n    total_chunks = 0\n\n    # Index profiles to scan (shared, gui, devclean)\n    base_paths = [\"out.noindex-shared\", \"out.noindex-gui\", \"out.noindex-devclean\"]\n    discovered_ts: List[str] = []\n    for base in base_paths:\n        base_path = ROOT / base\n        if not base_path.exists():\n            continue\n        profile_name = base.replace(\"out.noindex-\", \"\")\n        repo_dirs = [d for d in base_path.iterdir() if d.is_dir()]\n        for repo_dir in repo_dirs:\n            repo_name = repo_dir.name\n            chunks_file = repo_dir / \"chunks.jsonl\"\n            bm25_dir = repo_dir / \"bm25_index\"\n            cards_file = repo_dir / \"cards.jsonl\"\n\n            repo_stats: Dict[str, Any] = {\n                \"name\": repo_name,\n                \"profile\": profile_name,\n                \"paths\": {\n                    \"chunks\": str(chunks_file) if chunks_file.exists() else None,\n                    \"bm25\": str(bm25_dir) if bm25_dir.exists() else None,\n                    \"cards\": str(cards_file) if cards_file.exists() else None,\n                },\n                \"sizes\": {},\n                \"chunk_count\": 0,\n                \"has_cards\": cards_file.exists() if cards_file else False,\n            }\n\n            # Aggregate sizes and counts\n            if chunks_file.exists():\n                size = chunks_file.stat().st_size\n                repo_stats[\"sizes\"][\"chunks\"] = size\n                stats[\"total_storage\"] += size\n                stats[\"storage_breakdown\"][\"chunks_json\"] += size\n                try:\n                    with open(chunks_file, 'r') as f:\n                        cc = sum(1 for _ in f)\n                        repo_stats[\"chunk_count\"] = cc\n                        total_chunks += cc\n                except Exception:\n                    pass\n\n            if bm25_dir.exists():\n                bm25_size = sum(f.stat().st_size for f in bm25_dir.rglob('*') if f.is_file())\n                repo_stats[\"sizes\"][\"bm25\"] = bm25_size\n                stats[\"total_storage\"] += bm25_size\n                stats[\"storage_breakdown\"][\"bm25_index\"] += bm25_size\n\n            if cards_file.exists():\n                card_size = cards_file.stat().st_size\n                repo_stats[\"sizes\"][\"cards\"] = card_size\n                stats[\"total_storage\"] += card_size\n                stats[\"storage_breakdown\"][\"cards\"] += card_size\n\n            stats[\"repos\"].append(repo_stats)\n\n            # Try to resolve a last-index timestamp for this repo under this profile\n            ts = _last_index_timestamp_for_repo(base_path, repo_name)\n            if ts:\n                discovered_ts.append(ts)\n\n    # Embedding storage + rough costs when we have chunks\n    if total_chunks > 0:\n        bytes_per_float = 4\n        embeddings_raw = total_chunks * embedding_dim * bytes_per_float\n        qdrant_overhead_multiplier = 1.5\n        qdrant_total = embeddings_raw * qdrant_overhead_multiplier\n        reranker_cache = embeddings_raw * 0.5\n        stats[\"storage_breakdown\"][\"embeddings_raw\"] = embeddings_raw\n        stats[\"storage_breakdown\"][\"qdrant_overhead\"] = int(qdrant_total - embeddings_raw)\n        stats[\"storage_breakdown\"][\"reranker_cache\"] = int(reranker_cache)\n        stats[\"total_storage\"] += qdrant_total + reranker_cache + stats[\"storage_breakdown\"][\"redis\"]\n        if embedding_type == \"openai\":\n            est_tokens_per_chunk = 750\n            total_tokens = total_chunks * est_tokens_per_chunk\n            cost_per_million = 0.13\n            embedding_cost = (total_tokens / 1_000_000) * cost_per_million\n            stats[\"costs\"][\"total_tokens\"] = total_tokens\n            stats[\"costs\"][\"embedding_cost\"] = round(embedding_cost, 4)\n\n    # Try to get keywords count\n    keywords_file = ROOT / \"data\" / f\"keywords_{stats.get('current_repo','agro')}.json\"\n    if keywords_file.exists():\n        try:\n            kw_data = json.loads(keywords_file.read_text())\n            stats[\"keywords_count\"] = len(kw_data) if isinstance(kw_data, list) else len(kw_data.get(\"keywords\", []))\n        except Exception:\n            pass\n\n    # Set a better global timestamp if any per-repo timestamp found\n    if discovered_ts:\n        # Prefer the most recent lexicographically (ISO8601 UTC) which matches chronological order\n        stats[\"timestamp\"] = sorted(discovered_ts)[-1]\n\n    return stats"}
{"id":71,"text":"\"\"\"\nFeature gating helpers.\n\n- is_pro(): True if edition/tier is 'pro' or 'enterprise'\n- is_enterprise(): True if edition/tier is 'enterprise'\n\nEnv controls (any of these work):\n- AGRO_EDITION=oss|pro|enterprise  (preferred)\n- TIER=free|pro|enterprise         (back-compat)\n- PRO_ENABLED=true/false           (optional override)\n- ENTERPRISE_ENABLED=true/false    (optional override)\n\"\"\"\nimport os\n\n_truthy(val: str | None) -> bool:\n    if not val:\n        return False\n    return val.strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}\n\nis_pro() -> bool:\n    edition = (os.getenv(\"AGRO_EDITION\") or os.getenv(\"TIER\") or \"\").strip().lower()\n    if edition in {\"pro\", \"enterprise\"}:\n        return True\n    # Optional explicit override\n    if _truthy(os.getenv(\"PRO_ENABLED\")):\n        return True\n    if _truthy(os.getenv(\"ENTERPRISE_ENABLED\")):\n        return True\n    return False\n\nis_enterprise() -> bool:\n    edition = (os.getenv(\"AGRO_EDITION\") or os.getenv(\"TIER\") or \"\").strip().lower()\n    if edition == \"enterprise\":\n        return True\n    if _truthy(os.getenv(\"ENTERPRISE_ENABLED\")):\n        return True\n    return False"}
{"id":72,"text":"from __future__ import annotations\nimport os, json\nfrom typing import Dict, Any\n\nfrom fastmcp import FastMCP\n\n# Reuse internal pipeline\nfrom langgraph_app import build_graph\nfrom hybrid_search import search_routed_multi\nfrom config_loader import list_repos\n\n\nmcp = FastMCP(\"rag-service\")\n_graph = None\n\n_get_graph():\n    global _graph\n    if _graph is None:\n        _graph = build_graph()\n    return _graph\n\n\n@mcp.tool()"}
{"id":73,"text":"answer(repo: str, question: str) -> Dict[str, Any]:\n    \"\"\"Answer a codebase question using local LangGraph (retrieval+generation). Returns text + citations.\"\"\"\n    g = _get_graph()\n    allowed = set(list_repos())\n    if repo not in allowed:\n        return {\"error\": f\"invalid repo '{repo}', allowed={sorted(allowed)}\"}\n    cfg = {\"configurable\": {\"thread_id\": f\"http-{repo}\"}}\n    state = {\n        \"question\": question,\n        \"documents\": [],\n        \"generation\": \"\",\n        \"iteration\": 0,\n        \"confidence\": 0.0,\n        \"repo\": repo,\n    }\n    res = g.invoke(state, cfg)\n    docs = res.get(\"documents\", [])[:5]\n    citations = [f\"{d['file_path']}:{d['start_line']}-{d['end_line']}\" for d in docs]\n    return {\n        \"answer\": res.get(\"generation\", \"\"),\n        \"citations\": citations,\n        \"repo\": res.get(\"repo\", repo),\n        \"confidence\": float(res.get(\"confidence\", 0.0) or 0.0),\n    }\n\n\n@mcp.tool()"}
{"id":74,"text":"search(repo: str, question: str, top_k: int = 10) -> Dict[str, Any]:\n    \"\"\"Retrieve relevant code locations without generation.\"\"\"\n    allowed = set(list_repos())\n    if repo not in allowed:\n        return {\"error\": f\"invalid repo '{repo}', allowed={sorted(allowed)}\"}\n    docs = search_routed_multi(question, repo_override=repo, m=4, final_k=top_k)\n    results = [\n        {\n            \"file_path\": d.get(\"file_path\", \"\"),\n            \"start_line\": d.get(\"start_line\", 0),\n            \"end_line\": d.get(\"end_line\", 0),\n            \"language\": d.get(\"language\", \"\"),\n            \"rerank_score\": float(d.get(\"rerank_score\", 0.0) or 0.0),\n            \"repo\": d.get(\"repo\", repo),\n        }\n        for d in docs\n    ]\n    return {\"results\": results, \"repo\": repo, \"count\": len(results)}\n\n\nif __name__ == \"__main__\":\n    # Serve over HTTP for remote MCP (platform evals). Use env overrides for host/port/path.\n    host = os.getenv(\"MCP_HTTP_HOST\", \"0.0.0.0\")\n    port = int(os.getenv(\"MCP_HTTP_PORT\", \"8013\"))\n    path = os.getenv(\"MCP_HTTP_PATH\", \"/mcp\")\n    mcp.run(transport=\"http\", host=host, port=port, path=path)"}
{"id":75,"text":"# Python auto-imports sitecustomize at startup if present in sys.path.\n# We use it to block legacy Chat Completions usage at runtime.\ntry:\n    import openai  # type: ignore\n\n    def _blocked(*_args, **_kwargs):  # noqa: D401\n        raise RuntimeError(\n            \"Legacy Chat Completions API is disabled. Use Responses API via env_model.generate_text().\\n\"\n            \"Docs: https://openai.com/index/new-tools-and-features-in-the-responses-api/\"\n        )\n\n    # Block classic patterns if present on this installed version\n    if hasattr(openai, \"ChatCompletion\"):\n        try:\n            openai.ChatCompletion.create = staticmethod(_blocked)  # type: ignore[attr-defined]\n        except Exception:\n            pass\n    # Some older clients expose nested chat.completions\n    if hasattr(openai, \"chat\"):\n        chat = getattr(openai, \"chat\")\n        if hasattr(chat, \"completions\"):\n            try:\n                chat.completions.create = _blocked  # type: ignore[attr-defined]\n            except Exception:\n                pass\nexcept Exception:\n    # If openai not installed yet, do nothing.\n    pass"}
{"id":76,"text":"import os, operator\nfrom typing import List, Dict, TypedDict, Annotated\nfrom pathlib import Path\nfrom dotenv import load_dotenv, find_dotenv\nfrom langgraph.graph import END, StateGraph\nfrom langgraph.checkpoint.redis import RedisSaver\nfrom hybrid_search import search_routed_multi as hybrid_search_routed_multi\nfrom env_model import generate_text\nfrom index_stats import get_index_stats\n\n# Load environment from repo root .env without hard-coded paths\ntry:\n    # Load any existing env first\n    load_dotenv(override=False)\n    repo_root = Path(__file__).resolve().parent\n    env_path = repo_root / \".env\"\n    if env_path.exists():\n        load_dotenv(dotenv_path=env_path, override=False)\n    else:\n        alt = find_dotenv(usecwd=True)\n        if alt:\n            load_dotenv(dotenv_path=alt, override=False)\nexcept Exception:\n    pass\nRAGState(TypedDict):\n    question: str\n    documents: Annotated[List[Dict], operator.add]\n    generation: str\n    iteration: int\n    confidence: float\n    repo: str\n\n# Responses API shim used via generate_text()"}
{"id":77,"text":"should_use_multi_query(question: str) -> bool:\n    q = (question or '').lower().strip()\n    if len(q.split()) <= 3:\n        return False\n    for w in (\"how\", \"why\", \"explain\", \"compare\", \"tradeoff\"):\n        if w in q:\n            return True\n    return False\nretrieve_node(state: RAGState) -> Dict:\n    q = state['question']\n    repo = state.get('repo') if isinstance(state, dict) else None\n    mq = int(os.getenv('MQ_REWRITES','2')) if should_use_multi_query(q) else 1\n    docs = hybrid_search_routed_multi(q, repo_override=repo, m=mq, final_k=20)\n    conf = float(sum(d.get('rerank_score',0.0) for d in docs)/max(1,len(docs)))\n    # Propagate the routed repo into state so downstream nodes build correct headers\n    repo_used = (repo or (docs[0].get('repo') if docs else os.getenv('REPO','project')))\n    return {'documents': docs, 'confidence': conf, 'iteration': state.get('iteration',0)+1, 'repo': repo_used}"}
{"id":78,"text":"route_after_retrieval(state:RAGState)->str:\n    conf = float(state.get(\"confidence\", 0.0) or 0.0)\n    it = int(state.get(\"iteration\", 0) or 0)\n    docs = state.get(\"documents\", []) or []\n    scores = sorted([float(d.get(\"rerank_score\",0.0) or 0.0) for d in docs], reverse=True)\n    top1 = scores[0] if scores else 0.0\n    avg5 = (sum(scores[:5])/min(5, len(scores))) if scores else 0.0\n    # Allow env overrides so teams can tighten gates without code changes\n    try:\n        CONF_TOP1 = float(os.getenv('CONF_TOP1', '0.62'))\n        CONF_AVG5 = float(os.getenv('CONF_AVG5', '0.55'))\n        CONF_ANY = float(os.getenv('CONF_ANY', '0.55'))\n    except Exception:\n        CONF_TOP1, CONF_AVG5, CONF_ANY = 0.62, 0.55, 0.55\n    if top1 >= CONF_TOP1 or avg5 >= CONF_AVG5 or conf >= CONF_ANY:\n        return \"generate\"\n    if it >= 3:\n        return \"fallback\"\n    return \"rewrite_query\""}
{"id":79,"text":"rewrite_query(state: RAGState) -> Dict:\n    q = state['question']\n    sys = \"You rewrite developer questions into search-optimized queries without changing meaning.\"\n    user = f\"Rewrite this for code search (expand CamelCase, include API nouns), one line.\\n\\n{q}\"\n    newq, _ = generate_text(user_input=user, system_instructions=sys, reasoning_effort=None)\n    newq = (newq or '').strip()\n    return {'question': newq}"}
{"id":80,"text":"generate_node(state: RAGState) -> Dict:\n    q = state['question']; ctx = state['documents'][:5]\n    # Special-case: answer index status questions deterministically from local files\n    ql = (q or '').lower()\n    if any(kw in ql for kw in (\"last index\", \"last indexed\", \"when was this indexed\", \"when indexed\", \"index time\")):\n        stats = get_index_stats()\n        repo_hdr = state.get('repo') or os.getenv('REPO','project')\n        # Pick paths for the active repo if present\n        paths = None\n        for r in stats.get('repos', []):\n            if str(r.get('name')) == str(repo_hdr):\n                paths = r.get('paths', {})\n                break\n        lines = []\n        lines.append(f\"Most recent index: {stats.get('timestamp','unknown')}\")\n        if paths and (paths.get('chunks') or paths.get('bm25')):\n            if paths.get('chunks'):\n                lines.append(f\"chunks.jsonl: {paths['chunks']}\")\n            if paths.get('bm25'):\n                lines.append(f\"bm25_index: {paths['bm25']}\")\n        content = \"\\n\".join(lines)\n        header = f\"[repo: {repo_hdr}]\"\n        return {'generation': header + \"\\n\" + content}\n    citations = \"\\n\".join([f\"- {d['file_path']}:{d['start_line']}-{d['end_line']}\" for d in ctx])\n    context_text = \"\\n\\n\".join([d.get('code','') for d in ctx])\n    sys = 'You answer strictly from the provided code context. Always cite file paths and line ranges you used.'\n    user = f\"Question:\\n{q}\\n\\nContext:\\n{context_text}\\n\\nCitations (paths and line ranges):\\n{citations}\\n\\nAnswer:\"\n    content, _ = generate_text(user_input=user, system_instructions=sys, reasoning_effort=None)\n    content = content or ''\n    # Lightweight verifier: if confidence low, try multi-query retrieval and regenerate once\n    conf = float(state.get('confidence', 0.0) or 0.0)\n    if conf < 0.55:\n        repo = state.get('repo') or os.getenv('REPO','project')\n        alt_docs = hybrid_search_routed_multi(q, repo_override=repo, m=4, final_k=10)\n        if alt_docs:\n            ctx2 = alt_docs[:5]\n            citations2 = \"\\n\".join([f\"- {d['file_path']}:{d['start_line']}-{d['end_line']}\" for d in ctx2])\n            context_text2 = \"\\n\\n\".join([d.get('code','') for d in ctx2])\n            user2 = f\"Question:\\n{q}\\n\\nContext:\\n{context_text2}\\n\\nCitations (paths and line ranges):\\n{citations2}\\n\\nAnswer:\"\n            content2, _ = generate_text(user_input=user2, system_instructions=sys, reasoning_effort=None)\n            content = (content2 or content or '')\n    repo_hdr = state.get('repo') or (ctx[0].get('repo') if ctx else None) or os.getenv('REPO','project')\n    header = f\"[repo: {repo_hdr}]\"\n    return {'generation': header + \"\\n\" + content}"}
{"id":81,"text":"fallback_node(state: RAGState) -> Dict:\n    repo_hdr = state.get('repo') or (state.get('documents')[0].get('repo') if state.get('documents') else None) or os.getenv('REPO','project')\n    header = f\"[repo: {repo_hdr}]\"\n    msg = \"I don't have high confidence from local code. Try refining the question or expanding the context.\"\n    return {'generation': header + \"\\n\" + msg}"}
{"id":82,"text":"build_graph():\n    builder = StateGraph(RAGState)\n    builder.add_node('retrieve', retrieve_node)\n    builder.add_node('rewrite_query', rewrite_query)\n    builder.add_node('generate', generate_node)\n    builder.add_node('fallback', fallback_node)\n    builder.set_entry_point('retrieve')\n    builder.add_conditional_edges('retrieve', route_after_retrieval, {\n        'generate': 'generate', 'rewrite_query': 'rewrite_query', 'fallback': 'fallback'\n    })\n    builder.add_edge('rewrite_query', 'retrieve')\n    builder.add_edge('generate', END)\n    builder.add_edge('fallback', END)\n    DB_URI = os.getenv('REDIS_URL','redis://127.0.0.1:6379/0')\n    # Prefer Redis checkpointer, but do not fail hard if unavailable\n    try:\n        checkpointer = RedisSaver(redis_url=DB_URI)\n        graph = builder.compile(checkpointer=checkpointer)\n    except Exception:\n        graph = builder.compile()\n    return graph\n\nif __name__ == '__main__':\n    import sys\n    q = ' '.join(sys.argv[1:]) if len(sys.argv)>1 else 'Where is OAuth token validated?'\n    graph = build_graph(); cfg = {'configurable': {'thread_id': 'dev'}}\n    res = graph.invoke({'question': q, 'documents': [], 'generation':'', 'iteration':0, 'confidence':0.0}, cfg)\n    print(res['generation'])"}
{"id":83,"text":"import os, json, time\nfrom typing import List, Dict\nfrom dotenv import load_dotenv\nfrom hybrid_search import search_routed, search_routed_multi\n\nload_dotenv()\n\nGOLDEN_PATH = os.getenv('GOLDEN_PATH', 'golden.json')\nUSE_MULTI = os.getenv('EVAL_MULTI','1') == '1'\nFINAL_K = int(os.getenv('EVAL_FINAL_K','5'))\n\n\"\"\"\nGolden file format (golden.json):\n[\n  {\"q\": \"Where is ProviderSetupWizard rendered?\", \"repo\": \"project\", \"expect_paths\": [\"core/admin_ui/src/components/ProviderSetupWizard.tsx\"]},\n  {\"q\": \"Where do we mask PHI in events?\", \"repo\": \"project\", \"expect_paths\": [\"app/...\"]}\n]\n\"\"\"\nhit(paths: List[str], expect: List[str]) -> bool:\n    return any(any(exp in p for p in paths) for exp in expect)"}
{"id":84,"text":"main():\n    if not os.path.exists(GOLDEN_PATH):\n        print('No golden file found at', GOLDEN_PATH)\n        return\n    gold = json.load(open(GOLDEN_PATH))\n    total = len(gold); hits_top1 = 0; hits_topk = 0\n    t0 = time.time()\n    for i, row in enumerate(gold, 1):\n        q = row['q']; repo = row.get('repo') or os.getenv('REPO','project')\n        expect = row.get('expect_paths') or []\n        if USE_MULTI:\n            docs = search_routed_multi(q, repo_override=repo, m=4, final_k=FINAL_K)\n        else:\n            docs = search_routed(q, repo_override=repo, final_k=FINAL_K)\n        paths = [d.get('file_path','') for d in docs]\n        if paths:\n            if hit(paths[:1], expect): hits_top1 += 1\n            if hit(paths, expect): hits_topk += 1\n        print(f\"[{i}/{total}] repo={repo} q={q}\\n  top1={paths[:1]}\\n  top{FINAL_K} hit={hit(paths, expect)}\")\n    dt = time.time() - t0\n    print(json.dumps({\n        'total': total,\n        'top1': hits_top1,\n        'topk': hits_topk,\n        'final_k': FINAL_K,\n        'use_multi': USE_MULTI,\n        'secs': round(dt,2)\n    }, indent=2))\n\nif __name__ == '__main__':\n    main()"}
{"id":85,"text":"#!/usr/bin/env python3\n\"\"\"\nAutotune autoscaler (stub):\n- Samples local CPU/RAM (and GPU later) via psutil\n- Reads gui/autotune_policy.json\n- During off hours, POSTs /api/autotune/status with a suggested mode (ECO/BALANCED/TURBO)\n\nDefaults:\n- Does not change env or persist profiles; only signals current_mode to the server stub\n- Business hours gate: leaves user settings untouched during business hours\n\"\"\"\nfrom __future__ import annotations\nimport argparse\nimport json\nimport os\nimport sys\nimport time\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Dict, Optional\n\ntry:\n    import psutil  # type: ignore\nexcept Exception:\n    psutil = None  # type: ignore\n\nimport requests  # type: ignore\n\nROOT = Path(__file__).resolve().parent\nGUI = ROOT / \"gui\"\nPOLICY_PATH = GUI / \"autotune_policy.json\"\n\n\n@dataclassMetrics:\n    cpu: float\n    mem: float\n    gpu: Optional[float] = None"}
{"id":86,"text":"load_policy(path: Path) -> Dict[str, Any]:\n    try:\n        return json.loads(path.read_text())\n    except Exception:\n        return {\n            \"business_hours\": {\"start\": \"09:00\", \"end\": \"18:00\", \"days\": [1, 2, 3, 4, 5]},\n            \"thresholds\": {\"cpu_hot\": 0.8, \"mem_hot\": 0.85, \"gpu_hot\": 0.85},\n            \"modes\": {\n                \"ECO\": {\"when\": {\"cpu_util_max\": 0.25, \"mem_util_max\": 0.50, \"gpu_util_max\": 0.30}},\n                \"BALANCED\": {\"when\": {\"cpu_util_max\": 0.55, \"mem_util_max\": 0.70, \"gpu_util_max\": 0.60}},\n                \"TURBO\": {\"when\": {\"cpu_util_max\": 0.90, \"mem_util_max\": 0.90, \"gpu_util_max\": 0.90}},\n            },\n        }\n\nparse_hhmm(s: str) -> tuple[int, int]:\n    h, m = s.split(\":\")\n    return int(h), int(m)"}
{"id":87,"text":"is_business_hours(now: Optional[time.struct_time], policy: Dict[str, Any]) -> bool:\n    if now is None:\n        now = time.localtime()\n    days = set(policy.get(\"business_hours\", {}).get(\"days\", [1, 2, 3, 4, 5]))\n    if now.tm_wday + 1 not in days:\n        return False\n    start_s = policy.get(\"business_hours\", {}).get(\"start\", \"09:00\")\n    end_s = policy.get(\"business_hours\", {}).get(\"end\", \"18:00\")\n    sh, sm = parse_hhmm(start_s)\n    eh, em = parse_hhmm(end_s)\n    tmin = now.tm_hour * 60 + now.tm_min\n    start_m = sh * 60 + sm\n    end_m = eh * 60 + em\n    return start_m <= tmin <= end_m\n\nsample_metrics() -> Metrics:\n    if psutil is None:\n        return Metrics(cpu=0.0, mem=0.0, gpu=None)\n    cpu = psutil.cpu_percent(interval=0.3) / 100.0\n    mem = psutil.virtual_memory().percent / 100.0\n    # TODO: GPU (Metal/CUDA) sampling in future\n    return Metrics(cpu=cpu, mem=mem, gpu=None)"}
{"id":88,"text":"pick_mode(m: Metrics, policy: Dict[str, Any]) -> Optional[str]:\n    # Simple rule: choose the first mode whose 'when' limits are not exceeded\n    modes = policy.get(\"modes\", {})\n    order = [\"ECO\", \"BALANCED\", \"TURBO\"]\n    for name in order:\n        spec = modes.get(name, {}).get(\"when\", {})\n        cpu_ok = m.cpu <= float(spec.get(\"cpu_util_max\", 1.0))\n        mem_ok = m.mem <= float(spec.get(\"mem_util_max\", 1.0))\n        gpu_lim = spec.get(\"gpu_util_max\")\n        gpu_ok = True if gpu_lim is None or m.gpu is None else m.gpu <= float(gpu_lim)\n        if cpu_ok and mem_ok and gpu_ok:\n            return name\n    return None\n\npost_status(host: str, enabled: bool, mode: Optional[str]) -> None:\n    try:\n        requests.post(\n            f\"{host}/api/autotune/status\",\n            json={\"enabled\": enabled, \"current_mode\": mode},\n            timeout=3,\n        )\n    except Exception:\n        pass"}
{"id":89,"text":"main(argv: list[str]) -> int:\n    p = argparse.ArgumentParser(description=\"Autotune autoscaler (stub)\")\n    p.add_argument(\"--host\", default=os.getenv(\"AUTOTUNE_HOST\", \"http://127.0.0.1:8012\"))\n    p.add_argument(\"--interval\", type=int, default=int(os.getenv(\"AUTOTUNE_INTERVAL\", \"15\")))\n    args = p.parse_args(argv)\n\n    policy = load_policy(POLICY_PATH)\n    print(f\"[autoscaler] Using policy {POLICY_PATH}\")\n    print(f\"[autoscaler] Posting to {args.host} every {args.interval}s (off-hours only)\")\n\n    while True:\n        now = time.localtime()\n        bh = is_business_hours(now, policy)\n        m = sample_metrics()\n        if not bh:\n            mode = pick_mode(m, policy)\n            post_status(args.host, enabled=True, mode=mode)\n        time.sleep(max(3, args.interval))\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    raise SystemExit(main(sys.argv[1:]))"}
{"id":90,"text":"import os, re, hashlib\nfrom typing import Dict, List, Optional\n\n# Optional import: tree_sitter_languages may be unavailable on newer Python versions.\ntry:\n    from tree_sitter_languages import get_parser as _ts_get_parser\nexcept Exception:\n    _ts_get_parser = None\n\nLANG_MAP = {\n    \".py\": \"python\", \".js\": \"javascript\", \".jsx\": \"javascript\",\n    \".ts\": \"typescript\", \".tsx\": \"typescript\",\n    \".go\": \"go\", \".java\": \"java\", \".rs\": \"rust\",\n    \".c\": \"c\", \".h\": \"c\", \".cpp\": \"cpp\", \".cc\": \"cpp\", \".hpp\": \"cpp\",\n}\n\n# Optional overlap to provide additional local context when chunk boundaries split logical units\nOVERLAP_LINES = 20\n\nFUNC_NODES = {\n    \"python\": {\"function_definition\", \"class_definition\"},\n    \"javascript\": {\"function_declaration\", \"class_declaration\", \"method_definition\", \"arrow_function\"},\n    \"typescript\": {\"function_declaration\", \"class_declaration\", \"method_signature\", \"method_definition\", \"arrow_function\"},\n    \"go\": {\"function_declaration\", \"method_declaration\"},\n    \"java\": {\"class_declaration\", \"method_declaration\"},\n    \"rust\": {\"function_item\", \"impl_item\"},\n    \"c\": {\"function_definition\"},\n    \"cpp\": {\"function_definition\", \"class_specifier\"},\n}\n\nIMPORT_NODES = {\n    \"python\": {\"import_statement\", \"import_from_statement\"},\n    \"javascript\": {\"import_declaration\"},\n    \"typescript\": {\"import_declaration\"},\n    \"go\": {\"import_declaration\"},\n    \"java\": {\"import_declaration\"},\n    \"rust\": {\"use_declaration\"},\n    \"c\": set(), \"cpp\": set(),\n}"}
{"id":91,"text":"lang_from_path(path:str)->Optional[str]:\n    _, ext = os.path.splitext(path)\n    return LANG_MAP.get(ext.lower())\nnonws_len(s:str)->int:\n    return len(re.sub(r\"\\s+\", \"\", s))\nextract_imports(src:str, lang:str)->List[str]:\n    try:\n        if _ts_get_parser is None:\n            raise RuntimeError(\"tree_sitter_languages not available\")\n        parser = _ts_get_parser(lang)\n        tree = parser.parse(bytes(src, \"utf-8\"))\n        imports = []\n        def walk(n):\n            if n.type in IMPORT_NODES.get(lang, set()):\n                imports.append(src[n.start_byte:n.end_byte])\n            for c in n.children:\n                walk(c)\n        walk(tree.root_node)\n        return imports\n    except Exception:\n        if lang == \"python\":\n            return re.findall(r\"^(?:from\\s+[^\\n]+|import\\s+[^\\n]+)$\", src, flags=re.M)\n        if lang in {\"javascript\",\"typescript\"}:\n            return re.findall(r\"^import\\s+[^\\n]+;$\", src, flags=re.M)\n        return []"}
{"id":92,"text":"greedy_fallback(src:str, fpath:str, lang:str, target:int)->List[Dict]:\n    sep = r\"(?:\\nclass\\s+|\\ndef\\s+)\" if lang==\"python\" else r\"(?:\\nclass\\s+|\\nfunction\\s+)\"\n    parts = re.split(sep, src)\n    if len(parts) < 2:\n        out, cur, acc = [], [], 0\n        for line in src.splitlines(True):\n            cur.append(line); acc += nonws_len(line)\n            if acc >= target:\n                out.append(\"\".join(cur)); cur, acc = [], 0\n        if cur: out.append(\"\".join(cur))\n        return [{\n            \"id\": hashlib.md5((fpath+str(i)+s[:80]).encode()).hexdigest()[:12],\n            \"file_path\": fpath, \"language\": lang, \"type\":\"blob\",\"name\":None,\n            \"start_line\": 1, \"end_line\": s.count(\"\\n\")+1, \"imports\": extract_imports(src, lang), \"code\": s\n        } for i,s in enumerate(out)]\n    else:\n        rejoined, buf, acc = [], [], 0\n        for p in parts:\n            if acc + nonws_len(p) > target and buf:\n                s = \"\".join(buf); rejoined.append(s); buf, acc = [], 0\n            buf.append(p); acc += nonws_len(p)\n        if buf: rejoined.append(\"\".join(buf))\n        return [{\n            \"id\": hashlib.md5((fpath+str(i)+s[:80]).encode()).hexdigest()[:12],\n            \"file_path\": fpath, \"language\": lang, \"type\":\"section\",\"name\":None,\n            \"start_line\": 1, \"end_line\": s.count(\"\\n\")+1, \"imports\": extract_imports(s, lang), \"code\": s\n        } for i,s in enumerate(rejoined)]"}
{"id":93,"text":"collect_files(roots:List[str])->List[str]:\n    import fnmatch\n    out=[]\n    # Hardcoded skip dirs for safety\n    skip_dirs = {\".git\",\"node_modules\",\".venv\",\"venv\",\"dist\",\"build\",\"__pycache__\",\".next\",\".turbo\",\".parcel-cache\",\".pytest_cache\",\"vendor\",\"third_party\",\".bundle\",\"Pods\"}\n\n    # Try to load exclusion patterns from file\n    exclude_patterns = []\n    for root in roots:\n        # Check for exclude_globs.txt in data/ directory\n        parent_dir = os.path.dirname(root) if os.path.isfile(root) else root\n        exclude_file = os.path.join(parent_dir, 'data', 'exclude_globs.txt')\n        if os.path.exists(exclude_file):\n            try:\n                with open(exclude_file, 'r') as f:\n                    patterns = [line.strip() for line in f if line.strip() and not line.startswith('#')]\n                    exclude_patterns.extend(patterns)\n            except Exception:\n                pass\n\n    for root in roots:\n        for dp, dns, fns in os.walk(root):\n            # Skip directories that match our exclusion rules\n            dns[:] = [d for d in dns if d not in skip_dirs and not d.startswith('.venv') and not d.startswith('venv')]\n\n            for fn in fns:\n                p = os.path.join(dp, fn)\n\n                # Check if file matches any exclusion pattern\n                skip = False\n                for pattern in exclude_patterns:\n                    if fnmatch.fnmatch(p, pattern) or fnmatch.fnmatch(os.path.relpath(p, root), pattern):\n                        skip = True\n                        break\n\n                if not skip and lang_from_path(p):\n                    out.append(p)\n    return out"}
{"id":94,"text":"_guess_name(lang:str, text:str)->Optional[str]:\n    if lang==\"python\":\n        m = re.search(r\"^(?:def|class)\\s+([A-Za-z_][A-Za-z0-9_]*)\", text, flags=re.M)\n        return m.group(1) if m else None\n    if lang in {\"javascript\",\"typescript\"}:\n        m = re.search(r\"^(?:function|class)\\s+([A-Za-z_$][A-Za-z0-9_$]*)\", text, flags=re.M)\n        return m.group(1) if m else None\n    return None"}
{"id":95,"text":"chunk_code(src:str, fpath:str, lang:str, target:int=900)->List[Dict]:\n    \"\"\"AST-aware chunking around functions/classes; falls back if no nodes.\"\"\"\n    try:\n        if _ts_get_parser is None:\n            raise RuntimeError(\"tree_sitter_languages not available\")\n        parser = _ts_get_parser(lang)\n        tree = parser.parse(bytes(src, \"utf-8\"))\n        wanted = FUNC_NODES.get(lang, set())\n        nodes = []\n        stack = [tree.root_node]\n        while stack:\n            n = stack.pop()\n            if n.type in wanted:\n                nodes.append(n)\n            stack.extend(n.children)\n        if not nodes:\n            return greedy_fallback(src, fpath, lang, target)\n        chunks: List[Dict] = []\n        all_lines = src.splitlines()\n        for i, n in enumerate(nodes):\n            text = src[n.start_byte:n.end_byte]\n            if nonws_len(text) > target:\n                for j, sub in enumerate(greedy_fallback(text, fpath, lang, target)):\n                    sub[\"id\"] = hashlib.md5((fpath+f\"/{i}:{j}\"+sub[\"code\"][:80]).encode()).hexdigest()[:12]\n                    sub[\"start_line\"] = n.start_point[0]+1\n                    sub[\"end_line\"] = sub[\"start_line\"] + sub[\"code\"].count(\"\\n\")\n                    chunks.append(sub)\n            else:\n                name = _guess_name(lang, text)\n                start_line = n.start_point[0] + 1\n                end_line = n.end_point[0] + 1\n                actual_start = max(1, start_line - OVERLAP_LINES) if OVERLAP_LINES > 0 else start_line\n                # Slice lines with 1-based indexing\n                chunk_text = \"\\n\".join(all_lines[actual_start-1:end_line])\n                chunks.append({\n                    \"id\": hashlib.md5((fpath+str(i)+text[:80]).encode()).hexdigest()[:12],\n                    \"file_path\": fpath,\n                    \"language\": lang,\n                    \"type\": \"unit\",\n                    \"name\": name,\n                    \"start_line\": actual_start,\n                    \"end_line\": end_line,\n                    \"imports\": extract_imports(src, lang),\n                    \"code\": chunk_text,\n                })\n        return chunks\n    except Exception:\n        return greedy_fallback(src, fpath, lang, target)"}
{"id":96,"text":"import os\nimport json\n\n# MLX backend (default for Apple Silicon)\nENRICH_BACKEND = os.getenv(\"ENRICH_BACKEND\", \"mlx\").lower()  # mlx | ollama\nMLX_MODEL = os.getenv(\"ENRICH_MODEL\", \"mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit\")\nOLLAMA_URL = os.getenv(\"OLLAMA_URL\", \"http://127.0.0.1:11434/api/generate\")\nOLLAMA_MODEL = os.getenv(\"ENRICH_MODEL_OLLAMA\", \"qwen3-coder:30b\")\n\nSYSTEM = (\n    \"You are a senior code analyst. Extract: 1) concise summary of purpose, \"\n    \"2) key APIs/classes/functions referenced, 3) inputs/outputs/side-effects, \"\n    \"4) 8-15 retrieval keywords (snake_case). Keep under 120 tokens.\"\n)\n\nPROMPT_TMPL = (\n    \"<system>\" + SYSTEM + \"</system>\\n\"\n    \"<analyze file='{file}' lang='{lang}'>\\n{code}\\n</analyze>\\n\"\n    \"<format>JSON with keys: summary, keywords</format>\"\n)\n\n# Lazy-load MLX model (only once)\n_mlx_model = None\n_mlx_tokenizer = None"}
{"id":97,"text":"_get_mlx_model():\n    global _mlx_model, _mlx_tokenizer\n    if _mlx_model is None:\n        from mlx_lm import load\n        _mlx_model, _mlx_tokenizer = load(MLX_MODEL)\n    return _mlx_model, _mlx_tokenizer"}
{"id":98,"text":"enrich(file_path: str, lang: str, code: str) -> dict:\n    prompt = PROMPT_TMPL.format(file=file_path, lang=lang, code=(code or '')[:4000])\n\n    if ENRICH_BACKEND == \"mlx\":\n        # MLX backend (Apple Silicon, fast)\n        try:\n            from mlx_lm import generate\n            model, tokenizer = _get_mlx_model()\n            txt = generate(model, tokenizer, prompt=prompt, max_tokens=150, verbose=False)\n        except Exception as e:\n            return {\"summary\": f\"MLX error: {str(e)[:100]}\", \"keywords\": []}\n    else:\n        # Ollama backend (fallback)\n        import requests\n        try:\n            resp = requests.post(\n                OLLAMA_URL,\n                json={\n                    \"model\": OLLAMA_MODEL,\n                    \"prompt\": prompt,\n                    \"stream\": False,\n                    \"options\": {\"temperature\": 0.1, \"num_ctx\": 4096},\n                },\n                timeout=10,\n            )\n            resp.raise_for_status()\n            txt = resp.json().get(\"response\", \"{}\")\n        except Exception as e:\n            return {\"summary\": f\"Ollama error: {str(e)[:100]}\", \"keywords\": []}\n\n    # Parse JSON response\n    try:\n        data = json.loads(txt)\n        if isinstance(data, dict):\n            return {\"summary\": data.get(\"summary\", \"\"), \"keywords\": data.get(\"keywords\", [])}\n    except Exception:\n        pass\n    return {\"summary\": txt[:300], \"keywords\": []}"}
{"id":99,"text":"import os\nfrom dotenv import load_dotenv\n\nload_dotenv('env/project.env')\nfrom serve_rag import app\n\nif __name__ == '__main__':\n    import uvicorn\n    os.environ['COLLECTION_NAME'] = os.environ.get('COLLECTION_NAME', f\"{os.environ['REPO']}_{os.environ.get('COLLECTION_SUFFIX','default')}\")\n    port = int(os.environ.get('PORT', '8012'))\n    uvicorn.run(app, host='127.0.0.1', port=port)"}
{"id":100,"text":"#!/usr/bin/env python3\n\"\"\"\nInteractive CLI chat interface for RAG service.\nUses LangGraph with Redis checkpoints for conversation memory.\n\nUsage:\n    export REPO=agro\n    export THREAD_ID=my-session-1\n    python chat_cli.py\n\nCommands:\n    /repo <name>    - Switch repository (from repos.json)\n    /save           - Save conversation checkpoint\n    /clear          - Clear conversation history\n    /help           - Show commands\n    /exit, /quit    - Exit chat\n\"\"\"\nimport os\nimport sys\nfrom pathlib import Path\ntry:\n    from dotenv import load_dotenv\nexcept Exception:\n    # Graceful fallback if python-dotenv is not installed yet\n    def load_dotenv(*args, **kwargs):\n        return False\n\n# Load environment\nload_dotenv(Path(__file__).parent / \".env\")\n\nfrom langgraph_app import build_graph\nfrom config_loader import list_repos\nfrom rich.console import Console\nfrom rich.markdown import Markdown\nfrom rich.panel import Panel\nfrom rich.prompt import Prompt\n\nconsole = Console()\n\n# Configuration\nREPO = os.getenv('REPO', 'agro')\nTHREAD_ID = os.getenv('THREAD_ID', 'cli-chat')"}
{"id":101,"text":"ChatCLI:\n    \"\"\"Interactive CLI chat with RAG.\"\"\"\n\n    def __init__(self, repo: str = 'agro', thread_id: str = 'cli-chat'):\n        self.repo = repo\n        self.thread_id = thread_id\n        self.graph = None\n        self._init_graph()\n\n    def _init_graph(self):\n        \"\"\"Initialize LangGraph with Redis checkpoints.\"\"\"\n        try:\n            self.graph = build_graph()\n            console.print(f\"[green]✓[/green] Graph initialized with Redis checkpoints\")\n        except Exception as e:\n            console.print(f\"[red]✗[/red] Failed to initialize graph: {e}\")\n            sys.exit(1)\n\n    def _get_config(self):\n        \"\"\"Get config for current thread.\"\"\"\n        return {\"configurable\": {\"thread_id\": self.thread_id}}\n\n    def _format_answer(self, generation: str) -> str:\n        \"\"\"Format answer, removing repo header if present.\"\"\"\n        lines = generation.split('\\n')\n        # Remove [repo: ...] header if present\n        if lines and lines[0].startswith('[repo:'):\n            return '\\n'.join(lines[1:]).strip()\n        return generation\n\n    def ask(self, question: str) -> dict:\n        \"\"\"Ask a question and get answer.\"\"\"\n        try:\n            state = {\n                \"question\": question,\n                \"documents\": [],\n                \"generation\": \"\",\n                \"iteration\": 0,\n                \"confidence\": 0.0,\n                \"repo\": self.repo\n            }\n\n            result = self.graph.invoke(state, self._get_config())\n            return result\n        except Exception as e:\n            console.print(f\"[red]Error:[/red] {e}\")\n            return {\"generation\": f\"Error: {e}\", \"documents\": [], \"confidence\": 0.0}\n\n    def switch_repo(self, new_repo: str):\n        \"\"\"Switch to a different repository.\"\"\"\n        allowed = set(list_repos())\n        if new_repo not in allowed:\n            console.print(f\"[red]✗[/red] Invalid repo. Allowed: {sorted(allowed)}\")\n            return\n\n        self.repo = new_repo\n        console.print(f\"[green]✓[/green] Switched to repo: [bold]{new_repo}[/bold]\")\n\n    def show_help(self):\n        \"\"\"Show available commands.\"\"\"\n        help_text = \"\"\"\n## Commands\n\n- `/repo <name>` - Switch repository (from repos.json)\n- `/save` - Save conversation checkpoint\n- `/clear` - Clear conversation history\n- `/help` - Show this help\n- `/exit`, `/quit` - Exit chat\n\n## Examples\n\nAsk a question:\n```\n> Where is OAuth token validated?\n```\n\nSwitch repo:\n```\n> /repo agro\n> How do we handle inbound faxes?\n```\n        \"\"\"\n        console.print(Markdown(help_text))\n\n    def show_welcome(self):\n        \"\"\"Show welcome message.\"\"\"\n        welcome = f\"\"\"\n# 🤖 RAG CLI Chat\n\nConnected to: [bold cyan]{self.repo}[/bold cyan]\nThread ID: [bold]{self.thread_id}[/bold]\n\nType your question or use `/help` for commands.\n        \"\"\"\n        console.print(Panel(Markdown(welcome), border_style=\"cyan\"))\n\n    def run(self):\n        \"\"\"Main chat loop.\"\"\"\n        self.show_welcome()\n\n        while True:\n            try:\n                # Get user input\n                user_input = Prompt.ask(\n                    f\"\\n[bold cyan]{self.repo}[/bold cyan] >\",\n                    default=\"\"\n                )\n\n                if not user_input.strip():\n                    continue\n\n                # Handle commands\n                if user_input.startswith('/'):\n                    cmd = user_input.lower().split()[0]\n\n                    if cmd in ['/exit', '/quit']:\n                        console.print(\"[yellow]Goodbye![/yellow]\")\n                        break\n\n                    elif cmd == '/help':\n                        self.show_help()\n                        continue\n\n                    elif cmd == '/repo':\n                        parts = user_input.split(maxsplit=1)\n                        if len(parts) > 1:\n                            self.switch_repo(parts[1].strip())\n                        else:\n                            console.print(\"[red]Usage:[/red] /repo <project|project>\")\n                        continue\n\n                    elif cmd == '/save':\n                        console.print(f\"[green]✓[/green] Checkpoint saved (thread: {self.thread_id})\")\n                        continue\n\n                    elif cmd == '/clear':\n                        # Create new thread ID to start fresh\n                        import time\n                        self.thread_id = f\"cli-chat-{int(time.time())}\"\n                        console.print(f\"[green]✓[/green] Cleared history (new thread: {self.thread_id})\")\n                        continue\n\n                    else:\n                        console.print(f\"[red]Unknown command:[/red] {cmd}\")\n                        console.print(\"Type [bold]/help[/bold] for available commands\")\n                        continue\n\n                # Ask question\n                console.print(\"[dim]Thinking...[/dim]\")\n                result = self.ask(user_input)\n\n                # Show answer\n                answer = self._format_answer(result.get('generation', ''))\n                confidence = result.get('confidence', 0.0)\n                docs = result.get('documents', [])\n\n                # Display answer in panel\n                console.print(\"\\n\")\n                console.print(Panel(\n                    Markdown(answer),\n                    title=f\"Answer (confidence: {confidence:.2f})\",\n                    border_style=\"green\" if confidence > 0.6 else \"yellow\"\n                ))\n\n                # Show top citations\n                if docs:\n                    console.print(\"\\n[dim]Top sources:[/dim]\")\n                    for i, doc in enumerate(docs[:3], 1):\n                        fp = doc.get('file_path', 'unknown')\n                        start = doc.get('start_line', 0)\n                        end = doc.get('end_line', 0)\n                        score = doc.get('rerank_score', 0.0)\n                        console.print(f\"  [dim]{i}.[/dim] {fp}:{start}-{end} [dim](score: {score:.3f})[/dim]\")\n\n            except KeyboardInterrupt:\n                console.print(\"\\n[yellow]Use /exit to quit[/yellow]\")\n                continue\n            except EOFError:\n                console.print(\"\\n[yellow]Goodbye![/yellow]\")\n                break\n            except Exception as e:\n                console.print(f\"[red]Error:[/red] {e}\")\n                continue"}
{"id":102,"text":"main():\n    \"\"\"Entry point.\"\"\"\n    # Check dependencies\n    try:\n        from rich.console import Console\n        from rich.markdown import Markdown\n        from rich.panel import Panel\n        from rich.prompt import Prompt\n    except ImportError:\n        print(\"Error: Missing 'rich' library. Install with: pip install rich\")\n        sys.exit(1)\n\n    # Get config from environment\n    repo = os.getenv('REPO', 'agro')\n    thread_id = os.getenv('THREAD_ID', 'cli-chat')\n\n    # Create and run chat\n    chat = ChatCLI(repo=repo, thread_id=thread_id)\n    chat.run()\n\n\nif __name__ == '__main__':\n    main()"}
{"id":103,"text":"from typing import Any, Dict, List, Optional, Tuple\n\nNumber = float\n\n_looks_local(model_id: Optional[str]) -> bool:\n    return bool(model_id) and (\":\" in model_id)\n\n_any_true(d: Dict[str, Any], keys: List[str]) -> bool:\n    return any(bool(d.get(k)) for k in keys)\n\n_safe_num(x: Any, default: Number = 0.0) -> Number:\n    try:\n        n = float(x)\n        if n != n:  # NaN\n            return default\n        return n\n    except Exception:\n        return default\n\n_normalize_workload(workload: Dict[str, Any]) -> Dict[str, Number]:\n    R = _safe_num(workload.get(\"requests_per_day\"), 0)\n    Tin = _safe_num(workload.get(\"tokens_in_per_req\"), 0)\n    Tout = _safe_num(workload.get(\"tokens_out_per_req\"), 0)\n    MQ = _safe_num(workload.get(\"mq_rewrites\"), 1)\n    E_tokens = _safe_num(workload.get(\"embed_tokens_per_req\"), Tin) * MQ\n    K_base = max(256.0, float(int(Tout // 4)))\n    K_tokens = _safe_num(workload.get(\"rerank_tokens_per_req\"), K_base) * MQ\n    return dict(R=R, Tin=Tin, Tout=Tout, MQ=MQ, E_tokens=E_tokens, K_tokens=K_tokens)"}
{"id":104,"text":"_weights(wl: Dict[str, Number]) -> Dict[str, Number]:\n    W_GEN = wl[\"R\"] * (wl[\"Tin\"] + wl[\"Tout\"])\n    W_EMB = wl[\"R\"] * wl[\"E_tokens\"]\n    W_RR = wl[\"R\"] * wl[\"K_tokens\"]\n    total = W_GEN + W_EMB + W_RR\n    if total <= 0:\n        return dict(Wg=1 / 3, We=1 / 3, Wr=1 / 3)\n    return dict(Wg=W_GEN / total, We=W_EMB / total, Wr=W_RR / total)\n\n_allowed_set(policy: Dict[str, Any]) -> set:\n    providers = policy.get(\"providers_allowed\") or []\n    return set([p.lower() for p in providers if isinstance(p, str)])\n\n_meets_policy_maps(candidate: Dict[str, Any], policy: Dict[str, Any]) -> bool:\n    regions_allowed = policy.get(\"regions_allowed\")\n    compliance = policy.get(\"compliance\")\n    for comp in (\"GEN\", \"EMB\", \"RERANK\"):\n        row = candidate.get(comp, {})\n        if not row:\n            return False\n        region = row.get(\"region\")\n        comp_flags = set(row.get(\"compliance\", []) or [])\n        if regions_allowed and region and region not in regions_allowed:\n            return False\n        if compliance and comp_flags and not comp_flags.issuperset(set(compliance)):\n            return False\n    return True"}
{"id":105,"text":"_decorate_row(m: Dict[str, Any], comp_type: str) -> Dict[str, Any]:\n    out = dict(m)\n    out[\"comp\"] = comp_type.upper()\n    out[\"provider\"] = (out.get(\"provider\") or \"\").lower()\n    qs = out.get(\"quality_score\")\n    if qs is None:\n        # Nudge defaults to prefer cloud rows in performance mode tie-breaks\n        # Non-local rows default slightly higher; local stubs slightly lower.\n        out[\"quality_score\"] = 0.55 if out[\"provider\"] != \"local\" else 0.45\n    else:\n        out[\"quality_score\"] = _safe_num(qs, 0.5)\n    if out.get(\"latency_p95_ms\") is not None:\n        out[\"latency_p95_ms\"] = _safe_num(out[\"latency_p95_ms\"], None)\n    if out.get(\"throughput_qps\") is not None:\n        out[\"throughput_qps\"] = _safe_num(out[\"throughput_qps\"], None)\n    return out"}
{"id":106,"text":"_component_rows(\n    comp_type: str,\n    ALLOW: set,\n    prices: Dict[str, Any],\n    include_local: bool = False,\n) -> List[Dict[str, Any]]:\n    rows: List[Dict[str, Any]] = []\n    models = prices.get(\"models\") or []\n    comp = comp_type.upper()\n\n    for m in models:\n        prov = (m.get(\"provider\") or \"\").lower()\n        if prov == \"local\":\n            continue\n        if ALLOW and prov not in ALLOW:\n            continue\n        unit = (m.get(\"unit\") or \"\")\n        if comp == \"GEN\":\n            if unit == \"1k_tokens\" and (\n                _safe_num(m.get(\"input_per_1k\")) > 0 or _safe_num(m.get(\"output_per_1k\")) > 0\n            ):\n                rows.append(_decorate_row(m, comp))\n        elif comp == \"EMB\":\n            if _safe_num(m.get(\"embed_per_1k\")) > 0:\n                rows.append(_decorate_row(m, comp))\n        elif comp == \"RERANK\":\n            if _safe_num(m.get(\"rerank_per_1k\")) > 0 or unit == \"request\":\n                rows.append(_decorate_row(m, comp))\n\n    if include_local and ((not ALLOW) or (\"local\" in ALLOW)):\n        local_stub = dict(\n            provider=\"local\",\n            model=\"local\",\n            unit=\"request\",\n            quality_score=0.5,\n            latency_p95_ms=None,\n            throughput_qps=None,\n        )\n        rows.insert(0, _decorate_row(local_stub, comp))\n\n    rows.sort(key=lambda r: r[\"quality_score\"], reverse=True)\n    cap = 4 if comp == \"GEN\" else 3\n    return rows[:cap]"}
{"id":107,"text":"_pair_limited(GENs, EMBs, RRs, limit: int = 60) -> List[Dict[str, Any]]:\n    out: List[Dict[str, Any]] = []\n    for g in GENs:\n        for e in EMBs:\n            for r in RRs:\n                out.append({\"GEN\": g, \"EMB\": e, \"RERANK\": r})\n                if len(out) >= limit:\n                    return out\n    return out\n\n_valid_pipeline(c: Dict[str, Any]) -> bool:\n    g = c.get(\"GEN\")\n    return bool(g and g.get(\"provider\") and g.get(\"model\"))\n\n_meets_slos(c: Dict[str, Any], slo: Dict[str, Any]) -> bool:\n    target_ms = slo.get(\"latency_target_ms\")\n    min_qps = slo.get(\"min_qps\")\n    if target_ms is None and min_qps is None:\n        return True\n    for comp in (\"GEN\", \"EMB\", \"RERANK\"):\n        row = c.get(comp, {})\n        if target_ms is not None and row.get(\"latency_p95_ms\") is not None:\n            if _safe_num(row.get(\"latency_p95_ms\")) > float(target_ms):\n                return False\n        if min_qps is not None and row.get(\"throughput_qps\") is not None:\n            if _safe_num(row.get(\"throughput_qps\")) < float(min_qps):\n                return False\n    return True"}
{"id":108,"text":"_monthly_cost(c: Dict[str, Any], wl: Dict[str, Number]) -> Number:\n    R = wl[\"R\"]\n    Tin = wl[\"Tin\"]\n    Tout = wl[\"Tout\"]\n    E_tokens = wl[\"E_tokens\"]\n    K_tokens = wl[\"K_tokens\"]\n    P = 30.0\n\n    def gen_cost(row):\n        if row.get(\"provider\") == \"local\":\n            return 0.0\n        inp = _safe_num(row.get(\"input_per_1k\"))\n        out = _safe_num(row.get(\"output_per_1k\"))\n        return (Tin / 1000.0) * inp + (Tout / 1000.0) * out\n\n    def emb_cost(row):\n        if row.get(\"provider\") == \"local\":\n            return 0.0\n        emb = _safe_num(row.get(\"embed_per_1k\"))\n        return (E_tokens / 1000.0) * emb\n\n    def rr_cost(row):\n        if row.get(\"provider\") == \"local\":\n            return 0.0\n        rrk = row.get(\"rerank_per_1k\")\n        if rrk is not None:\n            return (K_tokens / 1000.0) * _safe_num(rrk)\n        return _safe_num(row.get(\"per_request\"))\n\n    per_req = gen_cost(c[\"GEN\"]) + emb_cost(c[\"EMB\"]) + rr_cost(c[\"RERANK\"])\n    return per_req * R * P"}
{"id":109,"text":"_lat_bonus(lat_ms: Optional[Number], target_ms: Optional[Number], alpha=0.02, beta=0.05) -> Number:\n    if lat_ms is None or target_ms is None:\n        return 0.0\n    if lat_ms <= target_ms:\n        return alpha\n    return -beta * ((lat_ms - target_ms) / target_ms)\n\n_utility(c: Dict[str, Any], wl_w: Dict[str, Number], defaults: Dict[str, Any], slo: Dict[str, Any]) -> Number:\n    Qg = _safe_num(c[\"GEN\"].get(\"quality_score\"), 0.5)\n    Qe = _safe_num(c[\"EMB\"].get(\"quality_score\"), 0.5)\n    Qr = _safe_num(c[\"RERANK\"].get(\"quality_score\"), 0.5)\n    target_ms = slo.get(\"latency_target_ms\")\n    Lg = _lat_bonus(c[\"GEN\"].get(\"latency_p95_ms\"), target_ms)\n    Le = _lat_bonus(c[\"EMB\"].get(\"latency_p95_ms\"), target_ms)\n    Lr = _lat_bonus(c[\"RERANK\"].get(\"latency_p95_ms\"), target_ms)\n    U_gen = Qg + Lg\n    U_emb = Qe + Le\n    U_rr = Qr + Lr\n    U = wl_w[\"Wg\"] * U_gen + wl_w[\"We\"] * U_emb + wl_w[\"Wr\"] * U_rr\n    def_gen = defaults.get(\"gen_model\")\n    if def_gen and c[\"GEN\"].get(\"model\") == def_gen:\n        U += 0.01\n    return U"}
{"id":110,"text":"_select_cost(C: List[Dict[str, Any]], B: Optional[Number]) -> Dict[str, Any]:\n    if B is not None:\n        feasible = [c for c in C if c[\"monthly\"] <= B]\n        if feasible:\n            return min(feasible, key=lambda x: x[\"monthly\"])\n    return min(C, key=lambda x: x[\"monthly\"])\n\n_select_performance(C: List[Dict[str, Any]]) -> Dict[str, Any]:\n    # Maximize utility. If tie, prefer higher sum of component qualities; then tie-break by min cost.\n    bestU = max(c[\"utility\"] for c in C)\n    top = [c for c in C if c[\"utility\"] == bestU]\n    if len(top) <= 1:\n        return top[0]\n    def qsum(c: Dict[str, Any]) -> Number:\n        return _safe_num(c[\"GEN\"].get(\"quality_score\"), 0.0) + _safe_num(c[\"EMB\"].get(\"quality_score\"), 0.0) + _safe_num(c[\"RERANK\"].get(\"quality_score\"), 0.0)\n    bestQ = max(qsum(c) for c in top)\n    top2 = [c for c in top if qsum(c) == bestQ]\n    return min(top2, key=lambda x: x[\"monthly\"])"}
{"id":111,"text":"_select_balanced(C: List[Dict[str, Any]], B: Optional[Number]) -> Dict[str, Any]:\n    if B is not None:\n        feasible = [c for c in C if c[\"monthly\"] <= B]\n        if feasible:\n            bestU = max(c[\"utility\"] for c in feasible)\n            top = [c for c in feasible if c[\"utility\"] == bestU]\n            return min(top, key=lambda x: x[\"monthly\"])\n        lam = 1.0 / (B if B and B > 0 else 1.0)\n        def score(c):\n            return c[\"utility\"] - lam * (c[\"monthly\"] - B)\n\n        return max(C, key=score)\n    return _select_performance(C)"}
{"id":112,"text":"autoprofile(request: Dict[str, Any], prices: Dict[str, Any]) -> Tuple[Dict[str, str], Dict[str, Any]]:\n    hw = request.get(\"hardware\", {})\n    rt = hw.get(\"runtimes\", {}) or {}\n    policy = request.get(\"policy\", {}) or {}\n    wl = _normalize_workload(request.get(\"workload\", {}) or {})\n    obj = request.get(\"objective\", {}) or {}\n    defaults = request.get(\"defaults\", {}) or {}\n\n    ALLOW = _allowed_set(policy)\n    local_cap = _any_true(rt, [\"cuda\", \"ollama\", \"coreml\", \"openvino\", \"vpu\", \"npu\", \"mps\"])\n    B = obj.get(\"monthly_budget_usd\")\n    mode = (obj.get(\"mode\") or \"balanced\").lower()\n    slo = {\"latency_target_ms\": obj.get(\"latency_target_ms\"), \"min_qps\": obj.get(\"min_qps\")}\n\n    W = _weights(wl)\n\n    def build_candidates(AL: set) -> List[Dict[str, Any]]:\n        C: List[Dict[str, Any]] = []\n        if local_cap:\n            gen_local = defaults.get(\"gen_model\") if _looks_local(defaults.get(\"gen_model\")) else None\n            top_cloud_gen = _component_rows(\"GEN\", AL, prices, include_local=False)\n            GENs = [{\"provider\": \"local\", \"model\": gen_local}] if gen_local else top_cloud_gen\n            EMBs = _component_rows(\"EMB\", AL, prices, include_local=True)\n            RRs = _component_rows(\"RERANK\", AL, prices, include_local=True)\n            C.extend(_pair_limited(GENs, EMBs, RRs, limit=60))\n        GENs = _component_rows(\"GEN\", AL, prices, include_local=False)\n        EMBs = _component_rows(\"EMB\", AL, prices, include_local=local_cap)\n        RRs = _component_rows(\"RERANK\", AL, prices, include_local=local_cap)\n        C.extend(_pair_limited(GENs, EMBs, RRs, limit=60))\n        C = [c for c in C if _valid_pipeline(c)]\n        C = [c for c in C if _meets_slos(c, slo)]\n        try:\n            C = [c for c in C if _meets_policy_maps(c, policy)]\n        except Exception:\n            pass\n        return C\n\n    C = build_candidates(ALLOW)\n\n    # Fallback: if providers_allowed is non-empty and produced no candidates, relax provider filter once.\n    relaxed = False\n    if not C and ALLOW:\n        C = build_candidates(set())\n        relaxed = bool(C)\n\n    if not C:\n        return {}, {\"error\": \"no_viable_candidate\", \"why\": \"after building/filters\", \"providers_allowed\": list(ALLOW)}\n\n    for c in C:\n        c[\"monthly\"] = _monthly_cost(c, wl)\n        c[\"utility\"] = _utility(c, W, defaults, slo)\n\n    if mode == \"cost\":\n        winner = _select_cost(C, B)\n    elif mode == \"performance\":\n        winner = _select_performance(C)\n    else:\n        winner = _select_balanced(C, B)\n\n    env: Dict[str, str] = {\n        \"HYDRATION_MODE\": \"lazy\",\n        \"MQ_REWRITES\": str(int(wl[\"MQ\"]) if wl[\"MQ\"] > 0 else 1),\n        \"GEN_MODEL\": winner[\"GEN\"][\"model\"],\n        \"EMBEDDING_TYPE\": \"local\" if winner[\"EMB\"][\"provider\"] == \"local\" else winner[\"EMB\"][\"provider\"],\n        \"RERANK_BACKEND\": \"local\" if winner[\"RERANK\"][\"provider\"] == \"local\" else winner[\"RERANK\"][\"provider\"],\n    }\n    if env[\"RERANK_BACKEND\"] == \"cohere\":\n        env[\"COHERE_RERANK_MODEL\"] = winner[\"RERANK\"][\"model\"]\n\n    reason = {\n        \"objective\": mode,\n        \"budget\": B,\n        \"workload\": wl,\n        \"weights\": W,\n        \"selected\": {\n            \"gen\": winner[\"GEN\"],\n            \"embed\": winner[\"EMB\"],\n            \"rerank\": winner[\"RERANK\"],\n            \"monthly\": winner[\"monthly\"],\n            \"utility\": winner[\"utility\"],\n        },\n        \"policy_relaxed\": relaxed,\n    }\n    return env, reason"}
{"id":113,"text":"# coding: utf-8\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http.exceptions import UnexpectedResponse\n\n_orig_recreate = QdrantClient.recreate_collection\n\n_extract_args(*args, **kwargs):\n    name = kwargs.get(\"collection_name\")\n    vectors_config = kwargs.get(\"vectors_config\")\n    if name is None and args:\n        name = args[0]\n    if vectors_config is None and len(args) > 1:\n        vectors_config = args[1]\n    return name, vectors_config"}
{"id":114,"text":"recreate_collection_safe(self, *args, **kwargs):\n    try:\n        return _orig_recreate(self, *args, **kwargs)\n    except UnexpectedResponse as e:\n        # Some servers return 404 on delete step inside recreate\n        if getattr(e, \"status_code\", None) == 404:\n            name, vectors_config = _extract_args(*args, **kwargs)\n            return self.create_collection(collection_name=name, vectors_config=vectors_config)\n        raise\n    except Exception:\n        # Very defensive fallback: try delete (ignore errors), then create\n        name, vectors_config = _extract_args(*args, **kwargs)\n        try:\n            try:\n                self.delete_collection(name)\n            except Exception:\n                pass\n            return self.create_collection(collection_name=name, vectors_config=vectors_config)\n        except Exception:\n            raise\n\n\nQdrantClient.recreate_collection = recreate_collection_safe"}
{"id":115,"text":"import { defineConfig } from '@playwright/test';\n\nexport default defineConfig({\n  testDir: 'tests',\n  fullyParallel: true,\n  timeout: 60_000,\n  expect: { timeout: 10_000 },\n  use: {\n    baseURL: 'http://127.0.0.1:8012',\n    trace: 'on-first-retry',\n    screenshot: 'only-on-failure',\n    video: 'retain-on-failure'\n  },\n  webServer: {\n    command: \"bash -lc '. .venv/bin/activate && source scripts/select_index.sh shared || true; uvicorn serve_rag:app --host 127.0.0.1 --port 8012'\",\n    url: 'http://127.0.0.1:8012/health',\n    reuseExistingServer: true,\n    timeout: 120_000,\n    stdout: 'pipe',\n    stderr: 'pipe'\n  }\n});"}
{"id":116,"text":"import os\nimport json\nimport hashlib\nfrom typing import List, Dict\nfrom pathlib import Path\nfrom dotenv import load_dotenv, find_dotenv\nfrom config_loader import get_repo_paths, out_dir\nfrom ast_chunker import lang_from_path, collect_files, chunk_code\nimport bm25s\nfrom bm25s.tokenization import Tokenizer\nfrom Stemmer import Stemmer\nfrom qdrant_client import QdrantClient, models\nimport uuid\nfrom openai import OpenAI\nfrom embed_cache import EmbeddingCache\nimport tiktoken\nfrom sentence_transformers import SentenceTransformer\nimport fnmatch, pathlib\nimport qdrant_recreate_fallback  # make recreate_collection 404-safe\nfrom datetime import datetime\n\n# --- global safe filters (avoid indexing junk) ---\nfrom filtering import _prune_dirs_in_place, _should_index_file, PRUNE_DIRS\nimport os\nfrom pathlib import Path\n\n# Patch os.walk to prune noisy dirs and skip junk file types\n_os_walk = os.walk"}
{"id":117,"text":"_filtered_os_walk(top, *args, **kwargs):\n    for root, dirs, files in _os_walk(top, *args, **kwargs):\n        _prune_dirs_in_place(dirs)\n        files[:] = [f for f in files if _should_index_file(f)]\n        yield root, dirs, files\nos.walk = _filtered_os_walk\n\n# Patch Path.rglob as well (if code uses it)\n_Path_rglob = Path.rglob"}
{"id":118,"text":"_filtered_rglob(self, pattern):\n    for p in _Path_rglob(self, pattern):\n        # skip if any pruned dir appears in the path\n        if any(part in PRUNE_DIRS for part in p.parts):\n            continue\n        if not _should_index_file(p.name):\n            continue\n        yield p\nPath.rglob = _filtered_rglob\n# --- end filters ---\n\n\n# Load local env and also repo-root .env if present (no hard-coded paths)\ntry:\n    load_dotenv(override=False)\n    repo_root = Path(__file__).resolve().parent\n    env_path = repo_root / \".env\"\n    if env_path.exists():\n        load_dotenv(dotenv_path=env_path, override=False)\n    else:\n        alt = find_dotenv(usecwd=True)\n        if alt:\n            load_dotenv(dotenv_path=alt, override=False)\nexcept Exception:\n    pass\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\nif OPENAI_API_KEY and OPENAI_API_KEY.strip().upper() in {\"SK-REPLACE\", \"REPLACE\"}:\n    OPENAI_API_KEY = None\nQDRANT_URL = os.getenv('QDRANT_URL','http://127.0.0.1:6333')\n# Repo scoping\nREPO = os.getenv('REPO', 'project').strip()\n# Resolve repo paths and outdir from config (repos.json or env)\ntry:\n    BASES = get_repo_paths(REPO)\nexcept Exception:\n    # Fallback to current directory when no config present (best-effort)\n    BASES = [str(Path(__file__).resolve().parent)]\nOUTDIR = out_dir(REPO)\n# Allow explicit collection override (for versioned collections per embedding config)\nCOLLECTION = os.getenv('COLLECTION_NAME', f'code_chunks_{REPO}')\n\n\n# Centralized file indexing gate (extensions, excludes, heuristics)\nSOURCE_EXTS = {\n    \".py\", \".rb\", \".ts\", \".tsx\", \".js\", \".jsx\", \".go\", \".rs\", \".java\",\n    \".cs\", \".c\", \".h\", \".cpp\", \".hpp\", \".m\", \".mm\", \".kt\", \".kts\", \".swift\",\n    \".sql\", \".yml\", \".yaml\", \".toml\", \".ini\", \".json\", \".md\"\n}\nEXCLUDE_GLOBS_FILE = \"data/exclude_globs.txt\""}
{"id":119,"text":"_load_exclude_globs() -> list[str]:\n    p = pathlib.Path(EXCLUDE_GLOBS_FILE)\n    if not p.exists():\n        return []\n    return [ln.strip() for ln in p.read_text().splitlines() if ln.strip() and not ln.startswith(\"#\")]\n\n_EXCLUDE_GLOBS = _load_exclude_globs()\nshould_index_file(path: str) -> bool:\n    p = pathlib.Path(path)\n    # 1) fast deny: extension must look like source\n    if p.suffix.lower() not in SOURCE_EXTS:\n        return False\n    # 2) glob excludes (vendor, caches, images, minified, etc.)\n    as_posix = p.as_posix()\n    for pat in _EXCLUDE_GLOBS:\n        if fnmatch.fnmatch(as_posix, pat):\n            return False\n    # 3) quick heuristic to skip huge/minified one-liners\n    try:\n        text = p.read_text(errors=\"ignore\")\n        if len(text) > 2_000_000:  # ~2MB\n            return False\n        # suspect minified if average line length is enormous\n        lines = text.splitlines()\n        if lines:\n            avg = sum(len(x) for x in lines) / max(1, len(lines))\n            if avg > 2500:\n                return False\n    except Exception:\n        return False\n    return True\n\n\n# --- Repo-aware layer tagging ---"}
{"id":120,"text":"detect_layer(fp: str) -> str:\n    f = (fp or '').lower()\n    if REPO == 'project':\n        if '/core/admin_ui/' in f or '/site/' in f or '/docs-site/' in f:\n            return 'ui'\n        if '/plugins/' in f or '/core/plugins/' in f or 'notification' in f or 'pushover' in f or 'apprise' in f:\n            return 'plugin'\n        if '/core/api/' in f or '/core/' in f or '/server' in f:\n            return 'kernel'\n        if '/docs/' in f or '/internal_docs/' in f:\n            return 'docs'\n        if '/tests/' in f or '/test_' in f:\n            return 'tests'\n        if '/infra/' in f or '/deploy/' in f or '/scripts/' in f:\n            return 'infra'\n        return 'kernel'\n    else:\n        if '/admin_ui/' in f or '/site/' in f or '/docs-site/' in f:\n            return 'ui'\n        if 'provider' in f or 'providers' in f or 'integration' in f or 'webhook' in f or 'adapter' in f:\n            return 'integration'\n        if '/api/' in f or '/backends/' in f or '/server' in f:\n            return 'server'\n        if '/sdks/' in f or '/python_mcp/' in f or '/node_mcp/' in f or '/plugin-dev-kit/' in f:\n            return 'sdk'\n        if '/docs/' in f or '/internal_docs/' in f:\n            return 'docs'\n        if '/asterisk/' in f or '/config/' in f or '/infra/' in f:\n            return 'infra'\n        return 'server'\n\nVENDOR_MARKERS = (\n    \"/vendor/\",\"/third_party/\",\"/external/\",\"/deps/\",\"/node_modules/\",\n    \"/Pods/\",\"/Godeps/\",\"/.bundle/\",\"/bundle/\"\n)"}
{"id":121,"text":"detect_origin(fp: str) -> str:\n    low = (fp or '').lower()\n    for m in VENDOR_MARKERS:\n        if m in low:\n            return 'vendor'\n    try:\n        with open(fp, 'r', encoding='utf-8', errors='ignore') as f:\n            head = ''.join([next(f) for _ in range(12)])\n        if any(k in head.lower() for k in (\n            'apache license','mit license','bsd license','mozilla public license'\n        )):\n            return 'vendor'\n    except Exception:\n        pass\n    return 'first_party'\nos.makedirs(OUTDIR, exist_ok=True)\n_clip_for_openai(text: str, enc, max_tokens: int = 8000) -> str:\n    toks = enc.encode(text)\n    if len(toks) <= max_tokens:\n        return text\n    return enc.decode(toks[:max_tokens])\nembed_texts(client: OpenAI, texts: List[str], batch: int = 64) -> List[List[float]]:\n    # Legacy non-cached embedder (kept for compatibility if needed)\n    embs = []\n    enc = tiktoken.get_encoding('cl100k_base')\n    for i in range(0, len(texts), batch):\n        sub = [_clip_for_openai(t, enc) for t in texts[i:i+batch]]\n        r = client.embeddings.create(model='text-embedding-3-large', input=sub)\n        for d in r.data:\n            embs.append(d.embedding)\n    return embs"}
{"id":122,"text":"embed_texts_local(texts: List[str], model_name: str = 'BAAI/bge-small-en-v1.5', batch: int = 128) -> List[List[float]]:\n    model = SentenceTransformer(model_name)\n    out = []\n    for i in range(0, len(texts), batch):\n        sub = texts[i:i+batch]\n        v = model.encode(sub, normalize_embeddings=True, show_progress_bar=False)\n        out.extend(v.tolist())\n    return out\n_renorm_truncate(vecs: List[List[float]], dim: int) -> List[List[float]]:\n    out: List[List[float]] = []\n    import math as _m\n    for v in vecs:\n        w = v[:dim] if dim and dim < len(v) else v\n        # renormalize\n        n = _m.sqrt(sum(x*x for x in w)) or 1.0\n        out.append([x / n for x in w])\n    return out\nembed_texts_mxbai(texts: List[str], dim: int = 512, batch: int = 128) -> List[List[float]]:\n    model = SentenceTransformer('mixedbread-ai/mxbai-embed-large-v1')\n    out: List[List[float]] = []\n    for i in range(0, len(texts), batch):\n        sub = texts[i:i+batch]\n        v = model.encode(sub, normalize_embeddings=True, show_progress_bar=False)\n        out.extend(v.tolist())\n    return _renorm_truncate(out, dim)"}
{"id":123,"text":"embed_texts_voyage(texts: List[str], batch: int = 128, output_dimension: int = 512) -> List[List[float]]:\n    import voyageai\n    client = voyageai.Client(api_key=os.getenv('VOYAGE_API_KEY'))\n    out: List[List[float]] = []\n    for i in range(0, len(texts), batch):\n        sub = texts[i:i+batch]\n        r = client.embed(sub, model='voyage-code-3', input_type='document', output_dimension=output_dimension)\n        out.extend(r.embeddings)\n    return out"}
{"id":124,"text":"main() -> None:\n    files = collect_files(BASES)\n    print(f'Discovered {len(files)} source files.')\n    all_chunks: List[Dict] = []\n    for fp in files:\n        if not should_index_file(fp):\n            continue\n        lang = lang_from_path(fp)\n        if not lang:\n            continue\n        try:\n            with open(fp, 'r', encoding='utf-8', errors='ignore') as f:\n                src = f.read()\n        except Exception:\n            continue\n        ch = chunk_code(src, fp, lang, target=900)\n        all_chunks.extend(ch)\n\n    seen, chunks = set(), []\n    for c in all_chunks:\n        c['repo'] = REPO\n        try:\n            c['layer'] = detect_layer(c.get('file_path',''))\n        except Exception:\n            c['layer'] = 'server'\n        try:\n            c['origin'] = detect_origin(c.get('file_path',''))\n        except Exception:\n            c['origin'] = 'first_party'\n        h = hashlib.md5(c['code'].encode()).hexdigest()\n        if h in seen:\n            continue\n        seen.add(h)\n        c['hash'] = h\n        chunks.append(c)\n    print(f'Prepared {len(chunks)} chunks.')\n\n    # Optional enrichment using a local code LLM via Ollama\n    ENRICH = (os.getenv('ENRICH_CODE_CHUNKS', 'false') or 'false').lower() == 'true'\n    if ENRICH:\n        try:\n            from metadata_enricher import enrich  # type: ignore\n        except Exception:\n            enrich = None\n        if enrich is not None:\n            for c in chunks:\n                try:\n                    meta = enrich(c.get('file_path',''), c.get('language',''), c.get('code',''))\n                    c['summary'] = meta.get('summary','')\n                    c['keywords'] = meta.get('keywords', [])\n                except Exception:\n                    c['summary'] = ''\n                    c['keywords'] = []\n\n    # BM25S index\n    corpus: List[str] = []\n    for c in chunks:\n        pre = []\n        if c.get('name'):\n            pre += [c['name']]*2\n        if c.get('imports'):\n            pre += [i[0] or i[1] for i in c['imports'] if isinstance(i, (list, tuple))]\n        body = c['code']\n        corpus.append((' '.join(pre)+'\\n'+body).strip())\n\n    stemmer = Stemmer('english')\n    tokenizer = Tokenizer(stemmer=stemmer, stopwords='en')\n    corpus_tokens = tokenizer.tokenize(corpus)\n    retriever = bm25s.BM25(method='lucene', k1=1.2, b=0.65)\n    retriever.index(corpus_tokens)\n    os.makedirs(os.path.join(OUTDIR, 'bm25_index'), exist_ok=True)\n    # Workaround: ensure JSON-serializable vocab keys\n    try:\n        retriever.vocab_dict = {str(k): v for k, v in retriever.vocab_dict.items()}\n    except Exception:\n        pass\n    retriever.save(os.path.join(OUTDIR, 'bm25_index'), corpus=corpus)\n    tokenizer.save_vocab(save_dir=os.path.join(OUTDIR, 'bm25_index'))\n    tokenizer.save_stopwords(save_dir=os.path.join(OUTDIR, 'bm25_index'))\n    with open(os.path.join(OUTDIR, 'bm25_index', 'corpus.txt'), 'w', encoding='utf-8') as f:\n        for doc in corpus:\n            f.write(doc.replace('\\n','\\\\n')+'\\n')\n    # Persist a stable mapping from BM25 doc index -> chunk id\n    # Persist mapping from BM25 doc index -> chunk id (string)\n    chunk_ids = [str(c['id']) for c in chunks]\n    with open(os.path.join(OUTDIR, 'bm25_index', 'chunk_ids.txt'), 'w', encoding='utf-8') as f:\n        for cid in chunk_ids:\n            f.write(cid+'\\n')\n    # Also write a JSON map for convenience\n    import json as _json\n    _json.dump({str(i): cid for i, cid in enumerate(chunk_ids)}, open(os.path.join(OUTDIR,'bm25_index','bm25_map.json'),'w'))\n    with open(os.path.join(OUTDIR,'chunks.jsonl'),'w',encoding='utf-8') as f:\n        for c in chunks:\n            f.write(json.dumps(c, ensure_ascii=False)+'\\n')\n    print('BM25 index saved.')\n\n    # Persist a lightweight last-index metadata file for downstream consumers (GUI/CLI)\n    try:\n        meta = {\n            'repo': REPO,\n            'timestamp': datetime.utcnow().isoformat() + 'Z',\n            'chunks_path': os.path.join(OUTDIR, 'chunks.jsonl'),\n            'bm25_index_dir': os.path.join(OUTDIR, 'bm25_index'),\n            'chunk_count': len(chunks),\n            'collection_name': COLLECTION,\n        }\n        with open(os.path.join(OUTDIR, 'last_index.json'), 'w', encoding='utf-8') as mf:\n            json.dump(meta, mf, indent=2)\n    except Exception:\n        pass\n\n    # Optionally skip dense embeddings/Qdrant for fast local-only BM25 indexing\n    if (os.getenv('SKIP_DENSE','0') or '0').strip() == '1':\n        print('Skipping dense embeddings and Qdrant upsert (SKIP_DENSE=1).')\n        return\n\n    client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None\n    # Choose text for embedding (optionally enriched)\n    texts = []\n    for c in chunks:\n        if c.get('summary') or c.get('keywords'):\n            kw = ' '.join(c.get('keywords', []))\n            texts.append(f\"{c.get('file_path','') }\\n{c.get('summary','')}\\n{kw}\\n{c.get('code','')}\")\n        else:\n            texts.append(c['code'])\n    embs: List[List[float]] = []\n    et = (os.getenv('EMBEDDING_TYPE','openai') or 'openai').lower()\n    if et == 'voyage':\n        try:\n            embs = embed_texts_voyage(texts, batch=64, output_dimension=int(os.getenv('VOYAGE_EMBED_DIM','512')))\n        except Exception as e:\n            print(f\"Voyage embedding failed ({e}); falling back to local embeddings.\")\n            embs = []\n        if not embs:\n            embs = embed_texts_local(texts)\n    elif et == 'mxbai':\n        try:\n            dim = int(os.getenv('EMBEDDING_DIM', '512'))\n            embs = embed_texts_mxbai(texts, dim=dim)\n        except Exception as e:\n            print(f\"MXBAI embedding failed ({e}); falling back to local embeddings.\")\n            embs = embed_texts_local(texts)\n    elif et == 'local':\n        embs = embed_texts_local(texts)\n    else:\n        if client is not None:\n            try:\n                cache = EmbeddingCache(OUTDIR)\n                hashes = [c['hash'] for c in chunks]\n                embs = cache.embed_texts(client, texts, hashes, model='text-embedding-3-large', batch=64)\n                # Prune orphaned embeddings from deleted/changed files\n                pruned = cache.prune(set(hashes))\n                if pruned > 0:\n                    print(f'Pruned {pruned} orphaned embeddings from cache.')\n                cache.save()\n            except Exception as e:\n                print(f'Embedding via OpenAI failed ({e}); falling back to local embeddings.')\n        if not embs:\n            embs = embed_texts_local(texts)\n    point_ids: List[str] = []\n    try:\n        q = QdrantClient(url=QDRANT_URL)\n        q.recreate_collection(\n            collection_name=COLLECTION,\n            vectors_config={'dense': models.VectorParams(size=len(embs[0]), distance=models.Distance.COSINE)}\n        )\n        points = []\n        for c, v in zip(chunks, embs):\n            # Derive a stable UUID from the chunk id string to satisfy Qdrant (expects int or UUID)\n            cid = str(c['id'])\n            pid = str(uuid.uuid5(uuid.NAMESPACE_DNS, cid))\n            # Create slim payload without code (code is stored locally in chunks.jsonl)\n            slim_payload = {\n                'id': c.get('id'),\n                'file_path': c.get('file_path'),\n                'start_line': c.get('start_line'),\n                'end_line': c.get('end_line'),\n                'layer': c.get('layer'),\n                'repo': c.get('repo'),\n                'origin': c.get('origin'),\n                'hash': c.get('hash'),\n                'language': c.get('language')\n            }\n            # Remove None values to keep payload minimal\n            slim_payload = {k: v for k, v in slim_payload.items() if v is not None}\n            points.append(models.PointStruct(id=pid, vector={'dense': v}, payload=slim_payload))\n            point_ids.append(pid)\n            if len(points) == 64:\n                q.upsert(COLLECTION, points=points)\n                points = []\n        if points:\n            q.upsert(COLLECTION, points=points)\n        # Persist point id mapping aligned to BM25 corpus order\n        import json as _json\n        _json.dump({str(i): pid for i, pid in enumerate(point_ids)}, open(os.path.join(OUTDIR,'bm25_index','bm25_point_ids.json'),'w'))\n        print(f'Indexed {len(chunks)} chunks to Qdrant (embeddings: {len(embs[0])} dims).')\n        # Update last-index metadata to reflect completed dense upsert\n        try:\n            meta = {\n                'repo': REPO,\n                'timestamp': datetime.utcnow().isoformat() + 'Z',\n                'chunks_path': os.path.join(OUTDIR, 'chunks.jsonl'),\n                'bm25_index_dir': os.path.join(OUTDIR, 'bm25_index'),\n                'chunk_count': len(chunks),\n                'collection_name': COLLECTION,\n                'embedding_type': (os.getenv('EMBEDDING_TYPE','openai') or 'openai').lower(),\n                'embedding_dim': len(embs[0]) if embs and embs[0] else None,\n            }\n            with open(os.path.join(OUTDIR, 'last_index.json'), 'w', encoding='utf-8') as mf:\n                json.dump(meta, mf, indent=2)\n        except Exception:\n            pass\n    except Exception as e:\n        # Allow offline usage (BM25-only search) when Qdrant is unavailable\n        print(f\"Qdrant unavailable or failed to index ({e}); continuing with BM25-only index. Dense retrieval will be disabled.\")\n\nif __name__ == '__main__':\n    main()"}
{"id":125,"text":"import os, json\nimport tiktoken"}
{"id":126,"text":"EmbeddingCache:\n    def __init__(self, outdir: str):\n        os.makedirs(outdir, exist_ok=True)\n        self.path = os.path.join(outdir, \"embed_cache.jsonl\")\n        self.cache = {}\n        if os.path.exists(self.path):\n            with open(self.path, \"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    try:\n                        o = json.loads(line)\n                        self.cache[o[\"hash\"]] = o[\"vec\"]\n                    except Exception:\n                        pass\n\n    def get(self, h: str):\n        return self.cache.get(h)\n\n    def put(self, h: str, v):\n        self.cache[h] = v\n\n    def save(self):\n        with open(self.path, \"w\", encoding=\"utf-8\") as f:\n            for h, v in self.cache.items():\n                f.write(json.dumps({\"hash\": h, \"vec\": v}) + \"\\n\")\n\n    def prune(self, valid_hashes: set):\n        \"\"\"Remove cached embeddings for chunks that no longer exist.\n\n        Args:\n            valid_hashes: Set of hashes for chunks that currently exist\n\n        Returns:\n            Number of entries pruned\n        \"\"\"\n        before = len(self.cache)\n        self.cache = {h: v for h, v in self.cache.items() if h in valid_hashes}\n        after = len(self.cache)\n        pruned = before - after\n        if pruned > 0:\n            self.save()\n        return pruned\n\n    def embed_texts(self, client, texts, hashes, model=\"text-embedding-3-large\", batch=64):\n        embs = [None] * len(texts)\n        to_embed, idx_map = [], []\n        for i, (t, h) in enumerate(zip(texts, hashes)):\n            v = self.get(h)\n            if v is None:\n                idx_map.append(i)\n                to_embed.append(t)\n            else:\n                embs[i] = v\n        enc = tiktoken.get_encoding('cl100k_base')\n        def _clip_for_openai(text: str, max_tokens: int = 8000) -> str:\n            toks = enc.encode(text)\n            if len(toks) <= max_tokens:\n                return text\n            return enc.decode(toks[:max_tokens])\n        for i in range(0, len(to_embed), batch):\n            sub = [_clip_for_openai(t) for t in to_embed[i:i+batch]]\n            r = client.embeddings.create(model=model, input=sub)\n            for j, d in enumerate(r.data):\n                orig = idx_map[i + j]\n                vec = d.embedding\n                embs[orig] = vec\n                self.put(hashes[orig], vec)\n        return embs"}
{"id":127,"text":"#!/usr/bin/env python3\n\"\"\"\nMCP server exposing RAG tools for Codex/Claude integration.\nImplements Model Context Protocol via stdio.\n\nTools (sanitized names for OpenAI tool spec):\n  - rag_answer(repo, question) → full LangGraph answer + citations\n  - rag_search(repo, question) → retrieval-only (for debugging)\nCompatibility: accepts legacy names \"rag.answer\" and \"rag.search\" on tools/call.\n\"\"\"\nimport sys\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, Any, List\n\n# Ensure we can import from the same directory\nsys.path.insert(0, str(Path(__file__).parent))\n\nfrom langgraph_app import build_graph\nfrom hybrid_search import search_routed_multi\nimport urllib.request, urllib.error, urllib.parse\nimport json as _json\nfrom config_loader import list_repos"}
{"id":128,"text":"MCPServer:\n    \"\"\"Minimal MCP server over stdio.\"\"\"\n\n    def __init__(self):\n        self.graph = None\n        self._init_graph()\n\n    def _init_graph(self):\n        \"\"\"Lazy-load the LangGraph.\"\"\"\n        try:\n            self.graph = build_graph()\n        except Exception as e:\n            self._error(f\"Failed to initialize graph: {e}\")\n\n    def _error(self, msg: str):\n        \"\"\"Write error to stderr (MCP uses stdout for protocol).\"\"\"\n        print(f\"ERROR: {msg}\", file=sys.stderr)\n\n    def _log(self, msg: str):\n        \"\"\"Write log to stderr.\"\"\"\n        print(f\"LOG: {msg}\", file=sys.stderr)\n\n    def handle_rag_answer(self, repo: str, question: str) -> Dict[str, Any]:\n        \"\"\"\n        Execute full LangGraph pipeline: retrieval → generation → answer.\n        Returns: {answer: str, citations: List[str], repo: str}\n        \"\"\"\n        if not self.graph:\n            self._init_graph()\n\n        if not self.graph:\n            return {\n                \"error\": \"Graph not initialized\",\n                \"answer\": \"\",\n                \"citations\": [],\n                \"repo\": repo or \"unknown\"\n            }\n\n        try:\n            allowed = set(list_repos())\n            if repo not in allowed:\n                return {\"error\": f\"invalid repo '{repo}', allowed={sorted(allowed)}\", \"answer\": \"\", \"citations\": [], \"repo\": repo or \"unknown\"}\n            cfg = {\"configurable\": {\"thread_id\": f\"mcp-{repo or 'default'}\"}}\n            state = {\n                \"question\": question,\n                \"documents\": [],\n                \"generation\": \"\",\n                \"iteration\": 0,\n                \"confidence\": 0.0,\n                \"repo\": repo\n            }\n\n            result = self.graph.invoke(state, cfg)\n\n            # Extract citations from documents\n            docs = result.get(\"documents\", [])[:5]\n            citations = [\n                f\"{d['file_path']}:{d['start_line']}-{d['end_line']}\"\n                for d in docs\n            ]\n\n            return {\n                \"answer\": result.get(\"generation\", \"\"),\n                \"citations\": citations,\n                \"repo\": result.get(\"repo\", repo or \"unknown\"),\n                \"confidence\": float(result.get(\"confidence\", 0.0))\n            }\n        except Exception as e:\n            self._error(f\"rag.answer error: {e}\")\n            return {\n                \"error\": str(e),\n                \"answer\": \"\",\n                \"citations\": [],\n                \"repo\": repo or \"unknown\"\n            }\n\n    def handle_rag_search(self, repo: str, question: str, top_k: int = 10) -> Dict[str, Any]:\n        \"\"\"\n        Retrieval-only path for debugging.\n        Returns: {results: List[Dict], repo: str, count: int}\n        \"\"\"\n        try:\n            allowed = set(list_repos())\n            if repo not in allowed:\n                return {\"error\": f\"invalid repo '{repo}', allowed={sorted(allowed)}\", \"results\": [], \"repo\": repo or \"unknown\", \"count\": 0}\n            docs = search_routed_multi(\n                question,\n                repo_override=repo,\n                m=4,\n                final_k=top_k\n            )\n\n            # Return slim results (no code bodies for MCP transport)\n            results = [\n                {\n                    \"file_path\": d.get(\"file_path\", \"\"),\n                    \"start_line\": d.get(\"start_line\", 0),\n                    \"end_line\": d.get(\"end_line\", 0),\n                    \"language\": d.get(\"language\", \"\"),\n                    \"rerank_score\": float(d.get(\"rerank_score\", 0.0)),\n                    \"repo\": d.get(\"repo\", repo or \"unknown\")\n                }\n                for d in docs\n            ]\n\n            return {\n                \"results\": results,\n                \"repo\": repo or (results[0][\"repo\"] if results else \"unknown\"),\n                \"count\": len(results)\n            }\n        except Exception as e:\n            self._error(f\"rag.search error: {e}\")\n            return {\n                \"error\": str(e),\n                \"results\": [],\n                \"repo\": repo or \"unknown\",\n                \"count\": 0\n            }\n\n    # --- Netlify helpers ---\n    def _netlify_api(self, path: str, method: str = \"GET\", data: dict | None = None) -> dict:\n        api_key = os.getenv(\"NETLIFY_API_KEY\")\n        if not api_key:\n            raise RuntimeError(\"NETLIFY_API_KEY not set in environment\")\n        url = f\"https://api.netlify.com/api/v1{path}\"\n        req = urllib.request.Request(url, method=method)\n        req.add_header(\"Authorization\", f\"Bearer {api_key}\")\n        req.add_header(\"Content-Type\", \"application/json\")\n        body = None\n        if data is not None:\n            body = _json.dumps(data).encode(\"utf-8\")\n        try:\n            with urllib.request.urlopen(req, data=body, timeout=30) as resp:\n                raw = resp.read().decode(\"utf-8\")\n                return _json.loads(raw) if raw else {}\n        except urllib.error.HTTPError as he:\n            err_body = he.read().decode(\"utf-8\", errors=\"ignore\")\n            raise RuntimeError(f\"Netlify HTTP {he.code}: {err_body}\")\n\n    def _netlify_find_site_by_domain(self, domain: str) -> dict | None:\n        sites = self._netlify_api(\"/sites\", method=\"GET\")\n        if isinstance(sites, list):\n            domain_low = (domain or \"\").strip().lower()\n            for s in sites:\n                for key in (\"custom_domain\", \"url\", \"ssl_url\"):\n                    val = (s.get(key) or \"\").lower()\n                    if val and domain_low in val:\n                        return s\n        return None\n\n    def handle_netlify_deploy(self, domain: str) -> Dict[str, Any]:\n        targets: list[str]\n        if domain == \"both\":\n            targets = [\"project.net\", \"project.dev\"]\n        else:\n            targets = [domain]\n        results = []\n        for d in targets:\n            site = self._netlify_find_site_by_domain(d)\n            if not site:\n                results.append({\"domain\": d, \"status\": \"not_found\"})\n                continue\n            site_id = site.get(\"id\")\n            if not site_id:\n                results.append({\"domain\": d, \"status\": \"no_site_id\"})\n                continue\n            try:\n                build = self._netlify_api(f\"/sites/{site_id}/builds\", method=\"POST\", data={})\n                results.append({\n                    \"domain\": d,\n                    \"status\": \"triggered\",\n                    \"site_id\": site_id,\n                    \"build_id\": build.get(\"id\"),\n                })\n            except Exception as e:\n                results.append({\"domain\": d, \"status\": \"error\", \"error\": str(e)})\n        return {\"results\": results}\n\n    # --- Web tools (allowlisted) ---\n    _WEB_ALLOWED = {\"openai.com\", \"platform.openai.com\", \"github.com\", \"openai.github.io\"}\n\n    def _is_allowed_url(self, url: str) -> bool:\n        try:\n            u = urllib.parse.urlparse(url)\n            host = (u.netloc or \"\").lower()\n            # allow subdomains of allowed hosts\n            return any(host == h or host.endswith(\".\" + h) for h in self._WEB_ALLOWED)\n        except Exception:\n            return False\n\n    def handle_web_get(self, url: str, max_bytes: int = 20000) -> Dict[str, Any]:\n        if not (url or \"\").startswith(\"http\"):\n            return {\"error\": \"url must start with http(s)\"}\n        if not self._is_allowed_url(url):\n            return {\"error\": \"host not allowlisted\"}\n        req = urllib.request.Request(url, method=\"GET\", headers={\"User-Agent\": \"project-rag-mcp/1.0\"})\n        try:\n            with urllib.request.urlopen(req, timeout=20) as resp:\n                raw = resp.read(max_bytes + 1)\n                clipped = raw[:max_bytes]\n                return {\n                    \"url\": url,\n                    \"status\": resp.status,\n                    \"length\": len(raw),\n                    \"clipped\": len(raw) > len(clipped),\n                    \"content_preview\": clipped.decode(\"utf-8\", errors=\"ignore\")\n                }\n        except urllib.error.HTTPError as he:\n            body = he.read().decode(\"utf-8\", errors=\"ignore\")\n            return {\"url\": url, \"status\": he.code, \"error\": body[:1000]}\n        except Exception as e:\n            return {\"url\": url, \"error\": str(e)}\n\n    def handle_request(self, request: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        Handle MCP tool call request.\n\n        Request format:\n        {\n          \"jsonrpc\": \"2.0\",\n          \"id\": <request_id>,\n          \"method\": \"tools/call\",\n          \"params\": {\n            \"name\": \"rag.answer\" | \"rag.search\",\n            \"arguments\": {\n              \"repo\": \"project\" | \"project\",\n              \"question\": \"...\",\n              \"top_k\": 10  # optional, search only\n            }\n          }\n        }\n        \"\"\"\n        method = request.get(\"method\")\n        req_id = request.get(\"id\")\n\n        if method == \"tools/list\":\n            # Return available tools\n            repos = list_repos()\n            return {\n                \"jsonrpc\": \"2.0\",\n                \"id\": req_id,\n                \"result\": {\n                    \"tools\": [\n                        {\n                            \"name\": \"rag_answer\",\n                            \"description\": \"Get RAG answer with citations for a configured repo\",\n                            \"inputSchema\": {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                    \"repo\": {\n                                        \"type\": \"string\",\n                                        \"description\": \"Repository name\",\n                                        \"enum\": repos\n                                    },\n                                    \"question\": {\n                                        \"type\": \"string\",\n                                        \"description\": \"Developer question to answer from codebase\"\n                                    }\n                                },\n                                \"required\": [\"repo\", \"question\"]\n                            }\n                        },\n                        {\n                            \"name\": \"rag_search\",\n                            \"description\": \"Retrieval-only search (debugging) - returns relevant code locations without generation\",\n                            \"inputSchema\": {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                    \"repo\": {\n                                        \"type\": \"string\",\n                                        \"description\": \"Repository name\",\n                                        \"enum\": repos\n                                    },\n                                    \"question\": {\n                                        \"type\": \"string\",\n                                        \"description\": \"Search query for code retrieval\"\n                                    },\n                                    \"top_k\": {\n                                        \"type\": \"integer\",\n                                        \"description\": \"Number of results to return (default: 10)\",\n                                        \"default\": 10\n                                    }\n                                },\n                                \"required\": [\"repo\", \"question\"]\n                            }\n                        },\n                        {\n                            \"name\": \"netlify_deploy\",\n                            \"description\": \"Trigger a Netlify build for project.net, project.dev, or both (uses NETLIFY_API_KEY)\",\n                            \"inputSchema\": {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                    \"domain\": {\n                                        \"type\": \"string\",\n                                        \"description\": \"Target domain\",\n                                        \"enum\": [\"project.net\", \"project.dev\", \"both\"],\n                                        \"default\": \"both\"\n                                    }\n                                }\n                            }\n                        },\n                        {\n                            \"name\": \"web_get\",\n                            \"description\": \"HTTP GET (allowlisted hosts only: openai.com, platform.openai.com, github.com, openai.github.io)\",\n                            \"inputSchema\": {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                    \"url\": {\"type\": \"string\", \"description\": \"Absolute URL to fetch\"},\n                                    \"max_bytes\": {\"type\": \"integer\", \"description\": \"Max bytes to return\", \"default\": 20000}\n                                },\n                                \"required\": [\"url\"]\n                            }\n                        }\n                    ]\n                }\n            }\n\n        elif method == \"tools/call\":\n            params = request.get(\"params\", {})\n            tool_name = params.get(\"name\")\n            args = params.get(\"arguments\", {})\n\n            # Backward-compat: accept legacy dotted names\n            if tool_name in (\"rag.answer\", \"rag_answer\"):\n                result = self.handle_rag_answer(\n                    repo=args.get(\"repo\"),\n                    question=args.get(\"question\", \"\")\n                )\n                return {\n                    \"jsonrpc\": \"2.0\",\n                    \"id\": req_id,\n                    \"result\": {\"content\": [{\"type\": \"text\", \"text\": json.dumps(result, indent=2)}]}\n                }\n\n            elif tool_name in (\"rag.search\", \"rag_search\"):\n                result = self.handle_rag_search(\n                    repo=args.get(\"repo\"),\n                    question=args.get(\"question\", \"\"),\n                    top_k=args.get(\"top_k\", 10)\n                )\n                return {\n                    \"jsonrpc\": \"2.0\",\n                    \"id\": req_id,\n                    \"result\": {\"content\": [{\"type\": \"text\", \"text\": json.dumps(result, indent=2)}]}\n                }\n\n            elif tool_name in (\"netlify.deploy\", \"netlify_deploy\"):\n                domain = args.get(\"domain\", \"both\")\n                result = self.handle_netlify_deploy(domain)\n                return {\n                    \"jsonrpc\": \"2.0\",\n                    \"id\": req_id,\n                    \"result\": {\"content\": [{\"type\": \"text\", \"text\": json.dumps(result, indent=2)}]}\n                }\n\n            elif tool_name in (\"web.get\", \"web_get\"):\n                url = args.get(\"url\", \"\")\n                max_bytes = args.get(\"max_bytes\", 20000)\n                result = self.handle_web_get(url, max_bytes=max_bytes)\n                return {\n                    \"jsonrpc\": \"2.0\",\n                    \"id\": req_id,\n                    \"result\": {\"content\": [{\"type\": \"text\", \"text\": json.dumps(result, indent=2)}]}\n                }\n\n            else:\n                return {\n                    \"jsonrpc\": \"2.0\",\n                    \"id\": req_id,\n                    \"error\": {\n                        \"code\": -32601,\n                        \"message\": f\"Unknown tool: {tool_name}\"\n                    }\n                }\n\n        elif method == \"initialize\":\n            return {\n                \"jsonrpc\": \"2.0\",\n                \"id\": req_id,\n                \"result\": {\n                    \"protocolVersion\": \"2024-11-05\",\n                    \"capabilities\": {\n                        \"tools\": {}\n                    },\n                    \"serverInfo\": {\n                        \"name\": \"project-rag-mcp\",\n                        \"version\": \"1.0.0\"\n                    }\n                }\n            }\n\n        else:\n            return {\n                \"jsonrpc\": \"2.0\",\n                \"id\": req_id,\n                \"error\": {\n                    \"code\": -32601,\n                    \"message\": f\"Method not found: {method}\"\n                }\n            }\n\n    def run(self):\n        \"\"\"Main stdio loop.\"\"\"\n        self._log(\"MCP server starting (stdio mode)...\")\n\n        for line in sys.stdin:\n            line = line.strip()\n            if not line:\n                continue\n\n            try:\n                request = json.loads(line)\n                response = self.handle_request(request)\n                print(json.dumps(response), flush=True)\n            except json.JSONDecodeError as e:\n                self._error(f\"Invalid JSON: {e}\")\n                print(json.dumps({\n                    \"jsonrpc\": \"2.0\",\n                    \"id\": None,\n                    \"error\": {\n                        \"code\": -32700,\n                        \"message\": \"Parse error\"\n                    }\n                }), flush=True)\n            except Exception as e:\n                self._error(f\"Unexpected error: {e}\")\n                print(json.dumps({\n                    \"jsonrpc\": \"2.0\",\n                    \"id\": None,\n                    \"error\": {\n                        \"code\": -32603,\n                        \"message\": f\"Internal error: {e}\"\n                    }\n                }), flush=True)\n\n\nif __name__ == \"__main__\":\n    server = MCPServer()\n    server.run()"}
{"id":129,"text":"import { defineConfig, devices } from '@playwright/test';\n\nexport default defineConfig({\n  testDir: './tests',\n  fullyParallel: true,\n  forbidOnly: !!process.env.CI,\n  retries: process.env.CI ? 2 : 0,\n  workers: process.env.CI ? 1 : undefined,\n  reporter: 'html',\n\n  use: {\n    baseURL: 'http://127.0.0.1:8012',\n    trace: 'on-first-retry',\n    screenshot: 'only-on-failure',\n  },\n\n  projects: [\n    {\n      name: 'chromium',\n      use: { ...devices['Desktop Chrome'] },\n    },\n  ],\n\n  webServer: {\n    command: '.venv/bin/uvicorn serve_rag:app --host 127.0.0.1 --port 8012',\n    url: 'http://127.0.0.1:8012/health',\n    reuseExistingServer: true,\n    timeout: 120 * 1000,\n  },\n});"}
{"id":130,"text":"\"\"\"\nInteraction Tests for AGRO GUI\nTests user interactions and workflows\n\"\"\"\nimport pytest\nfrom playwright.sync_api import Page, expect\n\ntest_cost_calculator_interaction(page: Page):\n    \"\"\"Test cost calculator interaction\"\"\"\n    page.goto(\"http://127.0.0.1:8012/\")\n    \n    # Fill in cost calculator inputs\n    page.fill(\"#cost-in\", \"1000\")\n    page.fill(\"#cost-out\", \"2000\")\n    page.fill(\"#cost-rpd\", \"50\")\n    \n    # Click calculate button\n    page.click(\"#btn-estimate\")\n    \n    # Wait for response\n    page.wait_for_timeout(1000)\n    \n    # Check that results are displayed (not just dashes)\n    daily = page.locator(\"#cost-daily\")\n    expect(daily).not_to_contain_text(\"—\")"}
{"id":131,"text":"test_profile_save_interaction(page: Page):\n    \"\"\"Test saving a profile\"\"\"\n    page.goto(\"http://127.0.0.1:8012/\")\n    \n    # Enter profile name\n    test_profile_name = \"test-profile-playwright\"\n    page.fill(\"#profile-name\", test_profile_name)\n    \n    # Click save button\n    page.click(\"#btn-save-profile\")\n    \n    # Wait for save operation\n    page.wait_for_timeout(1000)\n    \n    # Verify profile appears in list (if profiles list is visible)\n    # This might show in an alert or list - adjust based on actual behavior\n\ntest_model_selection_changes(page: Page):\n    \"\"\"Test changing model selections\"\"\"\n    page.goto(\"http://127.0.0.1:8012/\")\n    \n    # Navigate to Models tab\n    page.click(\"button:has-text('Models')\")\n    \n    # Change primary model\n    gen_model_input = page.locator(\"input[name='GEN_MODEL']\")\n    expect(gen_model_input).to_be_visible()\n    gen_model_input.fill(\"gpt-4o\")\n    \n    # Verify input was changed\n    expect(gen_model_input).to_have_value(\"gpt-4o\")"}
{"id":132,"text":"test_retrieval_parameters_editable(page: Page):\n    \"\"\"Test that retrieval parameters can be edited\"\"\"\n    page.goto(\"http://127.0.0.1:8012/\")\n    \n    # Navigate to Retrieval tab\n    page.click(\"button:has-text('Retrieval')\")\n    \n    # Change Multi-Query Rewrites\n    mq_input = page.locator(\"input[name='MQ_REWRITES']\")\n    expect(mq_input).to_be_visible()\n    mq_input.fill(\"5\")\n    expect(mq_input).to_have_value(\"5\")\n    \n    # Change Final K\n    final_k_input = page.locator(\"input[name='FINAL_K']\")\n    expect(final_k_input).to_be_visible()\n    final_k_input.fill(\"20\")\n    expect(final_k_input).to_have_value(\"20\")"}
{"id":133,"text":"test_infrastructure_settings_visible(page: Page):\n    \"\"\"Test infrastructure settings are visible and editable\"\"\"\n    page.goto(\"http://127.0.0.1:8012/\")\n    \n    # Navigate to Infrastructure tab\n    page.click(\"button:has-text('Infrastructure')\")\n    \n    # Check Qdrant URL field\n    qdrant_input = page.locator(\"input[name='QDRANT_URL']\")\n    expect(qdrant_input).to_be_visible()\n    expect(qdrant_input).to_have_value(\"http://127.0.0.1:6333\")\n    \n    # Check Redis URL field\n    redis_input = page.locator(\"input[name='REDIS_URL']\")\n    expect(redis_input).to_be_visible()\n    expect(redis_input).to_have_value(\"redis://127.0.0.1:6379/0\")\n\ntest_auto_profile_budget_input(page: Page):\n    \"\"\"Test auto-profile budget input\"\"\"\n    page.goto(\"http://127.0.0.1:8012/\")\n    \n    # Should be on dashboard by default\n    budget_input = page.locator(\"#budget\")\n    expect(budget_input).to_be_visible()\n    \n    # Change budget value\n    budget_input.fill(\"100\")\n    expect(budget_input).to_have_value(\"100\")"}
{"id":134,"text":"test_git_hooks_status_check(page: Page):\n    \"\"\"Test git hooks status check\"\"\"\n    page.goto(\"http://127.0.0.1:8012/\")\n    \n    # Navigate to Tools tab\n    page.click(\"button:has-text('Tools')\")\n    \n    # Check hooks status is displayed\n    hooks_status = page.locator(\"#hooks-status\")\n    expect(hooks_status).to_be_visible()\n\ntest_wizard_oneclick_button(page: Page):\n    \"\"\"Test the wizard one-click configuration button\"\"\"\n    page.goto(\"http://127.0.0.1:8012/\")\n    \n    # Should be on dashboard\n    wizard_btn = page.locator(\"#btn-wizard-oneclick\")\n    expect(wizard_btn).to_be_visible()\n    expect(wizard_btn).to_contain_text(\"Configure Automatically\")"}
{"id":135,"text":"test_responsive_sidebar(page: Page):\n    \"\"\"Test that sidebar is visible and contains expected sections\"\"\"\n    page.goto(\"http://127.0.0.1:8012/\")\n    \n    # Check sidepanel is visible\n    expect(page.locator(\".sidepanel\")).to_be_visible()\n    \n    # Check all expected sections\n    sections = [\n        \"Live Cost Calculator\",\n        \"Profiles\",\n        \"Auto‑Tune\",\n        \"Secrets Ingest\"\n    ]\n    \n    for section in sections:\n        expect(page.locator(f\"h4:has-text('{section}')\")).to_be_visible()\n\ntest_secrets_dropzone_present(page: Page):\n    \"\"\"Test that secrets dropzone is present\"\"\"\n    page.goto(\"http://127.0.0.1:8012/\")\n    \n    dropzone = page.locator(\"#dropzone\")\n    expect(dropzone).to_be_visible()\n    expect(dropzone).to_contain_text(\"Drop .env\")"}
{"id":136,"text":"\"\"\"\nBasic GUI Tests for AGRO Configuration Interface\nTests core functionality and navigation\n\"\"\"\nimport pytest\nimport re\nfrom playwright.sync_api import Page, expect\n\ntest_health_endpoint_accessible(page: Page):\n    \"\"\"Test that the health endpoint is accessible\"\"\"\n    response = page.request.get(\"http://127.0.0.1:8012/health\")\n    assert response.ok\n    data = response.json()\n    assert data[\"status\"] == \"healthy\"\n    assert data[\"graph_loaded\"] == True\n\ntest_gui_loads_successfully(page: Page):\n    \"\"\"Test that the GUI main page loads\"\"\"\n    page.goto(\"http://127.0.0.1:8012/\")\n    expect(page).to_have_title(\"AGRO — Local Configuration GUI\")\n    \n    # Check that main elements are present\n    expect(page.locator(\".topbar\")).to_be_visible()\n    expect(page.locator(\".brand\")).to_contain_text(\"AGRO\")"}
{"id":137,"text":"test_tabs_are_present(page: Page):\n    \"\"\"Test that all major tabs are present\"\"\"\n    page.goto(\"http://127.0.0.1:8012/\")\n    \n    tabs = [\n        \"Dashboard\",\n        \"Models\",\n        \"Retrieval\",\n        \"Repos & Indexing\",\n        \"Infrastructure\",\n        \"Tools\"\n    ]\n    \n    for tab_name in tabs:\n        expect(page.locator(f\"button:has-text('{tab_name}')\")).to_be_visible()\n\ntest_tab_switching_works(page: Page):\n    \"\"\"Test that clicking tabs changes the active content\"\"\"\n    page.goto(\"http://127.0.0.1:8012/\")\n    \n    # Click Models tab\n    page.click(\"button:has-text('Models')\")\n    expect(page.locator(\"#tab-generation\")).to_be_visible()\n    expect(page.locator(\"button[data-tab='models']\")).to_have_class(re.compile(\"active\"))\n    \n    # Click Infrastructure tab\n    page.click(\"button:has-text('Infrastructure')\")\n    expect(page.locator(\"#tab-infra\")).to_be_visible()\n    expect(page.locator(\"button[data-tab='infra']\")).to_have_class(re.compile(\"active\"))"}
{"id":138,"text":"test_health_button_works(page: Page):\n    \"\"\"Test that the health check button works\"\"\"\n    page.goto(\"http://127.0.0.1:8012/\")\n    \n    # Click health button\n    page.click(\"#btn-health\")\n    \n    # Wait for status update\n    page.wait_for_timeout(500)\n    \n    # Check that health status is updated\n    health_status = page.locator(\"#health-status\")\n    expect(health_status).not_to_contain_text(\"—\")\n\ntest_dashboard_overview_visible(page: Page):\n    \"\"\"Test that dashboard overview section is visible\"\"\"\n    page.goto(\"http://127.0.0.1:8012/\")\n    \n    # Should be on dashboard by default\n    expect(page.locator(\"#tab-dashboard\")).to_be_visible()\n    expect(page.locator(\".settings-section.overview\")).to_be_visible()\n    expect(page.locator(\"#dash-health\")).to_be_visible()"}
{"id":139,"text":"test_cost_calculator_present(page: Page):\n    \"\"\"Test that the live cost calculator is present in sidebar\"\"\"\n    page.goto(\"http://127.0.0.1:8012/\")\n    \n    expect(page.locator(\".sidepanel\")).to_be_visible()\n    expect(page.locator(\"h4:has-text('Live Cost Calculator')\")).to_be_visible()\n    expect(page.locator(\"#cost-provider\")).to_be_visible()\n    expect(page.locator(\"#btn-estimate\")).to_be_visible()\n\ntest_profiles_section_visible(page: Page):\n    \"\"\"Test that profiles section is visible\"\"\"\n    page.goto(\"http://127.0.0.1:8012/\")\n    \n    expect(page.locator(\"h4:has-text('Profiles')\")).to_be_visible()\n    expect(page.locator(\"#profile-name\")).to_be_visible()\n    expect(page.locator(\"#btn-save-profile\")).to_be_visible()"}
{"id":140,"text":"test_global_search_present(page: Page):\n    \"\"\"Test that global search is present\"\"\"\n    page.goto(\"http://127.0.0.1:8012/\")\n    \n    search_box = page.locator(\"#global-search\")\n    expect(search_box).to_be_visible()\n    expect(search_box).to_have_attribute(\"placeholder\", \"Search settings (Ctrl+K)\")\n\ntest_apply_changes_button_present(page: Page):\n    \"\"\"Test that Apply All Changes button is present\"\"\"\n    page.goto(\"http://127.0.0.1:8012/\")\n    \n    expect(page.locator(\"#save-btn\")).to_be_visible()\n    expect(page.locator(\"#save-btn\")).to_contain_text(\"Apply All Changes\")\n\n\n@pytest.mark.parametrize(\"tab,content_id\", [\n    (\"models\", \"tab-generation\"),\n    (\"retrieval\", \"tab-retrieval\"),\n    (\"repos\", \"tab-repos\"),\n    (\"infra\", \"tab-infra\"),\n    (\"tools\", \"tab-tools\"),\n])test_tab_content_loads(page: Page, tab: str, content_id: str):\n    \"\"\"Parametrized test for tab content loading\"\"\"\n    page.goto(\"http://127.0.0.1:8012/\")\n    page.click(f\"button[data-tab='{tab}']\")\n    expect(page.locator(f\"#{content_id}\")).to_be_visible()"}
{"id":141,"text":"\"\"\"\nAPI Tests for AGRO Backend\nTests the FastAPI endpoints\n\"\"\"\nimport pytest\nfrom playwright.sync_api import Page, APIRequestContext\n\ntest_config_endpoint(page: Page):\n    \"\"\"Test /api/config endpoint\"\"\"\n    response = page.request.get(\"http://127.0.0.1:8012/api/config\")\n    assert response.ok\n    data = response.json()\n    assert \"env\" in data\n    assert \"repos\" in data\n    assert isinstance(data[\"env\"], dict)\n    assert isinstance(data[\"repos\"], list)\n\ntest_prices_endpoint(page: Page):\n    \"\"\"Test /api/prices endpoint\"\"\"\n    response = page.request.get(\"http://127.0.0.1:8012/api/prices\")\n    assert response.ok\n    data = response.json()\n    assert \"models\" in data\n    assert isinstance(data[\"models\"], list)\n\ntest_scan_hw_endpoint(page: Page):\n    \"\"\"Test /api/scan-hw endpoint\"\"\"\n    response = page.request.post(\"http://127.0.0.1:8012/api/scan-hw\")\n    assert response.ok\n    data = response.json()\n    assert \"info\" in data\n    assert \"runtimes\" in data\n    assert \"tools\" in data\n    assert \"os\" in data[\"info\"]\n    assert \"arch\" in data[\"info\"]"}
{"id":142,"text":"test_profiles_list_endpoint(page: Page):\n    \"\"\"Test /api/profiles endpoint\"\"\"\n    response = page.request.get(\"http://127.0.0.1:8012/api/profiles\")\n    assert response.ok\n    data = response.json()\n    assert \"profiles\" in data\n    assert isinstance(data[\"profiles\"], list)\n\ntest_git_hooks_status_endpoint(page: Page):\n    \"\"\"Test /api/git/hooks/status endpoint\"\"\"\n    response = page.request.get(\"http://127.0.0.1:8012/api/git/hooks/status\")\n    assert response.ok\n    data = response.json()\n    assert \"dir\" in data\n    assert \"post_checkout\" in data\n    assert \"post_commit\" in data\n\ntest_keywords_endpoint(page: Page):\n    \"\"\"Test /api/keywords endpoint\"\"\"\n    response = page.request.get(\"http://127.0.0.1:8012/api/keywords\")\n    assert response.ok\n    data = response.json()\n    assert \"keywords\" in data\n    assert isinstance(data[\"keywords\"], list)"}
{"id":143,"text":"test_search_endpoint_requires_query(page: Page):\n    \"\"\"Test /search endpoint validation\"\"\"\n    response = page.request.get(\"http://127.0.0.1:8012/search\")\n    # Should fail without query parameter\n    assert not response.ok\n\ntest_answer_endpoint_requires_query(page: Page):\n    \"\"\"Test /answer endpoint validation\"\"\"\n    response = page.request.get(\"http://127.0.0.1:8012/answer\")\n    # Should fail without query parameter\n    assert not response.ok"}
{"id":144,"text":"import { test, expect } from '@playwright/test';\n\ntest.describe('GUI smoke', () => {\n  test.beforeEach(async ({ page }) => {\n    await page.goto('/gui/');\n    await page.waitForSelector('.tab-bar');\n  });\n\n  test('dashboard renders and health shows OK', async ({ page }) => {\n    await expect(page.locator('#dash-health')).toBeVisible();\n    // Kick health\n    await page.waitForTimeout(200);\n  });\n\n  test('tab switching works', async ({ page }) => {\n    await page.getByRole('button', { name: 'Models' }).click();\n    await expect(page.locator('#tab-generation')).toBeVisible();\n    await page.getByRole('button', { name: 'Repos & Indexing' }).click();\n    await expect(page.locator('#tab-repos')).toBeVisible();\n  });\n\n  test('global search highlights', async ({ page }) => {\n    await page.fill('#global-search', 'Model');\n    await page.keyboard.press('Enter');\n    await page.waitForTimeout(200);\n    const marks = await page.locator('mark.hl').count();\n    expect(marks).toBeGreaterThan(0);\n  });\n\n  test('Git hooks install via Tools tab', async ({ page }) => {\n    await page.getByRole('button', { name: 'Tools' }).click();"}
{"id":145,"text":"await page.getByRole('button', { name: 'Install' }).click();\n    await page.waitForTimeout(200);\n    const status = await page.locator('#hooks-status').textContent();\n    expect(status || '').not.toContain('Not installed');\n  });\n\n  test('indexer quick action updates status', async ({ page }) => {\n    // Dashboard quick action\n    await page.getByRole('button', { name: 'Dashboard' }).click();\n    await page.locator('#dash-index-start').click();\n    await page.waitForTimeout(400);\n    const txt = await page.locator('#dash-index-status').textContent();\n    expect((txt || '')).toContain('Chunks');\n  });\n\n  test('wizard one-click config produces preview', async ({ page }) => {\n    await page.getByRole('button', { name: 'Dashboard' }).click();\n    await page.locator('#btn-wizard-oneclick').click();\n    await page.waitForTimeout(500);\n    const preview = await page.locator('#profile-preview').textContent();\n    expect((preview || '')).toContain('Models:');\n  });\n\n  test('cost calculator estimates include embeddings', async ({ page }) => {\n    await page.getByRole('button', { name: 'Tools' }).click();"}
{"id":146,"text":"// Switch embedding provider model\n    await page.fill('input[name=\"GEN_MODEL\"]', 'gpt-4o-mini');\n    await page.selectOption('#cost-embed-provider', { label: 'openai' }).catch(()=>{});\n    const embedModel = page.locator('#cost-embed-model');\n    if (await embedModel.count()) {\n      await embedModel.fill('text-embedding-3-small');\n    }\n    // set some numbers\n    await page.fill('#cost-in', '1000');\n    await page.fill('#cost-out', '1000');\n    await page.fill('#cost-embeds', '1000');\n    await page.fill('#cost-rerank', '1000');\n    await page.fill('#cost-rpd', '10');\n    // Re-run wizard generate which triggers cost preview underneath\n    await page.getByRole('button', { name: 'Dashboard' }).click();\n    await page.locator('#btn-wizard-oneclick').click();\n    await page.waitForTimeout(400);\n    const preview = await page.locator('#profile-preview').textContent();\n    expect((preview || '')).toMatch(/Cost Estimate|Daily|Monthly/);\n  });\n\n  test('profiles save and list updates', async ({ page }) => {\n    await page.getByRole('button', { name: 'Dashboard' }).click();"}
{"id":147,"text":"await page.fill('#profile-name', 'pw-ui');\n    const btn = page.locator('#btn-save-profile');\n    if (await btn.count()) {\n      await btn.click();\n      await page.waitForTimeout(200);\n      const ul = page.locator('#profiles-ul');\n      await expect(ul).toContainText(/pw-ui/);\n    }\n  });\n});"}
{"id":148,"text":"import { test, expect, request } from '@playwright/test';\n\ntest.describe('HTTP API', () => {\n  test('health returns healthy', async ({ request }) => {\n    const res = await request.get('/health');\n    expect(res.ok()).toBeTruthy();\n    const body = await res.json();\n    expect(body.status).toBe('healthy');\n  });\n\n  test('keywords returns arrays and countable union', async ({ request }) => {\n    const res = await request.get('/api/keywords');\n    expect(res.ok()).toBeTruthy();\n    const body = await res.json();\n    expect(Array.isArray(body.keywords)).toBeTruthy();\n    expect(Array.isArray(body.discriminative)).toBeTruthy();\n    expect(Array.isArray(body.semantic)).toBeTruthy();\n  });\n\n  test('prices exposes models incl. embeddings + rerank', async ({ request }) => {\n    const res = await request.get('/api/prices');\n    expect(res.ok()).toBeTruthy();\n    const body = await res.json();\n    const models = body.models as any[];\n    expect(models.some(m => m.model?.includes('text-embedding'))).toBeTruthy();\n    expect(models.some(m => (m.provider === 'cohere' && m.rerank_per_1k > 0))).toBeTruthy();"}
{"id":149,"text":"});\n\n  test('cost estimate counts gen + embed + rerank', async ({ request }) => {\n    const payload = {\n      gen_provider: 'openai', gen_model: 'gpt-4o-mini',\n      tokens_in: 1000, tokens_out: 1000, requests_per_day: 10,\n      embeds: 1000, embed_provider: 'openai', embed_model: 'text-embedding-3-small',\n      reranks: 1000, rerank_provider: 'cohere', rerank_model: 'rerank-3.5'\n    };\n    const res = await request.post('/api/cost/estimate', { data: payload });\n    expect(res.ok()).toBeTruthy();\n    const body = await res.json();\n    expect(body.daily).toBeGreaterThan(0);\n    expect(body.monthly).toBeGreaterThan(0);\n  });\n\n  test('profiles list/save/apply flows', async ({ request }) => {\n    const list0 = await (await request.get('/api/profiles')).json();\n    const save = await request.post('/api/profiles/save', { data: { name: 'pw-test', profile: { GEN_MODEL: 'gpt-4o-mini' } } });\n    expect(save.ok()).toBeTruthy();\n    const list1 = await (await request.get('/api/profiles')).json();\n    expect((list1.profiles as string[]).includes('pw-test')).toBeTruthy();\n    const apply = await request.post('/api/profiles/apply', { data: { profile: { MQ_REWRITES: '3' } } });"}
{"id":150,"text":"expect(apply.ok()).toBeTruthy();\n  });\n\n  test('git hooks install + status endpoints', async ({ request }) => {\n    const before = await (await request.get('/api/git/hooks/status')).json();\n    const res = await request.post('/api/git/hooks/install');\n    expect(res.ok()).toBeTruthy();\n    const body = await res.json();\n    expect((body.message || '')).toContain('Installed git hooks');\n    const after = await (await request.get('/api/git/hooks/status')).json();\n    expect(after.post_checkout).toBeTruthy();\n    expect(after.post_commit).toBeTruthy();\n  });\n});"}
{"id":151,"text":"\"\"\"Test auto-profile functionality\"\"\"\nimport pytest\nfrom playwright.sync_api import Page, expect\ntest_autoprofile_button_exists(page: Page):\n    \"\"\"Test that auto-profile button exists and is clickable\"\"\"\n    page.goto(\"http://127.0.0.1:8012/\")\n    \n    # Find the configure automatically button\n    button = page.locator(\"#btn-wizard-oneclick\")\n    expect(button).to_be_visible()\n    expect(button).to_contain_text(\"Configure Automatically\")\ntest_autoprofile_layout(page: Page):\n    \"\"\"Test that the 2-column layout is visible\"\"\"\n    page.goto(\"http://127.0.0.1:8012/\")\n    \n    # Check results panel exists\n    results_panel = page.locator(\"#profile-results-panel\")\n    expect(results_panel).to_be_visible()\n    \n    # Check placeholder is visible initially\n    placeholder = page.locator(\"#profile-placeholder\")\n    expect(placeholder).to_be_visible()"}
{"id":152,"text":"test_autoprofile_button_click(page: Page):\n    \"\"\"Test clicking the configure button\"\"\"\n    page.goto(\"http://127.0.0.1:8012/\")\n    \n    # Click the button\n    page.click(\"#btn-wizard-oneclick\")\n    \n    # Wait a moment for any async operations\n    page.wait_for_timeout(2000)\n    \n    # Check console for errors\n    console_messages = []\n    page.on(\"console\", lambda msg: console_messages.append(f\"{msg.type}: {msg.text}\"))\n    \n    # Try clicking again to capture console\n    page.reload()\n    page.click(\"#btn-wizard-oneclick\")\n    page.wait_for_timeout(2000)\n    \n    # Print console messages for debugging\n    print(\"\\n=== Console Messages ===\")\n    for msg in console_messages:\n        print(msg)"}
{"id":153,"text":"import express from 'express';\nimport fetch from 'node-fetch';\n\nconst app = express();\nconst PORT = process.env.PORT || 8014;\nconst RAG_API_URL = process.env.RAG_API_URL || 'http://127.0.0.1:8012';\n\napp.get('/health', (req, res) => {\n  res.json({ status: 'ok', proxy: true, target: RAG_API_URL });\n});\n\n// JSON answer proxy\napp.get('/mcp/answer', async (req, res) => {\n  try {\n    const { q, repo, token } = req.query;\n    const u = new URL('/answer', RAG_API_URL);\n    if (q) u.searchParams.set('q', q);\n    if (repo) u.searchParams.set('repo', repo);\n    const headers = token ? { Authorization: `Bearer ${token}` } : {};\n    const r = await fetch(u.toString(), { headers });\n    const data = await r.json();\n    res.json(data);\n  } catch (e) {\n    res.status(500).json({ error: String(e) });\n  }\n});\n\n// JSON search proxy\napp.get('/mcp/search', async (req, res) => {\n  try {\n    const { q, repo, top_k, token } = req.query;\n    const u = new URL('/search', RAG_API_URL);\n    if (q) u.searchParams.set('q', q);\n    if (repo) u.searchParams.set('repo', repo);\n    if (top_k) u.searchParams.set('top_k', String(top_k));"}
{"id":154,"text":"const headers = token ? { Authorization: `Bearer ${token}` } : {};\n    const r = await fetch(u.toString(), { headers });\n    const data = await r.json();\n    res.json(data);\n  } catch (e) {\n    res.status(500).json({ error: String(e) });\n  }\n});\n\n// SSE proxy for streaming answer\napp.get('/mcp/answer_stream', async (req, res) => {\n  try {\n    const { q, repo, token } = req.query;\n    const u = new URL('/answer_stream', RAG_API_URL);\n    if (q) u.searchParams.set('q', q);\n    if (repo) u.searchParams.set('repo', repo);\n    const headers = token ? { Authorization: `Bearer ${token}` } : {};\n\n    const r = await fetch(u.toString(), { headers });\n    res.setHeader('Content-Type', 'text/event-stream; charset=utf-8');\n    res.setHeader('Cache-Control', 'no-cache');\n    res.setHeader('X-Accel-Buffering', 'no');\n\n    if (!r.ok || !r.body) {\n      res.write(`data: [ERROR] upstream ${r.status}\\n\\n`);\n      return res.end();\n    }\n\n    const reader = r.body.getReader();\n    const decoder = new TextDecoder();\n    while (true) {\n      const { done, value } = await reader.read();\n      if (done) break;\n      res.write(decoder.decode(value));\n      // flush"}
{"id":155,"text":"}\n    res.end();\n  } catch (e) {\n    res.setHeader('Content-Type', 'text/event-stream; charset=utf-8');\n    res.write(`data: [ERROR] ${String(e)}\\n\\n`);\n    res.end();\n  }\n});\n\napp.listen(PORT, () => {\n  console.log(`Node proxy listening on :${PORT}, targeting ${RAG_API_URL}`);\n});"}
{"id":156,"text":"#!/usr/bin/env python3\nfrom __future__ import annotations\nfrom fastapi.testclient import TestClient\nimport io\nfrom pathlib import Path\nimport json, sys\nROOT = Path(__file__).resolve().parents[1]\nsys.path.insert(0, str(ROOT))\n# Provide a lightweight stub for rerankers to avoid import-time type errors\nimport types as _types\nif 'rerankers' not in sys.modules:\n    m = _types.ModuleType('rerankers')\n    class Reranker:  # minimal placeholder\n        def __init__(self, *a, **k):\n            pass\n    m.Reranker = Reranker\n    sys.modules['rerankers'] = m\nimport serve_rag"}
{"id":157,"text":"main() -> int:\n    app = serve_rag.app\n    c = TestClient(app)\n\n    # Prices\n    r = c.get('/api/prices')\n    assert r.status_code == 200, r.text\n    models = r.json().get('models', [])\n    print('prices models:', len(models))\n\n    # Upsert a model\n    r = c.post('/api/prices/upsert', json={\"provider\":\"local\",\"model\":\"qwen3-coder:14b\",\"unit\":\"request\"})\n    assert r.status_code == 200 and r.json().get('ok'), r.text\n\n    # Cost estimate\n    r = c.post('/api/cost/estimate', json={\"provider\":\"openai\",\"model\":\"gpt-4o-mini\",\"tokens_in\":500,\"tokens_out\":800,\"embeds\":0,\"reranks\":0,\"requests_per_day\":100})\n    assert r.status_code == 200, r.text\n    print('cost:', r.json().get('daily'), r.json().get('monthly'))\n\n    # Secrets ingest\n    buf = io.BytesIO(b\"OPENAI_API_KEY=sk-test-xyz\\nREPO=agro\\n\")\n    files = {\"file\": (\"tmp.env\", buf, \"text/plain\")}\n    data = {\"persist\": \"true\"}\n    r = c.post('/api/secrets/ingest', files=files, data=data)\n    assert r.status_code == 200, r.text\n    r = c.get('/api/config')\n    env = r.json().get('env', {})\n    assert env.get('OPENAI_API_KEY') == 'sk-test-xyz', env\n    print('env OPENAI_API_KEY:', env.get('OPENAI_API_KEY'))\n\n    # Autotune\n    r = c.get('/api/autotune/status')\n    assert r.status_code == 200\n    print('autotune:', r.json())\n\n    return 0\n\n\nif __name__ == '__main__':\n    raise SystemExit(main())"}
{"id":158,"text":"#!/usr/bin/env python3\n\"\"\"Quick token test for docs - measure actual usage\"\"\"\nimport os\nos.environ[\"OLLAMA_URL\"] = \"http://127.0.0.1:11434/api\"\nos.environ[\"GEN_MODEL\"] = \"qwen3-coder:30b\"\n\nimport sys\nROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\nsys.path.insert(0, ROOT_DIR)\n\ntry:\n    import tiktoken\n    enc = tiktoken.encoding_for_model(\"gpt-4o\")\n    def count_tokens(text): return len(enc.encode(text))\nexcept:\n    def count_tokens(text): return len(text) // 4\n\n# Test 1: Claude Alone (read full files)\nfrom pathlib import Path\nquestion = \"How are fax jobs created and dispatched\"\nkeywords = [\"fax\", \"jobs\", \"created\", \"dispatched\"]\nrepo_path = os.getenv('project_PATH', '/abs/path/to/project')\n\nfull_content = \"\"\nfor py_file in list(Path(repo_path).rglob('*.py'))[:10]:\n    try:\n        content = py_file.read_text(errors='ignore')\n        if any(kw in content.lower() for kw in keywords):\n            full_content += f\"\\n{'='*70}\\n{content}\\n\"\n    except:\n        pass\n\ntokens_claude_alone = count_tokens(full_content)\nprint(f\"1. Claude Alone: {tokens_claude_alone:,} tokens\")"}
{"id":159,"text":"# Test 2: MCP metadata only (simulate what Claude Code gets)\nmcp_response = \"\"\"{\"results\": [\n  {\"file_path\": \"server.py\", \"start_line\": 120, \"end_line\": 145, \"score\": 0.89},\n  {\"file_path\": \"tasks.py\", \"start_line\": 67, \"end_line\": 89, \"score\": 0.85},\n  {\"file_path\": \"models.py\", \"start_line\": 234, \"end_line\": 267, \"score\": 0.78}\n], \"count\": 3}\"\"\"\n\n# Tool schema (sent with every request)\ntool_schema = \"\"\"{\"tools\": [{\"name\": \"rag_search\", \"description\": \"Search codebase\", \"inputSchema\": {...}}]}\"\"\"\n\ntokens_mcp = count_tokens(mcp_response + tool_schema)\nprint(f\"2. Claude + RAG via MCP: {tokens_mcp:,} tokens\")\n\n# Calculate savings\nsaved = tokens_claude_alone - tokens_mcp\npct = (saved / tokens_claude_alone * 100) if tokens_claude_alone > 0 else 0\nreduction = tokens_claude_alone / tokens_mcp if tokens_mcp > 0 else 0\n\nprint(f\"\\nSavings: {saved:,} tokens ({pct:.1f}%)\")\nprint(f\"Reduction: {reduction:.1f}x\")\n\n# Cost (gpt-4o: $2.50/1M input)\ncost_alone = tokens_claude_alone * (2.50 / 1_000_000)\ncost_mcp = tokens_mcp * (2.50 / 1_000_000)"}
{"id":160,"text":"cost_saved = cost_alone - cost_mcp\n\nprint(f\"\\nPer query: ${cost_saved:.6f} saved\")\nprint(f\"Per 100 queries: ${cost_saved * 100:.2f} saved\")\nprint(f\"Per month (100/day): ${cost_saved * 3000:.2f} saved\")"}
{"id":161,"text":"#!/usr/bin/env python3\nimport os, sys, json, urllib.request, urllib.error\n\nAPI = \"https://api.netlify.com/api/v1\"\napi(path: str, method: str = \"GET\", data: dict | None = None) -> dict:\n    token = os.getenv(\"NETLIFY_API_KEY\")\n    if not token:\n        print(\"NETLIFY_API_KEY not set\", file=sys.stderr)\n        sys.exit(2)\n    url = f\"{API}{path}\"\n    req = urllib.request.Request(url, method=method)\n    req.add_header(\"Authorization\", f\"Bearer {token}\")\n    req.add_header(\"Content-Type\", \"application/json\")\n    body = json.dumps(data).encode(\"utf-8\") if data is not None else None\n    with urllib.request.urlopen(req, data=body, timeout=30) as resp:\n        raw = resp.read().decode(\"utf-8\")\n        return json.loads(raw) if raw else {}\nfind_site(domain: str) -> dict | None:\n    sites = api(\"/sites\", \"GET\")\n    dom = (domain or \"\").strip().lower()\n    if isinstance(sites, list):\n        for s in sites:\n            for key in (\"custom_domain\", \"url\", \"ssl_url\"):\n                val = (s.get(key) or \"\").lower()\n                if val and dom in val:\n                    return s\n    return None"}
{"id":162,"text":"trigger(domain: str) -> dict:\n    s = find_site(domain)\n    if not s:\n        return {\"domain\": domain, \"status\": \"not_found\"}\n    sid = s.get(\"id\")\n    if not sid:\n        return {\"domain\": domain, \"status\": \"no_site_id\"}\n    try:\n        b = api(f\"/sites/{sid}/builds\", \"POST\", {})\n        return {\"domain\": domain, \"status\": \"triggered\", \"site_id\": sid, \"build_id\": b.get(\"id\")}\n    except Exception as e:\n        return {\"domain\": domain, \"status\": \"error\", \"error\": str(e)}"}
{"id":163,"text":"main():\n    if len(sys.argv) < 2:\n        print(\"Usage: netlify_deploy.py [project.net|project.dev|both|list]\", file=sys.stderr)\n        sys.exit(2)\n    cmd = sys.argv[1].strip().lower()\n    if cmd == \"list\":\n        sites = api(\"/sites\", \"GET\")\n        out = []\n        for s in sites if isinstance(sites, list) else []:\n            out.append({\"id\": s.get(\"id\"), \"name\": s.get(\"name\"), \"url\": s.get(\"url\"), \"custom_domain\": s.get(\"custom_domain\")})\n        print(json.dumps(out, indent=2))\n        return\n    domains = [\"project.net\", \"project.dev\"] if cmd == \"both\" else [cmd]\n    results = [trigger(d) for d in domains]\n    print(json.dumps(results, indent=2))\n\nif __name__ == \"__main__\":\n    main()"}
{"id":164,"text":"#!/usr/bin/env python3\nimport os, sys, re\n\nSCAN_ALL = os.getenv(\"SCAN_ALL\", \"0\").lower() in {\"1\",\"true\",\"yes\"}\nROOTS = [os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))]\nif SCAN_ALL:\n    ROOTS += [\n        os.getenv('PROJECT_PATH', '/abs/path/to/project'),\n        os.getenv('project_PATH', '/abs/path/to/project'),\n    ]\n\nBAD_PATTERNS = [\n    r\"\\bChatCompletion\\b\",\n    r\"\\bclient\\.chat\\.completions\\b\",\n    r\"\\bassistants?\\.v1\\b\",\n    r\"\\bgpt-3\\.5\\b\",\n    r\"\\bgpt-4(?!\\.1|o)\\b\",\n    r\"\\bgpt-4o(-mini)?\\b\",\n    r\"\\btext-embedding-ada\\b\",\n    r\"\\btext-embedding-00[23]\\b\",\n]\nALLOWLIST_FILES = {\n    # add filenames you want ignored (e.g., historical docs)\n}\n\nSKIP_DIRS = {\".git\", \".venv\", \"venv\", \"node_modules\", \"dist\", \"build\", \"vendor\", \"third_party\", \"site-packages\", \"__pycache__\"}\n\nscan_file(path: str) -> list[str]:\n    try:\n        with open(path, \"r\", errors=\"ignore\") as f:\n            s = f.read()\n    except Exception:\n        return []\n    hits = []\n    for pat in BAD_PATTERNS:\n        if re.search(pat, s):\n            hits.append(pat)\n    return hits"}
{"id":165,"text":"main() -> int:\n    offenders = []\n    for root in ROOTS:\n        if not os.path.isdir(root):\n            continue\n        for base, dirs, files in os.walk(root):\n            # prune skip dirs\n            dirs[:] = [d for d in dirs if d not in SKIP_DIRS and not d.startswith('.')]\n            for name in files:\n                if name in ALLOWLIST_FILES:\n                    continue\n                # Scan code files only (skip docs like .md)\n                if not any(name.endswith(ext) for ext in (\".py\", \".ts\", \".tsx\", \".js\", \".rb\")):\n                    continue\n                path = os.path.join(base, name)\n                # skip this guard file and sitecustomize self-detection\n                if path.endswith(\"scripts/guard_legacy_api.py\") or path.endswith(\"sitecustomize.py\"):\n                    continue\n                hits = scan_file(path)\n                if hits:\n                    offenders.append((path, hits))\n    if offenders:\n        print(\"\\u274c Legacy APIs/models detected:\")\n        for p, pats in offenders:\n            print(f\"- {p}\")\n            for pat in pats:\n                print(f\"    \\u21b3 {pat}\")\n        print(\"\\nAction: replace Chat Completions with Responses API calls; update model pins (e.g., gpt-4o-mini-latest or a dated pin).\")\n        print(\"Docs:\")\n        print(\"  https://openai.com/index/new-tools-and-features-in-the-responses-api/\")\n        print(\"  https://openai.com/index/introducing-upgrades-to-codex/\")\n        return 2\n    print(\"\\u2713 No legacy APIs/models detected.\")\n    return 0\n\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())"}
{"id":166,"text":"#!/usr/bin/env python3\n\"\"\"Measure MCP tool schema overhead - the part sent on EVERY request\"\"\"\nimport sys, os\nimport json\nROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\nsys.path.insert(0, ROOT_DIR)\n\ntry:\n    import tiktoken\n    enc = tiktoken.encoding_for_model(\"gpt-4o\")\n    def count_tokens(text): return len(enc.encode(text))\nexcept:\n    def count_tokens(text): return len(text) // 4\n\n# Get tool schemas\nfrom mcp_server import MCPServer\nserver = MCPServer()\ntools_req = {'jsonrpc': '2.0', 'id': 1, 'method': 'tools/list', 'params': {}}\ntools_resp = server.handle_request(tools_req)\ntools_json = json.dumps(tools_resp['result']['tools'], indent=2)\n\nschema_tokens = count_tokens(tools_json)\n\nprint(\"=\" * 80)\nprint(\"MCP TOOL SCHEMA OVERHEAD (sent with EVERY Claude Code request)\")\nprint(\"=\" * 80)\nprint(f\"Schema tokens: {schema_tokens:,}\")\nprint(f\"Schema size: {len(tools_json):,} bytes\")\nprint(f\"\\nThis overhead is ADDED to every single request.\")\nprint(f\"Even if MCP response is small, you always pay for the tool schemas.\\n\")"}
{"id":167,"text":"# Show the actual schema\nprint(\"Tool schemas:\")\nfor tool in tools_resp['result']['tools']:\n    print(f\"  - {tool['name']}: {len(json.dumps(tool)):,} bytes\")\n\nwith open('/tmp/mcp_schema.json', 'w') as f:\n    f.write(tools_json)\nprint(f\"\\nFull schema saved to: /tmp/mcp_schema.json\")"}
{"id":168,"text":"#!/usr/bin/env python3\n\"\"\"\nCompare token usage across three approaches:\n1. Claude alone (no RAG) - reads full files\n2. RAG via direct Python calls (hybrid_search.py)\n3. RAG via MCP tools (what Claude Code uses)\n\nThis shows actual token savings from using RAG.\n\"\"\"\n\nimport sys\nimport os\nimport json\nfrom pathlib import Path\n\n# Try tiktoken for precise counts\ntry:\n    import tiktoken\n    HAS_TIKTOKEN = True\n    print(\"✓ Using tiktoken for precise token counts\\n\")\nexcept ImportError:\n    HAS_TIKTOKEN = False\n    print(\"⚠️  tiktoken not installed - using estimates (1 token ≈ 4 chars)\")\n    print(\"   Install: pip install tiktoken\\n\")\n\ncount_tokens(text: str, model: str = \"gpt-4o\") -> int:\n    \"\"\"Count tokens precisely or estimate\"\"\"\n    if HAS_TIKTOKEN:\n        try:\n            encoding = tiktoken.encoding_for_model(model)\n            return len(encoding.encode(text))\n        except:\n            pass\n    return len(text) // 4\n\n\n# ============================================================\n# Approach 1: Claude Alone (Traditional - NO RAG)\n# ============================================================"}
{"id":169,"text":"measure_claude_alone(question: str, repo: str):\n    \"\"\"\n    Simulate what Claude would do WITHOUT RAG:\n    - Extract keywords from question\n    - Grep files for those keywords\n    - Read 5-10 full files\n    \"\"\"\n    repo_paths = {\n        'project': os.getenv('PROJECT_PATH', '/abs/path/to/project'),\n        'faxbot': os.getenv('FAXBOT_PATH', '/abs/path/to/faxbot')\n    }\n\n    repo_path = repo_paths.get(repo)\n    if not repo_path or not os.path.exists(repo_path):\n        return {'error': f'Repo not found: {repo}'}\n\n    # Extract keywords (what Claude would search for)\n    keywords = [w.lower() for w in question.split() if len(w) > 3][:5]\n\n    # Find matching files\n    matched_files = []\n    combined_text = \"\"\n\n    for py_file in Path(repo_path).rglob('*.py'):\n        # Skip vendor/node_modules\n        if any(skip in str(py_file) for skip in ['node_modules', '.venv', 'vendor', '.git']):\n            continue\n\n        try:\n            content = py_file.read_text(errors='ignore')\n\n            # If keywords match, Claude would read this ENTIRE file\n            if any(kw in content.lower() for kw in keywords):\n                matched_files.append(str(py_file))\n                combined_text += f\"\\n{'='*70}\\n{py_file}\\n{'='*70}\\n{content}\\n\"\n\n                if len(matched_files) >= 10:  # Limit to 10 files\n                    break\n        except:\n            pass\n\n    tokens = count_tokens(combined_text)\n\n    return {\n        'approach': 'Claude Alone (no RAG)',\n        'files_read': len(matched_files),\n        'chars': len(combined_text),\n        'tokens': tokens,\n        'files': matched_files[:5]  # Show first 5\n    }\n\n\n# ============================================================\n# Approach 2: RAG via Direct Python\n# ============================================================"}
{"id":170,"text":"measure_rag_python(question: str, repo: str, top_k: int = 10):\n    \"\"\"Use hybrid_search.py directly (local Python calls)\"\"\"\n    try:\n        from hybrid_search import search_routed_multi\n\n        results = search_routed_multi(question, repo_override=repo, final_k=top_k)\n\n        # Combine retrieved chunks\n        combined_text = \"\"\n        for r in results:\n            combined_text += f\"{r['file_path']}:{r['start_line']}-{r['end_line']}\\n\"\n            combined_text += r.get('code', '') + \"\\n\\n\"\n\n        tokens = count_tokens(combined_text)\n\n        return {\n            'approach': 'RAG (direct Python)',\n            'chunks': len(results),\n            'chars': len(combined_text),\n            'tokens': tokens,\n            'files_touched': len(set(r['file_path'] for r in results)),\n            'top_scores': [r['rerank_score'] for r in results[:3]]\n        }\n    except Exception as e:\n        return {'error': str(e)}\n\n\n# ============================================================\n# Approach 3: RAG via MCP (What Claude Code Uses)\n# ============================================================"}
{"id":171,"text":"measure_rag_mcp(question: str, repo: str, top_k: int = 10):\n    \"\"\"\n    Simulate MCP tool call (what Claude Code actually uses).\n    This calls the same backend as direct Python but through MCP layer.\n    \"\"\"\n    try:\n        from mcp_server import MCPServer\n\n        # Call rag_search tool\n        req = {\n            'jsonrpc': '2.0',\n            'id': 1,\n            'method': 'tools/call',\n            'params': {\n                'name': 'rag_search',\n                'arguments': {\n                    'repo': repo,\n                    'question': question,\n                    'top_k': top_k\n                }\n            }\n        }\n\n        server = MCPServer()\n        resp = server.handle_request(req)\n\n        # Extract results\n        result_text = resp['result']['content'][0]['text']\n        result_data = json.loads(result_text)\n\n        # MCP returns file paths + line ranges (no full code in the response)\n        # But we need to count what gets sent to Claude\n        combined_text = result_text  # This is what Claude receives\n\n        tokens = count_tokens(combined_text)\n\n        return {\n            'approach': 'RAG (via MCP tools)',\n            'chunks': result_data.get('count', 0),\n            'chars': len(combined_text),\n            'tokens': tokens,\n            'mcp_result_size': len(result_text)\n        }\n    except Exception as e:\n        return {'error': str(e)}\n\n\n# ============================================================\n# Run Comparison\n# ============================================================"}
{"id":172,"text":"run_test(question: str, repo: str):\n    \"\"\"Run all three approaches and compare\"\"\"\n    print(f\"\\n{'='*70}\")\n    print(f\"TEST: {question}\")\n    print(f\"REPO: {repo}\")\n    print(f\"{'='*70}\\n\")\n\n    # Method 1: Claude alone\n    print(\"⏳ Measuring: Claude Alone (traditional grep + read files)...\")\n    claude_alone = measure_claude_alone(question, repo)\n\n    # Method 2: RAG Python\n    print(\"⏳ Measuring: RAG via Direct Python...\")\n    rag_python = measure_rag_python(question, repo, top_k=10)\n\n    # Method 3: RAG MCP\n    print(\"⏳ Measuring: RAG via MCP tools...\")\n    rag_mcp = measure_rag_mcp(question, repo, top_k=10)\n\n    # Print results\n    print(f\"\\n{'='*70}\")\n    print(\"RESULTS:\")\n    print(f\"{'='*70}\\n\")\n\n    # Claude Alone\n    if 'error' not in claude_alone:\n        print(f\"1️⃣  CLAUDE ALONE (no RAG):\")\n        print(f\"   Files read: {claude_alone['files_read']}\")\n        print(f\"   Total tokens: {claude_alone['tokens']:,}\")\n        print(f\"   Characters: {claude_alone['chars']:,}\")\n\n    # RAG Python\n    if 'error' not in rag_python:\n        print(f\"\\n2️⃣  RAG (Direct Python):\")\n        print(f\"   Chunks retrieved: {rag_python['chunks']}\")\n        print(f\"   Files touched: {rag_python['files_touched']}\")\n        print(f\"   Total tokens: {rag_python['tokens']:,}\")\n        print(f\"   Top scores: {[f'{s:.3f}' for s in rag_python.get('top_scores', [])]}\")\n\n    # RAG MCP\n    if 'error' not in rag_mcp:\n        print(f\"\\n3️⃣  RAG (via MCP - what Claude Code uses):\")\n        print(f\"   Chunks retrieved: {rag_mcp['chunks']}\")\n        print(f\"   Total tokens: {rag_mcp['tokens']:,}\")\n\n    # Calculate savings\n    if all('error' not in r for r in [claude_alone, rag_python, rag_mcp]):\n        alone_tokens = claude_alone['tokens']\n        python_tokens = rag_python['tokens']\n        mcp_tokens = rag_mcp['tokens']\n\n        print(f\"\\n{'='*70}\")\n        print(\"💰 TOKEN SAVINGS:\")\n        print(f\"{'='*70}\")\n\n        # Python vs Alone\n        saved_python = alone_tokens - python_tokens\n        pct_python = (saved_python / alone_tokens * 100) if alone_tokens > 0 else 0\n\n        print(f\"\\nRAG Python vs Claude Alone:\")\n        print(f\"   Tokens saved: {saved_python:,}\")\n        print(f\"   Percentage: {pct_python:.1f}%\")\n        print(f\"   Reduction: {alone_tokens / max(python_tokens, 1):.1f}x smaller\")\n\n        # MCP vs Alone\n        saved_mcp = alone_tokens - mcp_tokens\n        pct_mcp = (saved_mcp / alone_tokens * 100) if alone_tokens > 0 else 0\n\n        print(f\"\\nRAG MCP vs Claude Alone:\")\n        print(f\"   Tokens saved: {saved_mcp:,}\")\n        print(f\"   Percentage: {pct_mcp:.1f}%\")\n        print(f\"   Reduction: {alone_tokens / max(mcp_tokens, 1):.1f}x smaller\")\n\n        # Cost estimate (gpt-4o: $2.50/1M input tokens)\n        cost_per_token = 2.50 / 1_000_000\n\n        print(f\"\\n💵 COST SAVINGS (gpt-4o @ $2.50/1M input tokens):\")\n        print(f\"   Per query (Python): ${saved_python * cost_per_token:.6f}\")\n        print(f\"   Per 1000 queries (Python): ${saved_python * cost_per_token * 1000:.2f}\")\n        print(f\"   Per query (MCP): ${saved_mcp * cost_per_token:.6f}\")\n        print(f\"   Per 1000 queries (MCP): ${saved_mcp * cost_per_token * 1000:.2f}\")\n\n    return {\n        'question': question,\n        'repo': repo,\n        'claude_alone': claude_alone.get('tokens', 0),\n        'rag_python': rag_python.get('tokens', 0),\n        'rag_mcp': rag_mcp.get('tokens', 0)\n    }\n\n\nif __name__ == '__main__':\n    # Test cases\n    tests = [\n        (\"Where is OAuth token validated\", \"project\"),\n        (\"How are fax jobs created and dispatched\", \"project\"),\n        (\"EventStream component event types in dropdown\", \"project\"),\n    ]\n\n    results = []\n\n    for question, repo in tests:\n        try:\n            result = run_test(question, repo)\n            results.append(result)\n        except Exception as e:\n            print(f\"\\n❌ Error: {e}\")\n\n    # Overall summary\n    if results:\n        print(f\"\\n\\n{'='*70}\")\n        print(\"📊 OVERALL SUMMARY\")\n        print(f\"{'='*70}\\n\")\n\n        total_alone = sum(r['claude_alone'] for r in results)\n        total_python = sum(r['rag_python'] for r in results)\n        total_mcp = sum(r['rag_mcp'] for r in results)\n\n        print(f\"Total queries: {len(results)}\")\n        print(f\"\\nClaude Alone: {total_alone:,} tokens\")\n        print(f\"RAG Python: {total_python:,} tokens\")\n        print(f\"RAG MCP: {total_mcp:,} tokens\")\n\n        if total_alone > 0:\n            print(f\"\\nAverage reduction (Python): {total_alone / max(total_python, 1):.1f}x\")\n            print(f\"Average reduction (MCP): {total_alone / max(total_mcp, 1):.1f}x\")\n\n            saved_python = total_alone - total_python\n            saved_mcp = total_alone - total_mcp\n\n            print(f\"\\nTotal saved (Python): {saved_python:,} tokens ({saved_python/total_alone*100:.1f}%)\")\n            print(f\"Total saved (MCP): {saved_mcp:,} tokens ({saved_mcp/total_alone*100:.1f}%)\")"}
{"id":173,"text":"#!/usr/bin/env python3\nimport sys, json, re\n\n\"\"\"\nUsage:\n  python scripts/eval_gate_guard.py <answers.jsonl>\n\nWhere each line is a JSON object containing:\n  {\"q\": \"...\", \"repo\": \"project\", \"answer\": \"...\"}\nThis fails if the answer lacks a [repo: ...] header or no file path-like citation.\n\"\"\"\n\nHEADER_RE = re.compile(r\"^\\[repo:\\s*(project|project)\\]\", re.I | re.M)\nPATH_RE = re.compile(r\"[A-Za-z0-9_\\-./]+?\\.[A-Za-z0-9_]+:\\d+-\\d+\")\n\nok(answer: str) -> bool:\n    if not HEADER_RE.search(answer or \"\"):\n        return False\n    if not PATH_RE.search(answer or \"\"):\n        return False\n    return True"}
{"id":174,"text":"main():\n    if len(sys.argv) < 2:\n        print(\"usage: python scripts/eval_gate_guard.py <answers.jsonl>\")\n        sys.exit(2)\n    bad = 0\n    with open(sys.argv[1], \"r\", errors=\"ignore\") as f:\n        for i, line in enumerate(f, 1):\n            line = line.strip()\n            if not line:\n                continue\n            try:\n                obj = json.loads(line)\n            except Exception:\n                print(f\"line {i}: not json\")\n                bad += 1\n                continue\n            ans = obj.get(\"answer\", \"\")\n            if not ok(ans):\n                print(f\"line {i}: FAIL (missing repo header or file citation)\")\n                bad += 1\n    if bad:\n        print(f\"\\u274c guard failed: {bad} bad answer(s)\")\n        sys.exit(3)\n    print(\"\\u2713 guard passed\")\n    sys.exit(0)\n\n\nif __name__ == \"__main__\":\n    main()"}
{"id":175,"text":"import json\nimport os\nimport re\nfrom collections import Counter, defaultdict\nfrom pathlib import Path\nshould_skip_directory(path):\n    \"\"\"Skip vendor/dependency directories\"\"\"\n    skip_patterns = [\n        'node_modules', '.venv', 'venv', '__pycache__', \n        '.git', 'dist', 'build', 'vendor', 'tmp',\n        'test', 'tests', 'spec', 'specs',  # test files\n        'migrations', 'db/migrate',  # migrations\n        'locale', 'locales', 'i18n',  # translations\n        '.bundle', 'coverage', '.pytest_cache'\n    ]\n    return any(skip in path for skip in skip_patterns)"}
{"id":176,"text":"extract_semantic_terms(file_path, code):\n    \"\"\"Extract meaningful business/domain terms\"\"\"\n    terms = set()\n    \n    # 1. Extract from file/directory names (most semantic!)\n    path_parts = file_path.split('/')\n    for part in path_parts:\n        # Clean up: UserController.rb -> user, controller\n        cleaned = re.sub(r'[._-]', ' ', part)\n        words = re.findall(r'[A-Z][a-z]+|[a-z]+', cleaned)\n        terms.update(w.lower() for w in words if len(w) > 3)\n    \n    # 2. Extract class names (PascalCase)\n    class_names = re.findall(r'\\bclass ([A-Z][a-zA-Z0-9_]+)', code)\n    for name in class_names:\n        # Split camelCase: AIStudioComponent -> ai, studio, component\n        words = re.findall(r'[A-Z][a-z]+|[A-Z]+(?=[A-Z]|$)', name)\n        terms.update(w.lower() for w in words if len(w) > 2)\n    \n    # 3. Extract function names (meaningful ones only)\n    func_names = re.findall(r'\\b(?:def|function|const)\\s+([a-z][a-zA-Z0-9_]+)', code)\n    for name in func_names:\n        # Only keep multi-word functions: validate_oauth not just get\n        if '_' in name:\n            words = name.split('_')\n            terms.update(w for w in words if len(w) > 3)\n    \n    # 4. Extract from comments (gold mine!)\n    comments = re.findall(r'(?:#|//|/\\*|\\*)\\s*(.+)', code)\n    for comment in comments:\n        # Extract capitalized words (likely domain terms)\n        words = re.findall(r'\\b[A-Z][a-z]{2,}\\b', comment)\n        terms.update(w.lower() for w in words)\n    \n    # 5. Extract string literals (API endpoints, routes, etc)\n    strings = re.findall(r'[\"\\']([^\"\\']{5,50})[\"\\']', code)\n    for s in strings:\n        if '/' in s:  # likely a route\n            parts = s.split('/')\n            terms.update(p.lower() for p in parts if p.isalpha() and len(p) > 3)\n    \n    # Filter out programming keywords\n    stop_words = {\n        'return', 'function', 'class', 'const', 'import', 'export',\n        'from', 'self', 'this', 'super', 'none', 'null', 'true', 'false',\n        'async', 'await', 'yield', 'raise', 'assert', 'break', 'continue',\n        'string', 'number', 'boolean', 'object', 'array', 'type', 'interface',\n        'params', 'args', 'kwargs', 'options', 'config', 'props', 'state'\n    }\n    \n    return {t for t in terms if t not in stop_words and t.isalpha()}"}
{"id":177,"text":"analyze_repo_semantic(repo_path, repo_name):\n    \"\"\"Find meaningful business domain terms\"\"\"\n    term_counts = Counter()\n    term_files = defaultdict(set)\n    directory_terms = Counter()\n    \n    total_files = 0\n    \n    for root, dirs, files in os.walk(repo_path):\n        # Skip vendor directories\n        if should_skip_directory(root):\n            continue\n        \n        # Remove skippable dirs from traversal\n        dirs[:] = [d for d in dirs if not should_skip_directory(os.path.join(root, d))]\n        \n        # Analyze directory name itself\n        dir_name = os.path.basename(root)\n        if dir_name and len(dir_name) > 3:\n            directory_terms[dir_name.lower()] += 1\n        \n        for file in files:\n            # Only source code files\n            if not any(file.endswith(ext) for ext in ['.py', '.js', '.ts', '.tsx', '.rb', '.yml', '.java']):\n                continue\n            \n            file_path = os.path.join(root, file)\n            rel_path = os.path.relpath(file_path, repo_path)\n            \n            try:\n                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n                    code = f.read()\n                    \n                terms = extract_semantic_terms(rel_path, code)\n                \n                for term in terms:\n                    term_counts[term] += 1\n                    term_files[term].add(rel_path)\n                \n                total_files += 1\n            except:\n                continue\n    \n    # Calculate relevance scores\n    scored_terms = []\n    for term, count in term_counts.items():\n        file_count = len(term_files[term])\n        \n        # Score formula:\n        # - Appears in multiple files (2-20% of codebase) = domain term\n        # - Too rare (1 file) = noise\n        # - Too common (>20% files) = generic utility\n        if file_count >= 2 and file_count <= total_files * 0.2:\n            # Boost if term appears in directory names (very semantic)\n            dir_boost = 2.0 if term in directory_terms else 1.0\n            \n            # Calculate domain specificity score\n            score = (count * file_count * dir_boost) / (total_files + 1)\n            \n            scored_terms.append({\n                'term': term,\n                'score': score,\n                'files': file_count,\n                'mentions': count,\n                'in_directories': term in directory_terms,\n                'sample_files': list(term_files[term])[:3]\n            })\n    \n    # Sort by score\n    scored_terms.sort(key=lambda x: x['score'], reverse=True)\n    \n    return scored_terms, total_files, directory_terms\n\nif __name__ == '__main__':\n    repos = {\n        'project': os.getenv('PROJECT_PATH', '/abs/path/to/project'),\n        'faxbot': os.getenv('FAXBOT_PATH', '/abs/path/to/faxbot')\n    }\n    \n    all_results = {}\n    \n    for repo_name, repo_path in repos.items():\n        print(f'\\n{\"=\"*80}')\n        print(f'SEMANTIC ANALYSIS: {repo_name}')\n        print(f'{\"=\"*80}')\n        \n        terms, total_files, directories = analyze_repo_semantic(repo_path, repo_name)\n        all_results[repo_name] = terms[:50]\n        \n        print(f'\\nAnalyzed {total_files} files')\n        print(f'Found {len(terms)} meaningful domain terms')\n        print(f'\\nTop 30 Business/Domain Keywords:\\n')\n        \n        for i, t in enumerate(terms[:30], 1):\n            dir_marker = '📁' if t['in_directories'] else '  '\n            print(f'{i:2}. {dir_marker} {t[\"term\"]:20} | Score: {t[\"score\"]:8.1f} | {t[\"files\"]:3} files | {t[\"mentions\"]:4} mentions')\n        \n        # Show sample context\n        print(f'\\n📄 Sample file locations for top terms:')\n        for t in terms[:5]:\n            print(f'\\n  {t[\"term\"]}:')\n            for f in t['sample_files']:\n                print(f'    - {f}')\n    \n    # Cross-analysis\n    print(f'\\n{\"=\"*80}')\n    print('CROSS-REPO COMPARISON')\n    print(f'{\"=\"*80}')\n    \n    viv_terms = {t['term'] for t in all_results['project'][:30]}\n    fax_terms = {t['term'] for t in all_results['project'][:30]}\n    \n    shared = viv_terms & fax_terms\n    viv_only = viv_terms - fax_terms\n    fax_only = fax_terms - viv_terms\n    \n    print(f'\\n🔄 Shared terms ({len(shared)}):')\n    if shared:\n        print(f'   {\", \".join(sorted(shared)[:10])}')\n    \n    print(f'\\n💊 PROJECT-specific ({len(viv_only)}):')\n    print(f'   {\", \".join(sorted(list(viv_only)[:15]))}')\n    \n    print(f'\\n📠 PROJECT-specific ({len(fax_only)}):')\n    print(f'   {\", \".join(sorted(list(fax_only)[:15]))}')\n    \n    # Generate suggested queries\n    print(f'\\n{\"=\"*80}')\n    print('SUGGESTED EVAL QUERIES (based on actual terms)')\n    print(f'{\"=\"*80}')\n    \n    for repo_name, terms in all_results.items():\n        print(f'\\n{repo_name.upper()}:')\n        top_terms = terms[:10]\n        \n        # Generate natural queries\n        queries = []\n        for t in top_terms[:5]:\n            queries.append(f'  - \"Where is {t[\"term\"]} implemented?\"')\n            if t['in_directories']:\n                queries.append(f'  - \"How does {t[\"term\"]} work?\"')\n        \n        for q in queries[:8]:\n            print(q)\n    \n    # Save\n    with open('semantic_keywords.json', 'w') as f:\n        json.dump(all_results, f, indent=2)\n    \n    print(f'\\n✓ Saved to semantic_keywords.json')"}
{"id":178,"text":"#!/usr/bin/env python3\n\"\"\"\nCompare token usage across FOUR approaches:\n\n1. Claude Alone (no RAG) - reads full files via grep\n2. RAG CLI Standalone - RAG answers directly (no Claude)\n3. Claude + RAG Direct - Claude gets full code chunks from RAG\n4. Claude + RAG via MCP - Claude gets MCP metadata responses\n\nShows actual tokens sent to LLM in each scenario.\n\"\"\"\n\nimport sys\nimport os\nROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\nsys.path.insert(0, ROOT_DIR)\nimport json\nfrom pathlib import Path\n\n# Try tiktoken for precise counts\ntry:\n    import tiktoken\n    HAS_TIKTOKEN = True\nexcept ImportError:\n    HAS_TIKTOKEN = False\n    print(\"⚠️  Install tiktoken for precise counts: pip install tiktoken\\n\")"}
{"id":179,"text":"count_tokens(text: str, model: str = \"gpt-4o\") -> int:\n    \"\"\"Count tokens precisely or estimate\"\"\"\n    if HAS_TIKTOKEN:\n        try:\n            encoding = tiktoken.encoding_for_model(model)\n            return len(encoding.encode(text))\n        except:\n            pass\n    return len(text) // 4\n\n\n# ============================================================\n# Approach 1: Claude Alone (Traditional - NO RAG)\n# ============================================================"}
{"id":180,"text":"approach1_claude_alone(question: str, repo: str):\n    \"\"\"\n    Claude without RAG:\n    - Extract keywords\n    - Grep files\n    - Read 5-10 FULL files\n    \"\"\"\n    repo_paths = {\n        'project': os.getenv('PROJECT_PATH', '/abs/path/to/project'),\n        'project': os.getenv('project_PATH', '/abs/path/to/project')\n    }\n\n    repo_path = repo_paths.get(repo)\n    if not repo_path or not os.path.exists(repo_path):\n        return {'error': f'Repo not found: {repo_path}'}\n\n    # Keywords from question\n    keywords = [w.lower() for w in question.split() if len(w) > 3][:5]\n\n    # Find files\n    matched_files = []\n    full_content = \"\"\n\n    for py_file in Path(repo_path).rglob('*.py'):\n        if any(skip in str(py_file) for skip in ['node_modules', '.venv', 'vendor', '.git', '__pycache__']):\n            continue\n\n        try:\n            content = py_file.read_text(errors='ignore')\n            if any(kw in content.lower() for kw in keywords):\n                matched_files.append(str(py_file))\n                full_content += f\"\\n{'='*70}\\nFile: {py_file}\\n{'='*70}\\n{content}\\n\"\n\n                if len(matched_files) >= 10:\n                    break\n        except:\n            pass\n\n    tokens = count_tokens(full_content)\n\n    return {\n        'method': '1. Claude Alone (no RAG)',\n        'description': 'Reads full files matching keywords',\n        'files_read': len(matched_files),\n        'tokens': tokens,\n        'sample_files': [Path(f).name for f in matched_files[:3]]\n    }\n\n\n# ============================================================\n# Approach 2: RAG CLI Standalone (no Claude)\n# ============================================================"}
{"id":181,"text":"approach2_rag_standalone(question: str, repo: str):\n    \"\"\"\n    RAG CLI standalone - full answer generation without Claude.\n    Counts the generated answer + citations.\n    \"\"\"\n    try:\n        from langgraph_app import build_graph\n\n        # Build graph and run (with required thread_id config)\n        graph = build_graph()\n        result = graph.invoke(\n            {\n                \"question\": question,\n                \"repo\": repo,\n            },\n            config={\"configurable\": {\"thread_id\": \"test-comparison\"}}\n        )\n\n        # What gets generated\n        answer_text = result.get('answer', '')\n        citations_text = '\\n'.join([\n            f\"{c.get('file_path', '')}:{c.get('start_line', '')}-{c.get('end_line', '')}\"\n            for c in result.get('citations', [])\n        ])\n\n        full_output = f\"Answer:\\n{answer_text}\\n\\nCitations:\\n{citations_text}\"\n        tokens = count_tokens(full_output)\n\n        return {\n            'method': '2. RAG CLI Standalone',\n            'description': 'RAG generates answer directly (no Claude)',\n            'tokens': tokens,\n            'answer_length': len(answer_text),\n            'citations_count': len(result.get('citations', []))\n        }\n    except Exception as e:\n        return {'error': str(e)}\n\n\n# ============================================================\n# Approach 3: Claude + RAG Direct (full chunks)\n# ============================================================"}
{"id":182,"text":"approach3_claude_plus_rag_direct(question: str, repo: str, top_k: int = 10):\n    \"\"\"\n    Claude gets full code chunks from RAG.\n    This is what would happen if Claude called hybrid_search directly.\n    \"\"\"\n    try:\n        from hybrid_search import search_routed_multi\n\n        results = search_routed_multi(question, repo_override=repo, final_k=top_k)\n\n        # Build what gets sent to Claude\n        context = \"Retrieved code chunks:\\n\\n\"\n        for r in results:\n            context += f\"File: {r['file_path']}:{r['start_line']}-{r['end_line']}\\n\"\n            context += f\"Score: {r['rerank_score']:.3f}\\n\"\n            context += f\"Code:\\n{r.get('code', '')}\\n\\n\"\n\n        tokens = count_tokens(context)\n\n        return {\n            'method': '3. Claude + RAG Direct',\n            'description': 'Claude gets full code chunks from RAG',\n            'chunks': len(results),\n            'tokens': tokens,\n            'files_touched': len(set(r['file_path'] for r in results))\n        }\n    except Exception as e:\n        return {'error': str(e)}\n\n\n# ============================================================\n# Approach 4: Claude + RAG via MCP (metadata only)\n# ============================================================"}
{"id":183,"text":"approach4_claude_plus_rag_mcp(question: str, repo: str, top_k: int = 10):\n    \"\"\"\n    Claude gets MCP tool response (metadata, no full code).\n    This is what I (Claude Code) actually receive.\n\n    IMPORTANT: MCP tool schemas are sent with EVERY request!\n    \"\"\"\n    try:\n        from mcp_server import MCPServer\n\n        server = MCPServer()\n\n        # Get tool schemas (sent with every request)\n        tools_req = {'jsonrpc': '2.0', 'id': 1, 'method': 'tools/list', 'params': {}}\n        tools_resp = server.handle_request(tools_req)\n        tools_json = json.dumps(tools_resp['result']['tools'])\n        schema_tokens = count_tokens(tools_json)\n\n        # Get the actual search response\n        search_req = {\n            'jsonrpc': '2.0',\n            'id': 1,\n            'method': 'tools/call',\n            'params': {\n                'name': 'rag_search',\n                'arguments': {\n                    'repo': repo,\n                    'question': question,\n                    'top_k': top_k\n                }\n            }\n        }\n\n        search_resp = server.handle_request(search_req)\n\n        # The MCP response is what Claude receives\n        mcp_response = search_resp['result']['content'][0]['text']\n        response_tokens = count_tokens(mcp_response)\n\n        # Total = schemas + response\n        total_tokens = schema_tokens + response_tokens\n\n        # Parse to get metadata\n        result_data = json.loads(mcp_response)\n\n        return {\n            'method': '4. Claude + RAG via MCP',\n            'description': 'Claude gets MCP metadata (paths + scores only) + tool schemas',\n            'chunks': result_data.get('count', 0),\n            'tokens': total_tokens,\n            'schema_tokens': schema_tokens,\n            'response_tokens': response_tokens,\n            'breakdown': f'{schema_tokens} (schemas) + {response_tokens} (response)'\n        }\n    except Exception as e:\n        return {'error': str(e)}\n\n\n# ============================================================\n# Run Comparison\n# ============================================================"}
{"id":184,"text":"run_comparison(question: str, repo: str):\n    \"\"\"Run all four approaches and compare\"\"\"\n    print(f\"\\n{'='*75}\")\n    print(f\"QUESTION: {question}\")\n    print(f\"REPO: {repo}\")\n    print(f\"{'='*75}\\n\")\n\n    results = []\n\n    # Run each approach\n    approaches = [\n        (\"Claude Alone\", approach1_claude_alone),\n        (\"RAG CLI Standalone\", approach2_rag_standalone),\n        (\"Claude + RAG Direct\", approach3_claude_plus_rag_direct),\n        (\"Claude + RAG via MCP\", approach4_claude_plus_rag_mcp),\n    ]\n\n    for name, func in approaches:\n        print(f\"⏳ Testing: {name}...\")\n        result = func(question, repo)\n        results.append(result)\n\n    # Print results\n    print(f\"\\n{'='*75}\")\n    print(\"RESULTS (tokens sent to LLM):\")\n    print(f\"{'='*75}\\n\")\n\n    for i, result in enumerate(results, 1):\n        if 'error' in result:\n            print(f\"{i}. {result.get('method', 'Unknown')}: ERROR - {result['error']}\")\n            continue\n\n        print(f\"{i}. {result['method']}\")\n        print(f\"   {result['description']}\")\n        print(f\"   Tokens: {result['tokens']:,}\")\n\n        # Show method-specific details\n        if 'files_read' in result:\n            print(f\"   Files read: {result['files_read']}\")\n            if result.get('sample_files'):\n                print(f\"   Sample: {', '.join(result['sample_files'])}\")\n\n        if 'chunks' in result:\n            print(f\"   Chunks: {result['chunks']}\")\n\n        if 'files_touched' in result:\n            print(f\"   Files: {result['files_touched']}\")\n\n        if 'citations_count' in result:\n            print(f\"   Citations: {result['citations_count']}\")\n\n        if 'breakdown' in result:\n            print(f\"   Breakdown: {result['breakdown']}\")\n\n        print()\n\n    # Calculate savings\n    valid_results = [r for r in results if 'error' not in r and 'tokens' in r]\n\n    if len(valid_results) >= 2:\n        baseline = valid_results[0]['tokens']  # Claude alone\n\n        print(f\"{'='*75}\")\n        print(\"💰 SAVINGS vs Claude Alone:\")\n        print(f\"{'='*75}\\n\")\n\n        for result in valid_results[1:]:\n            tokens = result['tokens']\n            saved = baseline - tokens\n            pct = (saved / baseline * 100) if baseline > 0 else 0\n            reduction = baseline / tokens if tokens > 0 else 0\n\n            print(f\"{result['method']}:\")\n            print(f\"   Tokens saved: {saved:,}\")\n            print(f\"   Percentage: {pct:.1f}%\")\n            print(f\"   Reduction: {reduction:.1f}x\")\n\n            # Cost (gpt-4o: $2.50/1M input)\n            cost_saved = saved * (2.50 / 1_000_000)\n            print(f\"   $ saved/query: ${cost_saved:.6f}\")\n            print(f\"   $ saved/1000: ${cost_saved * 1000:.2f}\\n\")\n\n    return results\n\n\n# ============================================================\n# Main\n# ============================================================\n\nif __name__ == '__main__':\n    if not HAS_TIKTOKEN:\n        print(\"Installing tiktoken for accurate counts...\")\n        os.system(\"pip install -q tiktoken\")\n        try:\n            import tiktoken\n            HAS_TIKTOKEN = True\n            print(\"✓ tiktoken installed\\n\")\n        except:\n            print(\"⚠️  Using estimates (1 token ≈ 4 chars)\\n\")\n\n    # Test cases\n    tests = [\n        (\"Where is OAuth token validated\", \"project\"),\n        (\"How are fax jobs created and dispatched\", \"project\"),\n    ]\n\n    all_results = []\n\n    for question, repo in tests:\n        try:\n            results = run_comparison(question, repo)\n            all_results.append({\n                'question': question,\n                'repo': repo,\n                'results': results\n            })\n        except Exception as e:\n            print(f\"\\n❌ Error: {e}\\n\")\n\n    # Overall summary\n    if all_results:\n        print(f\"\\n{'='*75}\")\n        print(\"📊 SUMMARY\")\n        print(f\"{'='*75}\\n\")\n\n        print(f\"Total queries tested: {len(all_results)}\\n\")\n\n        # Average by method\n        methods = ['Claude Alone', 'RAG CLI Standalone', 'Claude + RAG Direct', 'Claude + RAG via MCP']\n\n        for method in methods:\n            tokens = []\n            for test in all_results:\n                for r in test['results']:\n                    if r.get('method', '').startswith(method.split()[0]) and 'tokens' in r:\n                        tokens.append(r['tokens'])\n\n            if tokens:\n                avg = sum(tokens) / len(tokens)\n                print(f\"{method}: {avg:,.0f} avg tokens\")\n\n        print(f\"\\n🎯 Recommendation:\")\n        print(f\"   Use MCP tools for maximum token efficiency\")\n        print(f\"   Use RAG CLI for standalone Q&A without Claude\")\n        print(f\"   Use Direct calls for custom integrations\")"}
{"id":185,"text":"#!/usr/bin/env python3\n\"\"\"\nInteractive quick setup to:\n  1) Add the current working directory as a repo (repos.json)\n  2) Optionally index it\n  3) Ensure venv + deps\n  4) Optionally start infra (Qdrant/Redis via docker compose)\n  5) Register MCP servers with Codex CLI and Claude Code\n\nRun this from the ROOT of the repo you want to index:\n  python /path/to/rag-service/scripts/quick_setup.py\n\nNotes:\n  - Never writes secrets without confirmation\n  - Creates timestamped backups of modified config files\n  - Uses Rich spinners/progress so users always see activity\n\"\"\"\nimport os\nimport sys\nimport json\nimport time\nimport platform\nimport shutil\nimport subprocess\nfrom pathlib import Path\n\ntry:\n    from rich.console import Console\n    from rich.panel import Panel\n    from rich.prompt import Confirm, Prompt\n    from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn\nexcept Exception:\n    print(\"This setup requires 'rich'. Install with: pip install rich\", file=sys.stderr)\n    sys.exit(1)\n\nconsole = Console()"}
{"id":186,"text":"write_repos_json(rag_root: Path, name: str, code_path: Path) -> Path:\n    p = os.getenv('REPOS_FILE') or str(rag_root / 'repos.json')\n    repos_path = Path(p)\n    cfg = {'default_repo': name, 'repos': []}\n    if repos_path.exists():\n        try:\n            cfg = json.loads(repos_path.read_text())\n            if not isinstance(cfg, dict):\n                cfg = {'default_repo': name, 'repos': []}\n        except Exception:\n            cfg = {'default_repo': name, 'repos': []}\n    # Update or append\n    repos = cfg.get('repos') or []\n    found = False\n    for r in repos:\n        if (r.get('name') or '').strip().lower() == name.lower():\n            r['path'] = str(code_path)\n            found = True\n            break\n    if not found:\n        repos.append({'name': name, 'path': str(code_path), 'keywords': [], 'path_boosts': [], 'layer_bonuses': {}})\n    cfg['repos'] = repos\n    # Ask to set default\n    if Confirm.ask(f\"Make [bold]{name}[/bold] the default repo?\", default=True):\n        cfg['default_repo'] = name\n    repos_path.write_text(json.dumps(cfg, indent=2))\n    return repos_path"}
{"id":187,"text":"_venv_python(repo_root: Path) -> Path:\n    if platform.system().lower().startswith('win'):\n        return repo_root / '.venv' / 'Scripts' / 'python.exe'\n    return repo_root / '.venv' / 'bin' / 'python'"}
{"id":188,"text":"ensure_venv_and_deps(rag_root: Path, progress: Progress, task_id) -> bool:\n    \"\"\"Create .venv and install deps if needed.\"\"\"\n    py = _venv_python(rag_root)\n    # Create venv if missing\n    if not py.exists():\n        progress.update(task_id, description='Creating virtualenv (.venv)')\n        try:\n            subprocess.check_call([sys.executable, '-m', 'venv', str(rag_root / '.venv')])\n        except subprocess.CalledProcessError as e:\n            console.print(f\"[red]Failed to create venv:[/red] {e}\")\n            return False\n    # Install deps\n    progress.update(task_id, description='Installing dependencies')\n    try:\n        reqs = [str(rag_root / 'requirements-rag.txt'), str(rag_root / 'requirements.txt')]\n        for req in reqs:\n            if Path(req).exists():\n                subprocess.check_call([str(py), '-m', 'pip', 'install', '--disable-pip-version-check', '-r', req])\n        # quick sanity imports\n        subprocess.check_call([str(py), '-c', 'import fastapi,qdrant_client,bm25s,langgraph;print(\"ok\")'])\n        return True\n    except subprocess.CalledProcessError as e:\n        console.print(f\"[red]Dependency install failed:[/red] {e}\")\n        return False"}
{"id":189,"text":"start_infra(rag_root: Path, progress: Progress, task_id) -> None:\n    progress.update(task_id, description='Starting Qdrant/Redis (docker compose)')\n    up = rag_root / 'scripts' / 'up.sh'\n    if not up.exists():\n        progress.update(task_id, description='Infra script not found (skipping)')\n        time.sleep(0.3)\n        return\n    try:\n        subprocess.check_call(['bash', str(up)])\n    except Exception as e:\n        console.print(f\"[yellow]Infra start skipped/failed:[/yellow] {e}\")\n    # quick qdrant ping\n    progress.update(task_id, description='Verifying Qdrant/Redis health')\n    try:\n        subprocess.check_call(['bash', '-lc', 'curl -s http://127.0.0.1:6333/collections >/dev/null || true'])\n    except Exception:\n        pass\n\ndetect_codex() -> str | None:\n    path = shutil.which('codex')\n    return path"}
{"id":190,"text":"codex_register(rag_root: Path, progress: Progress, task_id) -> None:\n    path = detect_codex()\n    if not path:\n        progress.update(task_id, description='Codex CLI not found (skip)')\n        time.sleep(0.3)\n        return\n    py = _venv_python(rag_root)\n    server = rag_root / 'mcp_server.py'\n    name = 'rag-service'\n    progress.update(task_id, description='Registering MCP with Codex')\n    try:\n        # remove existing silently\n        subprocess.run(['codex', 'mcp', 'remove', name], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n        subprocess.check_call(['codex', 'mcp', 'add', name, '--', str(py), str(server)])\n    except subprocess.CalledProcessError as e:\n        console.print(f\"[yellow]Codex registration failed:[/yellow] {e}\")"}
{"id":191,"text":"_claude_config_path() -> Path | None:\n    sysname = platform.system().lower()\n    home = Path.home()\n    if 'darwin' in sysname or 'mac' in sysname:\n        return (home / 'Library' / 'Application Support' / 'Claude' / 'claude_desktop_config.json')\n    if 'linux' in sysname:\n        return (home / '.config' / 'Claude' / 'claude_desktop_config.json')\n    if 'windows' in sysname or 'win' in sysname:\n        appdata = os.getenv('APPDATA')\n        if appdata:\n            return Path(appdata) / 'Claude' / 'claude_desktop_config.json'\n    return None"}
{"id":192,"text":"claude_register(rag_root: Path, progress: Progress, task_id) -> None:\n    cfgp = _claude_config_path()\n    if not cfgp:\n        progress.update(task_id, description='Claude config path not found (skip)')\n        time.sleep(0.3)\n        return\n    cfgp.parent.mkdir(parents=True, exist_ok=True)\n    py = _venv_python(rag_root)\n    server = rag_root / 'mcp_server.py'\n    # Load existing\n    data = {}\n    if cfgp.exists():\n        try:\n            data = json.loads(cfgp.read_text())\n        except Exception:\n            data = {}\n        # backup\n        bak = cfgp.with_suffix(cfgp.suffix + f'.bak.{time.strftime(\"%Y%m%d-%H%M%S\")}')\n        bak.write_text(json.dumps(data, indent=2))\n    # Merge entry\n    ms = data.get('mcpServers') or {}\n    ms['rag-service'] = {\n        'command': str(py),\n        'args': [str(server)],\n        'env': {\n            'OPENAI_API_KEY': os.getenv('OPENAI_API_KEY', '')\n        }\n    }\n    data['mcpServers'] = ms\n    progress.update(task_id, description='Writing Claude config')\n    cfgp.write_text(json.dumps(data, indent=2))"}
{"id":193,"text":"main():\n    rag_root = Path(__file__).resolve().parents[1]\n    # Allow explicit path override for code repo\n    forced_path = None\n    forced_name = None\n    argv = sys.argv[1:]\n    for i, a in enumerate(argv):\n        if a.startswith('--path='):\n            forced_path = a.split('=', 1)[1].strip()\n        elif a == '--path' and i+1 < len(argv):\n            forced_path = argv[i+1].strip()\n        elif a.startswith('--name='):\n            forced_name = a.split('=', 1)[1].strip()\n        elif a == '--name' and i+1 < len(argv):\n            forced_name = argv[i+1].strip()\n\n    code_root = Path(forced_path or os.getcwd()).resolve()\n    suggested = (forced_name or code_root.name.lower().replace(' ', '-').replace('_', '-'))\n    title = \"RAG Service — Quick Setup\"\n    msg = (\n        f\"Detected current directory:\\n[bold]{code_root}[/bold]\\n\\n\"\n        \"Create or update repos.json to include this path?\\n\"\n    )\n    console.print(Panel(msg, title=title, border_style=\"cyan\"))\n    if not Confirm.ask(\"Add this repo?\", default=True):\n        console.print(\"[yellow]Canceled.[/yellow]\")\n        return\n    name = forced_name or Prompt.ask(\"Repository name\", default=suggested)\n    repos_path = write_repos_json(rag_root, name, code_root)\n    console.print(f\"[green]✓[/green] Updated {repos_path}\")\n\n    # Offer to index\n    console.print(Panel(\n        \"Index now? This builds BM25 and embeddings; it may take time and bill your provider if configured.\",\n        title=\"Index Repository\", border_style=\"yellow\"\n    ))\n    do_index = Confirm.ask(\"Start indexing now?\", default=False)\n\n    console.print(Panel(\"Setup environment and agents?\", title=\"Agents & Infra\", border_style=\"cyan\"))\n    do_env = Confirm.ask(\"Ensure virtualenv + dependencies?\", default=True)\n    do_infra = Confirm.ask(\"Start Qdrant/Redis (docker compose)?\", default=True)\n    do_codex = Confirm.ask(\"Register Codex MCP?\", default=True)\n    do_claude = Confirm.ask(\"Register Claude MCP?\", default=True)\n\n    with Progress(\n        SpinnerColumn(style='cyan'),\n        TextColumn(\"{task.description}\"),\n        BarColumn(bar_width=None),\n        TimeElapsedColumn(),\n        transient=True,\n    ) as progress:\n        if do_env:\n            t = progress.add_task(\"Preparing environment\", total=None)\n            ok = ensure_venv_and_deps(rag_root, progress, t)\n            progress.remove_task(t)\n            if not ok:\n                console.print(\"[red]Environment setup failed; continuing without guarantees.[/red]\")\n        if do_infra:\n            t = progress.add_task(\"Starting infra\", total=None)\n            start_infra(rag_root, progress, t)\n            progress.remove_task(t)\n        if do_index:\n            t = progress.add_task(\"Indexing repository\", total=None)\n            env = os.environ.copy()\n            env['REPO'] = name\n            try:\n                subprocess.check_call([str(_venv_python(rag_root)), str(rag_root / 'index_repo.py')], env=env, cwd=str(rag_root))\n                console.print(f\"[green]✓[/green] Indexed repo: [bold]{name}[/bold]\")\n            except subprocess.CalledProcessError as e:\n                console.print(f\"[red]Indexing failed:[/red] {e}\")\n            progress.remove_task(t)\n        if do_codex:\n            t = progress.add_task(\"Registering Codex\", total=None)\n            codex_register(rag_root, progress, t)\n            progress.remove_task(t)\n        if do_claude:\n            t = progress.add_task(\"Registering Claude\", total=None)\n            claude_register(rag_root, progress, t)\n            progress.remove_task(t)\n\n    # Friendly next-steps banner\n    console.print(Panel(\n        \"Setup complete. Next steps:\\n\"\n        \" • Type 'codex' and try: Use rag_search to find OAuth in your repo\\n\"\n        f\" • Or run API: uvicorn serve_rag:app --host 127.0.0.1 --port 8012\\n\"\n        f\" • CLI streaming: python chat_cli.py --stream --api-url http://127.0.0.1:8012\\n\",\n        title=\"You're ready!\", border_style=\"green\"\n    ))\n\n\nif __name__ == '__main__':\n    main()"}
{"id":194,"text":"#!/usr/bin/env python3\nfrom __future__ import annotations\nimport os, time, json, tempfile, signal, subprocess\nfrom pathlib import Path\nfrom playwright.sync_api import sync_playwright, expect\n\nROOT = Path(__file__).resolve().parents[1]\nBASE = \"http://127.0.0.1:8012\"\n\nwait_health(timeout=20):\n    import urllib.request, urllib.error\n    start = time.time()\n    while time.time() - start < timeout:\n        try:\n            with urllib.request.urlopen(f\"{BASE}/health\", timeout=2) as resp:\n                if resp.status == 200:\n                    return True\n        except Exception:\n            time.sleep(0.5)\n    return False"}
{"id":195,"text":"main() -> int:\n    # Use existing server\n    assert wait_health(5), \"server not running on 8012\"\n\n    with sync_playwright() as p:\n        browser = p.chromium.launch(headless=False)\n        ctx = browser.new_context()\n        page = ctx.new_page()\n        page.goto(f\"{BASE}/gui/\", wait_until=\"domcontentloaded\")\n        page.wait_for_timeout(1000)\n\n        # Test Health button\n        print(\"Testing health button...\")\n        page.click('#btn-health')\n        page.wait_for_timeout(1000)\n        hs = page.locator('#health-status').text_content()\n        print(f\"  health: {hs}\")\n\n        # Test Overview section load\n        print(\"\\nTesting overview section...\")\n        overview_text = page.locator('#overview-section').text_content()\n        print(f\"  overview contains {len(overview_text)} chars\")\n        if \"—\" in overview_text or not overview_text.strip():\n            print(\"  ❌ Overview appears empty/placeholder\")\n        else:\n            print(\"  ✓ Overview has content\")\n\n        # Test Configure button (wizard)\n        print(\"\\nTesting configure button...\")\n        page.click('#btn-wizard')\n        page.wait_for_timeout(2000)\n        wizard_out = page.locator('#wizard-output').text_content()\n        print(f\"  wizard output: {wizard_out[:200] if wizard_out else '(empty)'}\")\n        if not wizard_out or wizard_out.strip() == \"\":\n            print(\"  ❌ Wizard produced no output\")\n        else:\n            print(\"  ✓ Wizard generated output\")\n\n        # Test Cost calc with select_option\n        print(\"\\nTesting cost calculator...\")\n        page.select_option('#cost-provider', 'openai')\n        page.select_option('#cost-model', 'gpt-4o-mini')\n        page.fill('#cost-in', '500')\n        page.fill('#cost-out', '800')\n        page.fill('#cost-rpd', '100')\n        page.click('#btn-estimate')\n        page.wait_for_timeout(1000)\n        daily = page.locator('#cost-daily').text_content()\n        print(f\"  cost daily: {daily}\")\n        if daily == \"—\":\n            print(\"  ❌ Cost calc not working\")\n        else:\n            print(\"  ✓ Cost calc working\")\n\n        print(\"\\n\\nPress Enter to close browser...\")\n        input()\n        browser.close()\n    return 0\n\n\nif __name__ == '__main__':\n    raise SystemExit(main())"}
{"id":196,"text":"#!/usr/bin/env python3\n\"\"\"\nMeasure actual token savings from RAG vs traditional file reading.\nCompares targeted RAG retrieval against reading full files.\n\"\"\"\n\nimport sys\nimport os\nfrom pathlib import Path\nfrom hybrid_search import search_routed_multi\n\ntry:\n    import tiktoken\n    HAS_TIKTOKEN = True\nexcept ImportError:\n    HAS_TIKTOKEN = False\n    print(\"⚠️  tiktoken not installed - using rough estimates (1 token ≈ 4 chars)\")\n    print(\"   Install with: pip install tiktoken\\n\")\n\ncount_tokens(text: str, model: str = \"gpt-4o\") -> int:\n    \"\"\"Count tokens precisely if tiktoken available, else estimate\"\"\"\n    if HAS_TIKTOKEN:\n        try:\n            encoding = tiktoken.encoding_for_model(model)\n            return len(encoding.encode(text))\n        except:\n            pass\n    # Fallback: rough estimate\n    return len(text) // 4"}
{"id":197,"text":"measure_rag_tokens(question: str, repo: str, top_k: int = 10):\n    \"\"\"Measure tokens using RAG hybrid search\"\"\"\n    results = search_routed_multi(question, repo_override=repo, final_k=top_k)\n\n    # Combine all retrieved code\n    combined_text = \"\"\n    for r in results:\n        combined_text += f\"File: {r['file_path']}:{r['start_line']}-{r['end_line']}\\n\"\n        combined_text += r.get('code', '') + \"\\n\\n\"\n\n    tokens = count_tokens(combined_text)\n\n    return {\n        'approach': 'RAG (hybrid search)',\n        'chunks': len(results),\n        'text': combined_text,\n        'tokens': tokens,\n        'files_touched': len(set(r['file_path'] for r in results))\n    }"}
{"id":198,"text":"measure_traditional_tokens(question: str, repo: str, max_files: int = 10):\n    \"\"\"\n    Simulate traditional approach: grep for keywords, read full files.\n    This is what you'd do WITHOUT RAG.\n    \"\"\"\n    repo_paths = {\n        'project': os.getenv('PROJECT_PATH', '/abs/path/to/project'),\n        'faxbot': os.getenv('FAXBOT_PATH', '/abs/path/to/faxbot')\n    }\n\n    repo_path = repo_paths.get(repo)\n    if not repo_path or not os.path.exists(repo_path):\n        return {'approach': 'Traditional', 'error': f'Repo not found: {repo_path}'}\n\n    # Extract keywords from question (simulate what a human would grep for)\n    keywords = [w.lower() for w in question.split() if len(w) > 3][:5]\n\n    # Find files containing keywords\n    combined_text = \"\"\n    matched_files = []\n\n    for py_file in Path(repo_path).rglob('*.py'):\n        if 'node_modules' in str(py_file) or '.venv' in str(py_file):\n            continue\n\n        try:\n            content = py_file.read_text(errors='ignore')\n\n            # If any keyword appears, a human would likely read this whole file\n            if any(kw in content.lower() for kw in keywords):\n                matched_files.append(str(py_file))\n                combined_text += f\"\\n{'='*60}\\nFile: {py_file}\\n{'='*60}\\n\"\n                combined_text += content + \"\\n\"\n\n                if len(matched_files) >= max_files:\n                    break\n        except Exception as e:\n            pass\n\n    tokens = count_tokens(combined_text)\n\n    return {\n        'approach': 'Traditional (grep + read full files)',\n        'files_read': len(matched_files),\n        'text': combined_text,\n        'tokens': tokens\n    }"}
{"id":199,"text":"run_comparison(question: str, repo: str):\n    \"\"\"Run both approaches and compare\"\"\"\n    print(f\"\\n{'='*70}\")\n    print(f\"Question: {question}\")\n    print(f\"Repository: {repo}\")\n    print(f\"{'='*70}\\n\")\n\n    # Measure RAG\n    print(\"⏳ Running RAG hybrid search...\")\n    rag = measure_rag_tokens(question, repo, top_k=10)\n\n    # Measure traditional\n    print(\"⏳ Simulating traditional grep + file reading...\")\n    trad = measure_traditional_tokens(question, repo, max_files=10)\n\n    # Print results\n    print(f\"\\n{'='*70}\")\n    print(\"📊 RESULTS:\")\n    print(f\"{'='*70}\")\n\n    print(f\"\\n🔍 RAG Approach:\")\n    print(f\"   Chunks retrieved: {rag['chunks']}\")\n    print(f\"   Files touched: {rag['files_touched']}\")\n    print(f\"   Total tokens: {rag['tokens']:,}\")\n\n    print(f\"\\n📁 Traditional Approach (grep + read full files):\")\n    print(f\"   Files read: {trad['files_read']}\")\n    print(f\"   Total tokens: {trad['tokens']:,}\")\n\n    # Calculate savings\n    if trad['tokens'] > 0 and rag['tokens'] > 0:\n        saved = trad['tokens'] - rag['tokens']\n        saved_pct = (saved / trad['tokens']) * 100\n        reduction = trad['tokens'] / rag['tokens']\n\n        print(f\"\\n{'='*70}\")\n        print(\"💰 TOKEN SAVINGS:\")\n        print(f\"{'='*70}\")\n        print(f\"   Tokens saved: {saved:,} tokens\")\n        print(f\"   Percentage saved: {saved_pct:.1f}%\")\n        print(f\"   Reduction factor: {reduction:.1f}x smaller\")\n\n        # Cost estimate (rough: $15/1M input tokens for gpt-4o)\n        cost_per_token = 15 / 1_000_000\n        cost_saved = saved * cost_per_token\n        print(f\"   Cost saved per query: ${cost_saved:.6f}\")\n        print(f\"   Cost saved per 1000 queries: ${cost_saved * 1000:.2f}\")\n\n    return rag, trad\n\n\nif __name__ == '__main__':\n    # Test queries\n    test_cases = [\n        (\"Where is OAuth token validated\", \"project\"),\n        (\"How are fax jobs created and dispatched\", \"project\"),\n        (\"EventStream component event types\", \"project\"),\n        (\"provider health status implementation\", \"project\"),\n    ]\n\n    results = []\n\n    for question, repo in test_cases:\n        try:\n            rag, trad = run_comparison(question, repo)\n            results.append({\n                'question': question,\n                'repo': repo,\n                'rag_tokens': rag['tokens'],\n                'trad_tokens': trad['tokens'],\n                'savings': trad['tokens'] - rag['tokens']\n            })\n        except Exception as e:\n            print(f\"\\n❌ Error testing '{question}': {e}\")\n\n    # Summary\n    if results:\n        print(f\"\\n\\n{'='*70}\")\n        print(\"📈 OVERALL SUMMARY\")\n        print(f\"{'='*70}\")\n\n        total_rag = sum(r['rag_tokens'] for r in results)\n        total_trad = sum(r['trad_tokens'] for r in results)\n        total_saved = total_trad - total_rag\n\n        print(f\"\\nTotal queries tested: {len(results)}\")\n        print(f\"Total RAG tokens: {total_rag:,}\")\n        print(f\"Total traditional tokens: {total_trad:,}\")\n        print(f\"Total saved: {total_saved:,} tokens ({(total_saved/total_trad*100):.1f}%)\")\n        print(f\"Average reduction: {total_trad/total_rag:.1f}x\\n\")"}
{"id":200,"text":"#!/usr/bin/env python3\n\"\"\"\nMake a repos.json from simple CLI args.\n\nUsage examples:\n  python scripts/make_repos_json.py repo-a=/abs/path/a repo-b=/abs/path/b --default repo-a\n\nEnvironment fallbacks:\n  REPO and REPO_PATH if provided (single repo).\n\nBehavior:\n  - Writes repos.json in repo root (or REPOS_FILE location if set)\n  - If repos.json exists, writes a timestamped backup next to it\n\"\"\"\nimport os, sys, json, time\nfrom pathlib import Path\n\nparse_args(argv):\n    pairs = []\n    default_repo = None\n    for arg in argv:\n        if arg == '--help' or arg == '-h':\n            print(__doc__)\n            sys.exit(0)\n        if arg.startswith('--default='):\n            default_repo = arg.split('=',1)[1].strip()\n            continue\n        if arg == '--default':\n            # next token is default\n            # handled in caller for simplicity\n            continue\n        if '=' in arg:\n            name, path = arg.split('=',1)\n            name = name.strip()\n            path = path.strip()\n            if name and path:\n                pairs.append((name, path))\n    # Handle \"--default name\" form\n    if '--default' in argv:\n        i = argv.index('--default')\n        if i+1 < len(argv):\n            default_repo = argv[i+1].strip()\n    return pairs, default_repo"}
{"id":201,"text":"main():\n    args = sys.argv[1:]\n    pairs, default_repo = parse_args(args)\n\n    # Fallback to env for single-repo if no pairs passed\n    if not pairs:\n        env_repo = (os.getenv('REPO') or '').strip()\n        env_path = (os.getenv('REPO_PATH') or '').strip()\n        if env_repo and env_path:\n            pairs = [(env_repo, env_path)]\n            if not default_repo:\n                default_repo = env_repo\n        else:\n            print('No repo arguments provided and REPO/REPO_PATH not set. Example: repo-a=/abs/path/a')\n            sys.exit(2)\n\n    # Build config structure\n    repos = []\n    for name, path in pairs:\n        repos.append({\n            'name': name,\n            'path': str(Path(path).expanduser()),\n            'keywords': [],\n            'path_boosts': [],\n            'layer_bonuses': {}\n        })\n\n    if not default_repo:\n        default_repo = repos[0]['name']\n\n    cfg = {'default_repo': default_repo, 'repos': repos}\n\n    # Output path\n    out = os.getenv('REPOS_FILE') or str(Path(__file__).resolve().parents[1] / 'repos.json')\n    outp = Path(out)\n    outp_parent = outp.parent\n    outp_parent.mkdir(parents=True, exist_ok=True)\n\n    # Backup existing\n    if outp.exists():\n        ts = time.strftime('%Y%m%d-%H%M%S')\n        bak = outp.with_suffix(outp.suffix + f'.bak.{ts}')\n        bak.write_text(outp.read_text())\n        print(f'Backed up existing {outp} -> {bak}')\n\n    outp.write_text(json.dumps(cfg, indent=2))\n    print(f'Wrote {outp} with {len(repos)} repo(s); default_repo={default_repo}')\n\n\nif __name__ == '__main__':\n    main()"}
{"id":202,"text":"import os\nfrom hybrid_search import search_routed_multi\n\nTESTS = [\n    ('project','ai studio','easy'),\n    ('project','TBAC trait system','easy'),\n    ('project','plugin builder','easy'),\n    ('project','webhook verification','easy'),\n    ('project','three lane gateway','medium'),\n    ('project','plugin sandbox isolation','medium'),\n    ('project','provider adapter traits','medium'),\n    ('project','canonical event normalization','medium'),\n    ('project','how does TBAC prevent PHI access','hard'),\n    ('project','what is the general purpose of project','hard'),\n    ('project','how do different providers interact','hard'),\n]\n\nos.environ.setdefault('EMBEDDING_TYPE', 'local')\n\nby_diff = {}\nfor repo, q, d in TESTS:\n    docs = search_routed_multi(q, repo_override=repo, final_k=5)\n    s = (docs or [{}])[0].get('rerank_score', 0.0)\n    by_diff.setdefault(d, []).append(s)\n\nprint('\\n' + '='*80)\nprint('FINAL PERFORMANCE METRICS')\nprint('='*80)\n\nTARGET = {'easy':0.80, 'medium':0.70, 'hard':0.65}\nall_scores = []\nfor d, arr in by_diff.items():\n    avg = sum(arr)/max(1,len(arr))"}
{"id":203,"text":"all_scores.extend(arr)\n    status = '✓' if avg >= TARGET[d] else '✗'\n    print(f\"{status} {d.upper():7} | Avg: {avg:.3f} | Target: {TARGET[d]:.3f}\")\n\noverall = sum(all_scores)/max(1,len(all_scores))\nprint(f\"\\n{'Overall Average:':20} {overall:.3f}\")\nprint('='*80)"}
{"id":204,"text":"import json\nimport os\nfrom collections import Counter, defaultdict\nfrom pathlib import Path\nimport re\nextract_tokens(code):\n    \"\"\"Extract meaningful tokens from code\"\"\"\n    # Remove strings and comments\n    code = re.sub(r'[\"\\'].*?[\"\\']', '', code)\n    code = re.sub(r'#.*?\\n', '', code)\n    code = re.sub(r'//.*?\\n', '', code)\n    \n    # Extract identifiers (camelCase, snake_case, etc)\n    tokens = re.findall(r'\\b[a-zA-Z_][a-zA-Z0-9_]*\\b', code)\n    return [t.lower() for t in tokens if len(t) > 2]"}
{"id":205,"text":"analyze_repo(repo_path):\n    \"\"\"Analyze a repo for discriminative keywords\"\"\"\n    file_tokens = defaultdict(set)  # file -> set of tokens\n    global_counts = Counter()  # token -> total count\n    \n    for root, dirs, files in os.walk(repo_path):\n        # Skip common ignore patterns\n        dirs[:] = [d for d in dirs if d not in {'.git', 'node_modules', '__pycache__', '.venv', 'dist', 'build'}]\n        \n        for file in files:\n            if not any(file.endswith(ext) for ext in ['.py', '.js', '.ts', '.tsx', '.rb', '.java', '.go']):\n                continue\n                \n            file_path = os.path.join(root, file)\n            try:\n                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n                    code = f.read()\n                    tokens = extract_tokens(code)\n                    file_tokens[file_path].update(tokens)\n                    global_counts.update(tokens)\n            except:\n                continue\n    \n    # Calculate TF-IDF style scores\n    num_files = len(file_tokens)\n    doc_freq = Counter()  # how many files contain each token\n    \n    for tokens in file_tokens.values():\n        doc_freq.update(tokens)\n    \n    # Score = term frequency * inverse document frequency\n    keyword_scores = {}\n    for token, total_count in global_counts.items():\n        df = doc_freq[token]\n        idf = num_files / df if df > 0 else 0\n        \n        # High score = appears often but in few files (discriminative)\n        # Low score = appears everywhere (stop word) or rarely (noise)\n        if df > 1 and df < num_files * 0.05:  # in 2+ files but <5% of files\n            keyword_scores[token] = total_count * idf\n    \n    return keyword_scores, doc_freq, num_files"}
{"id":206,"text":"find_discriminative_keywords(repo_path, top_n=50):\n    \"\"\"Find the most discriminative keywords in a repo\"\"\"\n    keyword_scores, doc_freq, num_files = analyze_repo(repo_path)\n    \n    # Sort by score\n    sorted_keywords = sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)\n    \n    results = []\n    for token, score in sorted_keywords[:top_n]:\n        results.append({\n            'keyword': token,\n            'score': round(score, 2),\n            'appears_in_files': doc_freq[token],\n            'file_percentage': round(100 * doc_freq[token] / num_files, 1)\n        })\n    \n    return results\n\nif __name__ == '__main__':\n    repos = {\n        'project': os.getenv('PROJECT_PATH', '/abs/path/to/project'),\n        'project': os.getenv('project_PATH', '/abs/path/to/project')\n    }\n    \n    all_results = {}\n    \n    for repo_name, repo_path in repos.items():\n        print(f'\\n{\"=\"*80}')\n        print(f'ANALYZING: {repo_name}')\n        print(f'{\"=\"*80}')\n        \n        keywords = find_discriminative_keywords(repo_path, top_n=30)\n        all_results[repo_name] = keywords\n        \n        print(f'\\nTop 30 Discriminative Keywords (best for queries):\\n')\n        for i, kw in enumerate(keywords, 1):\n            print(f'{i:2}. {kw[\"keyword\"]:20} | Score: {kw[\"score\"]:8.1f} | In {kw[\"appears_in_files\"]:3} files ({kw[\"file_percentage\"]:4.1f}%)')\n    \n    # Find cross-contamination terms\n    print(f'\\n{\"=\"*80}')\n    print('CROSS-CONTAMINATION ANALYSIS')\n    print(f'{\"=\"*80}')\n    \n    viv_keywords = {k['keyword'] for k in all_results['project'][:30]}\n    fax_keywords = {k['keyword'] for k in all_results['project'][:30]}\n    \n    overlap = viv_keywords & fax_keywords\n    print(f'\\nShared keywords (cause confusion): {len(overlap)}')\n    if overlap:\n        print(f'  {\", \".join(sorted(overlap))}')\n    \n    print(f'\\nPROJECT-only keywords (use these!): {len(viv_keywords - fax_keywords)}')\n    print(f'  {\", \".join(sorted(list(viv_keywords - fax_keywords)[:10]))}')\n    \n    print(f'\\nPROJECT-only keywords (use these!): {len(fax_keywords - viv_keywords)}')\n    print(f'  {\", \".join(sorted(list(fax_keywords - viv_keywords)[:10]))}')\n    \n    # Save to file\n    with open('discriminative_keywords.json', 'w') as f:\n        json.dump(all_results, f, indent=2)\n    \n    print(f'\\n✓ Results saved to discriminative_keywords.json')"}
{"id":207,"text":"#!/usr/bin/env python3\n\"\"\"Debug GUI by opening it and printing console errors\"\"\"\nfrom __future__ import annotations\nimport time\nfrom playwright.sync_api import sync_playwright\n\nBASE = \"http://127.0.0.1:8012\""}
{"id":208,"text":"main():\n    with sync_playwright() as p:\n        browser = p.chromium.launch(headless=False)\n        ctx = browser.new_context()\n        page = ctx.new_page()\n\n        # Capture console messages\n        console_msgs = []\n        page.on(\"console\", lambda msg: console_msgs.append(f\"[{msg.type}] {msg.text}\"))\n        page.on(\"pageerror\", lambda err: print(f\"❌ PAGE ERROR: {err}\"))\n\n        print(f\"Opening {BASE}/gui/...\")\n        page.goto(f\"{BASE}/gui/\", wait_until=\"domcontentloaded\")\n        page.wait_for_timeout(2000)\n\n        # Check overview populated\n        print(\"\\n=== Overview Section ===\")\n        try:\n            health = page.locator('#dash-health').text_content()\n            repo = page.locator('#dash-repo').text_content()\n            autotune = page.locator('#dash-autotune').text_content()\n            cards = page.locator('#dash-cards').text_content()\n            print(f\"Health: {health}\")\n            print(f\"Repo: {repo}\")\n            print(f\"Autotune: {autotune}\")\n            print(f\"Cards: {cards}\")\n            if all(x != \"—\" for x in [health, repo, autotune, cards]):\n                print(\"✓ Overview populated\")\n            else:\n                print(\"❌ Overview still has placeholders\")\n        except Exception as e:\n            print(f\"❌ Error reading overview: {e}\")\n\n        # Test wizard button\n        print(\"\\n=== Testing Wizard Button ===\")\n        try:\n            page.fill('#budget', '10')\n            page.click('#btn-wizard-oneclick')\n            page.wait_for_timeout(3000)\n            tri_out = page.locator('#tri-out').text_content()\n            print(\"Full tri-output:\")\n            print(tri_out)\n            if \"Press button\" in tri_out or len(tri_out) < 20:\n                print(\"❌ Wizard didn't generate output\")\n            else:\n                print(\"✓ Wizard generated output\")\n        except Exception as e:\n            print(f\"❌ Wizard error: {e}\")\n\n        # Test cost calculator with blur events\n        print(\"\\n=== Testing Cost Calculator ===\")\n        try:\n            print(\"Typing 500 into cost-in and blurring...\")\n            page.fill('#cost-in', '500')\n            page.locator('#cost-in').blur()\n            page.wait_for_timeout(300)\n            val = page.input_value('#cost-in')\n            print(f\"  Value after blur: '{val}'\")\n\n            print(\"Typing 800 into cost-out and blurring...\")\n            page.fill('#cost-out', '800')\n            page.locator('#cost-out').blur()\n            page.wait_for_timeout(300)\n            val2 = page.input_value('#cost-out')\n            print(f\"  Value after blur: '{val2}'\")\n\n            if not val or not val2:\n                print(\"❌ Cost inputs being cleared\")\n            else:\n                print(f\"✓ Cost inputs retained (values: {val}, {val2})\")\n\n            # Test with larger number (comma formatting disabled for type=\"number\" inputs)\n            print(\"Testing with 5000 (no comma formatting expected for number inputs)...\")\n            page.fill('#cost-in', '5000')\n            page.locator('#cost-in').blur()\n            page.wait_for_timeout(300)\n            val3 = page.input_value('#cost-in')\n            print(f\"  5000 after blur: '{val3}'\")\n            if val3 == '5000':\n                print(\"✓ Number input retains value\")\n            else:\n                print(f\"❌ Unexpected value: {val3}\")\n        except Exception as e:\n            print(f\"❌ Cost calculator error: {e}\")\n\n        # Print console\n        print(\"\\n=== Console Messages ===\")\n        for msg in console_msgs:\n            print(msg)\n\n        browser.close()\n        return 0\n\nif __name__ == '__main__':\n    main()"}
{"id":209,"text":"#!/usr/bin/env python3\n\"\"\"\nComplete, transparent comparison:\n- Qwen 3 vs OpenAI gpt-4o\n- Actual MCP tool schema overhead\n- Real latency measurements\n- Quality comparison\n\"\"\"\nimport sys\nimport os\nimport json\nimport time\nROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\nsys.path.insert(0, ROOT_DIR)\n\ntry:\n    import tiktoken\n    enc = tiktoken.encoding_for_model(\"gpt-4o\")\n    def count_tokens(text): return len(enc.encode(text))\nexcept:\n    def count_tokens(text): return len(text) // 4\n\nprint(\"=\" * 80)\nprint(\"COMPLETE MODEL COMPARISON - TRANSPARENT MEASUREMENTS\")\nprint(\"=\" * 80)\n\n# Test query\nquestion = \"How are fax jobs created and dispatched\"\nrepo = \"project\"\n\nprint(f\"\\nTest query: '{question}'\")\nprint(f\"Repo: {repo}\\n\")\n\n# ==================================================================\n# 1. MEASURE MCP TOOL SCHEMA OVERHEAD (sent on EVERY request)\n# ==================================================================\nprint(\"1. MCP Tool Schema Overhead\")\nprint(\"-\" * 80)\n\nfrom mcp_server import MCPServer"}
{"id":210,"text":"server = MCPServer()\ntools_req = {'jsonrpc': '2.0', 'id': 1, 'method': 'tools/list', 'params': {}}\ntools_resp = server.handle_request(tools_req)\ntools_json = json.dumps(tools_resp['result']['tools'])\nschema_tokens = count_tokens(tools_json)\n\nprint(f\"Tool schemas (sent with EVERY request): {schema_tokens:,} tokens\")\nprint(f\"Schema size: {len(tools_json):,} bytes\\n\")\n\n# ==================================================================\n# 2. MCP SEARCH RESPONSE SIZE\n# ==================================================================\nprint(\"2. MCP Search Response\")\nprint(\"-\" * 80)\n\nsearch_req = {\n    'jsonrpc': '2.0',\n    'id': 1,\n    'method': 'tools/call',\n    'params': {\n        'name': 'rag_search',\n        'arguments': {'repo': repo, 'question': question, 'top_k': 10}\n    }\n}\n\nstart = time.time()\nsearch_resp = server.handle_request(search_req)\nsearch_latency = time.time() - start\n\nmcp_response = search_resp['result']['content'][0]['text']\nresponse_tokens = count_tokens(mcp_response)\ntotal_mcp_tokens = schema_tokens + response_tokens"}
{"id":211,"text":"print(f\"Response tokens: {response_tokens:,}\")\nprint(f\"Total MCP tokens: {total_mcp_tokens:,} ({schema_tokens} schema + {response_tokens} response)\")\nprint(f\"Search latency: {search_latency:.2f}s\\n\")\n\n# ==================================================================\n# 3. QWEN 3 GENERATION\n# ==================================================================\nprint(\"3. Qwen 3 Generation (Local)\")\nprint(\"-\" * 80)\n\nos.environ[\"OLLAMA_URL\"] = \"http://127.0.0.1:11434/api\"\nos.environ[\"GEN_MODEL\"] = \"qwen3-coder:30b\"\n\nfrom env_model import generate_text\n\n# Parse MCP response to get context\nresult_data = json.loads(mcp_response)\ncontext = f\"Retrieved {result_data['count']} code locations:\\n\"\nfor r in result_data['results'][:5]:\n    context += f\"- {r['file_path']}:{r['start_line']}-{r['end_line']} (score: {r['rerank_score']:.3f})\\n\"\n\nprompt = f\"{context}\\n\\nQuestion: {question}\\nAnswer:\"\n\nstart = time.time()\nqwen_answer, _ = generate_text(prompt, model=\"qwen3-coder:30b\")\nqwen_latency = time.time() - start"}
{"id":212,"text":"qwen_output_tokens = count_tokens(qwen_answer)\nqwen_total_tokens = total_mcp_tokens + qwen_output_tokens\n\nprint(f\"Answer length: {len(qwen_answer)} chars\")\nprint(f\"Output tokens: {qwen_output_tokens:,}\")\nprint(f\"Total tokens (MCP + generation): {qwen_total_tokens:,}\")\nprint(f\"Generation latency: {qwen_latency:.2f}s\")\nprint(f\"Cost: $0.00 (local)\")\nprint(f\"\\nAnswer preview: {qwen_answer[:200]}...\\n\")\n\n# ==================================================================\n# 4. OPENAI GPT-4O GENERATION\n# ==================================================================\nprint(\"4. OpenAI gpt-4o Generation (API)\")\nprint(\"-\" * 80)\n\n# Use OpenAI for generation\nfrom openai import OpenAI\nclient = OpenAI()\n\nstart = time.time()\ntry:\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        max_tokens=500\n    )\n    openai_answer = response.choices[0].message.content\n    openai_latency = time.time() - start\n    \n    openai_output_tokens = count_tokens(openai_answer)"}
{"id":213,"text":"openai_total_tokens = total_mcp_tokens + openai_output_tokens\n    \n    # gpt-4o pricing (as of Oct 2025): $2.50/1M input, $10/1M output\n    input_cost = total_mcp_tokens * (2.50 / 1_000_000)\n    output_cost = openai_output_tokens * (10.00 / 1_000_000)\n    total_cost = input_cost + output_cost\n    \n    print(f\"Answer length: {len(openai_answer)} chars\")\n    print(f\"Output tokens: {openai_output_tokens:,}\")\n    print(f\"Total tokens (MCP + generation): {openai_total_tokens:,}\")\n    print(f\"Generation latency: {openai_latency:.2f}s\")\n    print(f\"Cost: ${total_cost:.6f} (${input_cost:.6f} input + ${output_cost:.6f} output)\")\n    print(f\"\\nAnswer preview: {openai_answer[:200]}...\\n\")\nexcept Exception as e:\n    print(f\"ERROR: {e}\\n\")\n    openai_answer = None\n\n# ==================================================================\n# 5. COMPARISON TABLE\n# ==================================================================\nprint(\"=\" * 80)\nprint(\"SUMMARY COMPARISON\")\nprint(\"=\" * 80)\n\nprint(\"\\nTOKEN BREAKDOWN:\")\nprint(f\"  MCP tool schemas:     {schema_tokens:,} tokens (sent on EVERY request)\")"}
{"id":214,"text":"print(f\"  MCP search response:  {response_tokens:,} tokens\")\nprint(f\"  Qwen 3 output:        {qwen_output_tokens:,} tokens\")\nif openai_answer:\n    print(f\"  gpt-4o output:        {openai_output_tokens:,} tokens\")\n\nprint(f\"\\nTOTAL TOKENS:\")\nprint(f\"  Qwen 3:   {qwen_total_tokens:,} tokens\")\nif openai_answer:\n    print(f\"  gpt-4o:   {openai_total_tokens:,} tokens\")\n\nprint(f\"\\nLATENCY:\")\nprint(f\"  MCP search:       {search_latency:.2f}s\")\nprint(f\"  Qwen 3 generate:  {qwen_latency:.2f}s\")\nif openai_answer:\n    print(f\"  gpt-4o generate:  {openai_latency:.2f}s\")\n\nprint(f\"\\nCOST PER QUERY:\")\nprint(f\"  Qwen 3:   $0.00 (local)\")\nif openai_answer:\n    print(f\"  gpt-4o:   ${total_cost:.6f}\")\n\nprint(f\"\\nANSWER QUALITY:\")\nprint(f\"  Qwen 3:   {len(qwen_answer)} chars - {qwen_answer[:100]}...\")\nif openai_answer:\n    print(f\"  gpt-4o:   {len(openai_answer)} chars - {openai_answer[:100]}...\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(f\"SAVED TO: /tmp/full_comparison_results.json\")\nprint(\"=\" * 80)\n\n# Save results\nresults = {\n    \"query\": question,\n    \"repo\": repo,\n    \"mcp\": {\n        \"schema_tokens\": schema_tokens,"}
{"id":215,"text":"\"response_tokens\": response_tokens,\n        \"total_tokens\": total_mcp_tokens,\n        \"latency_s\": search_latency\n    },\n    \"qwen3\": {\n        \"output_tokens\": qwen_output_tokens,\n        \"total_tokens\": qwen_total_tokens,\n        \"latency_s\": qwen_latency,\n        \"cost_usd\": 0.0,\n        \"answer\": qwen_answer\n    }\n}\n\nif openai_answer:\n    results[\"gpt4o\"] = {\n        \"output_tokens\": openai_output_tokens,\n        \"total_tokens\": openai_total_tokens,\n        \"latency_s\": openai_latency,\n        \"cost_usd\": total_cost,\n        \"answer\": openai_answer\n    }\n\nwith open('/tmp/full_comparison_results.json', 'w') as f:\n    json.dump(results, f, indent=2)\n\nprint(\"\\nDone!\")"}
{"id":216,"text":"// AGRO GUI app.js (complete with all handlers)\n(function () {\n    // Backend API base: respects ?api= override; defaults to local FastAPI\n    const API_BASE = (() => {\n        try {\n            const u = new URL(window.location.href);\n            const q = new URLSearchParams(u.search);\n            const override = q.get('api');\n            if (override) return override.replace(/\\/$/, '');\n            if (u.port === '8012') return u.origin;\n            return 'http://127.0.0.1:8012';\n        } catch { return 'http://127.0.0.1:8012'; }\n    })();\n    const api = (p) => `${API_BASE}${p}`;\n    const $ = (sel) => document.querySelector(sel);\n    const $$ = (sel) => Array.from(document.querySelectorAll(sel));\n\n    const state = {\n        prices: null,\n        config: null,\n        profiles: [],\n        defaultProfile: null,\n    };\n\n    // ---------------- Tabs ----------------\n    let storageCalculatorLoaded = false;\n\n    function loadStorageCalculator() {\n        if (storageCalculatorLoaded) return;\n        const container = document.getElementById('storage-calculator-container');\n        if (!container) return;\n\n        // Load the HTML template\n        if (typeof getStorageCalculatorHTML === 'function') {\n            container.innerHTML = getStorageCalculatorHTML();"}
{"id":217,"text":"// Initialize the calculator\n            if (typeof initStorageCalculator === 'function') {\n                initStorageCalculator();\n            }\n\n            storageCalculatorLoaded = true;\n        }\n    }\n\n    function switchTab(tabName) {\n        const groups = {\n            models: ['generation','embeddings','reranking'],\n            retrieval: ['retrieval','confidence','cards'],\n            repos: ['repos','indexing'],\n            tools: ['calculator','eval','misc'],\n            infra: ['infra'],\n            dashboard: ['dashboard'],\n            storage: ['storage']\n        };\n        const show = groups[tabName] || [tabName];\n        $$('.tab-content').forEach(el => el.classList.remove('active'));\n        show.forEach(id => { const el = document.getElementById(`tab-${id}`); if (el) el.classList.add('active'); });\n        $$('.tab-bar button').forEach(el => el.classList.remove('active'));\n        const btn = document.querySelector(`.tab-bar button[data-tab=\"${tabName}\"]`);\n        if (btn) btn.classList.add('active');\n\n        // Load storage calculator when the tab is opened\n        if (tabName === 'storage') {\n            loadStorageCalculator();\n        }\n    }\n\n    function bindTabs() {\n        $$('.tab-bar button').forEach(btn => {\n            btn.addEventListener('click', () => {"}
{"id":218,"text":"const tab = btn.getAttribute('data-tab');\n                switchTab(tab);\n            });\n        });\n    }\n\n    // ---------------- Tooltips (modular) ----------------\n    // Delegates to external module /gui/js/tooltips.js\n\n    // ---------------- Global Search ----------------\n    function clearHighlights() { $$('.hl').forEach(m => { const t=document.createTextNode(m.textContent); m.replaceWith(t); }); }\n    function highlightMatches(root, q) {\n        if (!q) return; const rx = new RegExp(q.replace(/[.*+?^${}()|[\\]\\\\]/g,'\\\\$&'), 'ig');\n        const walker = document.createTreeWalker(root, NodeFilter.SHOW_TEXT, null);\n        const hits = [];\n        while (walker.nextNode()) {\n            const n = walker.currentNode; if (!n.nodeValue || !n.parentElement) continue;\n            if (/SCRIPT|STYLE|IFRAME/.test(n.parentElement.tagName)) continue;\n            const m = n.nodeValue.match(rx); if (!m) continue;\n            const span = document.createElement('mark'); span.className='hl'; span.textContent = n.nodeValue;\n            const html = n.nodeValue.replace(rx, s => `<mark class=\"hl\">${s}</mark>`);\n            const frag = document.createElement('span'); frag.innerHTML = html;"}
{"id":219,"text":"n.parentElement.replaceChild(frag, n);\n            hits.push(frag.querySelector('mark.hl'));\n        }\n        return hits;\n    }\n\n    function bindGlobalSearch() {\n        const box = document.getElementById('global-search');\n        if (!box) return;\n        function run(q, jump=false) {\n            clearHighlights();\n            if (!q) return;\n            const hits = highlightMatches(document.querySelector('.content'), q);\n            if (jump && hits && hits.length) hits[0].scrollIntoView({behavior:'smooth', block:'center'});\n        }\n        box.addEventListener('keydown', (e)=>{ if ((e.ctrlKey||e.metaKey) && e.key.toLowerCase()==='k'){ e.preventDefault(); box.focus(); box.select(); }});\n        box.addEventListener('input', ()=> run(box.value.trim()));\n        box.addEventListener('keydown', (e)=>{ if (e.key==='Enter') run(box.value.trim(), true); });\n    }\n\n    // ---------------- Git Hooks ----------------\n    async function refreshHooksStatus(){\n        try{\n            const d = await (await fetch(api('/api/git/hooks/status'))).json();\n            const el = $('#hooks-status'); if (el) el.textContent = (d.post_checkout && d.post_commit) ? `Installed @ ${d.dir}` : 'Not installed';"}
{"id":220,"text":"}catch{ const el=$('#hooks-status'); if(el) el.textContent='Status unavailable'; }\n    }\n\n    async function installHooks(){\n        try{\n            const r = await fetch(api('/api/git/hooks/install'), { method:'POST' });\n            const d = await r.json();\n            alert(d.message || 'Hooks installed');\n            await refreshHooksStatus();\n        }catch(e){ alert('Failed to install hooks: ' + e.message); }\n    }\n\n    // ---------------- Health ----------------\n    async function checkHealth() {\n        try {\n            const r = await fetch(api('/health'));\n            const d = await r.json();\n            $('#health-status').textContent = d.ok || d.status === 'healthy' ? `OK @ ${d.ts || new Date().toISOString()}` : 'Not OK';\n        } catch (e) {\n            $('#health-status').textContent = 'Error';\n        }\n    }\n\n    // ---------------- Config ----------------\n    async function loadConfig() {\n        try {\n            try { await fetch(api('/api/env/reload'), { method: 'POST' }); } catch {}\n            const r = await fetch(api('/api/config'));\n            const d = await r.json();\n            state.config = d;\n            populateConfigForm(d);\n        } catch (e) {\n            console.error('Failed to load config:', e);\n        }\n    }\n\n    function populateConfigForm(data) {"}
{"id":221,"text":"const env = data.env || {};\n\n        // Fill all env variable fields\n        Object.entries(env).forEach(([k, v]) => {\n            const field = document.querySelector(`[name=\"${k}\"]`);\n            if (!field) return;\n\n            if (field.type === 'checkbox') {\n                field.checked = String(v).toLowerCase() === 'true' || v === '1' || v === true;\n            } else if (field.tagName === 'SELECT') {\n                field.value = v;\n            } else {\n                field.value = v;\n            }\n        });\n\n        // Populate repo select\n        const repoSelect = $('#repo-select');\n        if (repoSelect) {\n            repoSelect.innerHTML = '';\n            (data.repos || []).forEach((repo) => {\n                const opt = document.createElement('option');\n                opt.value = repo.name;\n                opt.textContent = repo.name;\n                repoSelect.appendChild(opt);\n            });\n            if (env.REPO) {\n                repoSelect.value = env.REPO;\n            } else if (data.default_repo) {\n                repoSelect.value = data.default_repo;\n            }\n        }\n\n        // Seed cost panel defaults from pricing if fields are empty\n        if (state.prices && Array.isArray(state.prices.models) && state.prices.models.length) {\n            if (!$('#cost-provider').value) $('#cost-provider').value = state.prices.models[0].provider || '';\n            if (!$('#cost-model').value) $('#cost-model').value = state.prices.models[0].model || '';"}
{"id":222,"text":"}\n\n        // Cost panel autopopulate from env\n        try {\n            // Generation provider heuristic: use GEN_MODEL hint if present; otherwise env keys\n            let provGuess = '';\n            const gm = env.GEN_MODEL || '';\n            if (/^gpt-|^o\\w+:/i.test(gm)) provGuess = 'openai';\n            else if (/^claude/i.test(gm)) provGuess = 'anthropic';\n            else if (/^gemini/i.test(gm)) provGuess = 'google';\n            else if (env.OLLAMA_URL) provGuess = 'local';\n            else if (env.OPENAI_API_KEY) provGuess = 'openai';\n            else if (env.ANTHROPIC_API_KEY) provGuess = 'anthropic';\n            else if (env.GOOGLE_API_KEY) provGuess = 'google';\n            if (provGuess) $('#cost-provider').value = provGuess;\n            if (env.GEN_MODEL) $('#cost-model').value = env.GEN_MODEL;\n\n            // Embeddings\n            if (env.EMBEDDING_TYPE) {\n                const ep = document.getElementById('cost-embed-provider'); if (ep) ep.value = env.EMBEDDING_TYPE;\n                if (env.EMBEDDING_TYPE === 'openai' && document.getElementById('cost-embed-model') && !$('#cost-embed-model').value) $('#cost-embed-model').value = 'text-embedding-3-small';\n                if (env.EMBEDDING_TYPE === 'voyage' && document.getElementById('cost-embed-model') && !$('#cost-embed-model').value) $('#cost-embed-model').value = 'voyage-3-large-embed';"}
{"id":223,"text":"}\n            // Reranker\n            if (env.RERANK_BACKEND) {\n                const rp = document.getElementById('cost-rerank-provider'); if (rp) rp.value = env.RERANK_BACKEND;\n            }\n            if (env.COHERE_RERANK_MODEL && document.getElementById('cost-rerank-model')) $('#cost-rerank-model').value = env.COHERE_RERANK_MODEL;\n            if (env.RERANKER_MODEL && document.getElementById('cost-rerank-model') && !$('#cost-rerank-model').value) $('#cost-rerank-model').value = env.RERANKER_MODEL;\n        } catch {}\n\n        // Wizard defaults: seed from env\n        try { seedWizardFromEnv(env); } catch {}\n        updateWizardSummary();\n\n        // Populate repos metadata editor\n        const reposSection = $('#repos-section');\n        if (reposSection) {\n            reposSection.innerHTML = '';\n            (data.repos || []).forEach((repo) => {\n                const div = document.createElement('div');\n                div.style.cssText = 'background: #0a0a0a; border: 1px solid #2a2a2a; border-radius: 6px; padding: 16px; margin-bottom: 16px;';\n                const rname = repo.name;\n                div.innerHTML = `\n                    <h4 style=\\\"color: #00ff88; font-size: 14px; margin-bottom: 12px;\\\">Repo: ${repo.name}</h4>"}
{"id":224,"text":"<div class=\\\"input-group\\\" style=\\\"margin-bottom: 12px;\\\">\n                        <label>Path</label>\n                        <input type=\\\"text\\\" name=\\\"repo_path_${repo.name}\\\" value=\\\"${repo.path || ''}\\\" />\n                    </div>\n                    <div class=\\\"input-group\\\" style=\\\"margin-bottom: 12px;\\\">\n                        <label>Keywords (comma-separated)</label>\n                        <input type=\\\"text\\\" name=\\\"repo_keywords_${repo.name}\\\" value=\\\"${(repo.keywords||[]).join(',')}\\\" list=\\\"keywords-list\\\" placeholder=\\\"search or type to add\\\" />\n                    </div>\n                    <div class=\\\"input-group\\\" style=\\\"margin-bottom: 12px;\\\">\n                        <label>Path Boosts (comma-separated)</label>\n                        <input type=\\\"text\\\" name=\\\"repo_pathboosts_${repo.name}\\\" value=\\\"${(repo.path_boosts||[]).join(',')}\\\" />\n                    </div>\n                    <div class=\\\"input-group\\\">\n                        <label>Layer Bonuses (JSON)</label>\n                        <textarea name=\\\"repo_layerbonuses_${repo.name}\\\" rows=\\\"3\\\">${repo.layer_bonuses ? JSON.stringify(repo.layer_bonuses, null, 2) : ''}</textarea>\n                    </div>\n                    <div class=\\\"input-group full-width\\\" style=\\\"margin-top:12px;\\\">\n                        <label>Keyword Manager</label>"}
{"id":225,"text":"<div style=\\\"display:grid; grid-template-columns: 1fr auto 1fr; gap:8px; align-items:center;\\\">\n                            <div>\n                                <div style=\\\"display:flex; gap:6px; margin-bottom:6px;\\\">\n                                    <input type=\\\"text\\\" id=\\\"kw-filter-${rname}\\\" placeholder=\\\"filter...\\\" style=\\\"width:60%;\\\">\n                                    <select id=\\\"kw-src-${rname}\\\">\n                                        <option value=\\\"all\\\">All</option>\n                                        <option value=\\\"discriminative\\\">Discriminative</option>\n                                        <option value=\\\"semantic\\\">Semantic</option>\n                                        <option value=\\\"repos\\\">Repo</option>\n                                    </select>\n                                </div>\n                                <select id=\\\"kw-all-${rname}\\\" multiple size=\\\"8\\\" style=\\\"width:100%;\\\"></select>\n                            </div>\n                            <div style=\\\"display:flex; flex-direction:column; gap:8px;\\\">\n                                <button class=\\\"small-button\\\" id=\\\"kw-add-${rname}\\\">&gt;&gt;</button>\n                                <button class=\\\"small-button\\\" id=\\\"kw-rem-${rname}\\\">&lt;&lt;</button>\n                            </div>\n                            <div>\n                                <div class=\\\"small\\\" style=\\\"margin-bottom:6px;\\\">Repo Keywords</div>\n                                <select id=\\\"kw-repo-${rname}\\\" multiple size=\\\"8\\\" style=\\\"width:100%;\\\"></select>\n                            </div>\n                        </div>\n                    </div>"}
{"id":226,"text":"`;\n                reposSection.appendChild(div);\n\n                // Hook keyword manager events\n                const fld = div.querySelector(`[name=\\\"repo_keywords_${rname}\\\"]`);\n                const allSel = div.querySelector(`#kw-all-${rname}`);\n                const repoSel = div.querySelector(`#kw-repo-${rname}`);\n                const srcSel = div.querySelector(`#kw-src-${rname}`);\n                const filter = div.querySelector(`#kw-filter-${rname}`);\n                const addBtn = div.querySelector(`#kw-add-${rname}`);\n                const remBtn = div.querySelector(`#kw-rem-${rname}`);\n\n                function currentRepoKws() {\n                    return (fld.value || '').split(',').map(s => s.trim()).filter(Boolean);\n                }\n                function setRepoKws(arr) {\n                    fld.value = arr.join(',');\n                    // repaint repo list\n                    repoSel.innerHTML = '';\n                    arr.forEach(k => { const o=document.createElement('option'); o.value=k; o.textContent=k; repoSel.appendChild(o); });\n                }\n                function sourceList() {\n                    const cat = (srcSel.value||'all');\n                    const catMap = (state.keywordsCatalog||{});\n                    let base = [];\n                    if (cat === 'all') base = catMap.keywords||[]; else base = catMap[cat]||[];\n                    const f = (filter.value||'').toLowerCase();"}
{"id":227,"text":"const inRepo = new Set(currentRepoKws());\n                    return base.filter(k => !inRepo.has(k) && (!f || k.toLowerCase().includes(f)));\n                }\n                function paintSource() {\n                    allSel.innerHTML = '';\n                    sourceList().slice(0,500).forEach(k => { const o=document.createElement('option'); o.value=k; o.textContent=k; allSel.appendChild(o); });\n                }\n                addBtn.addEventListener('click', () => {\n                    const cur = currentRepoKws();\n                    const selected = Array.from(allSel.selectedOptions).map(o=>o.value);\n                    const next = Array.from(new Set([...cur, ...selected]));\n                    setRepoKws(next); paintSource();\n                });\n                remBtn.addEventListener('click', () => {\n                    const cur = currentRepoKws();\n                    const remove = new Set(Array.from(repoSel.selectedOptions).map(o=>o.value));\n                    const next = cur.filter(k => !remove.has(k));\n                    setRepoKws(next); paintSource();\n                });\n                srcSel.addEventListener('change', paintSource);\n                filter.addEventListener('input', paintSource);\n\n                // initial fill using existing values + catalog (if loaded later, loadKeywords will repaint)\n                setRepoKws((repo.keywords||[]));"}
{"id":228,"text":"if (state.keywordsCatalog) paintSource();\n            });\n        }\n\n        // Attach tooltips after DOM is populated\n        try { window.Tooltips && window.Tooltips.attachTooltips && window.Tooltips.attachTooltips(); } catch {}\n    }\n\n    function gatherConfigForm() {\n        const update = { env: {}, repos: [] };\n\n        // Gather all env vars from form\n        const envFields = $$('[name]').filter(f => !f.name.startsWith('repo_'));\n        envFields.forEach(field => {\n            const key = field.name;\n            let val;\n\n            if (field.type === 'checkbox') {\n                val = field.checked;\n            } else if (field.type === 'number') {\n                val = field.value;\n            } else {\n                val = field.value;\n            }\n\n            if (val !== '' && val !== null && val !== undefined) {\n                update.env[key] = val;\n            }\n        });\n\n        // Gather repo-specific fields\n        const repoFields = $$('[name^=\"repo_\"]');\n        const repoMap = {};\n\n        repoFields.forEach(field => {\n            const parts = field.name.split('_');\n            const fieldType = parts[1]; // path, keywords, pathboosts, layerbonuses\n            const repoName = parts.slice(2).join('_');\n\n            if (!repoMap[repoName]) {\n                repoMap[repoName] = { name: repoName };\n            }\n\n            if (fieldType === 'keywords' || fieldType === 'pathboosts') {"}
{"id":229,"text":"const key = fieldType === 'pathboosts' ? 'path_boosts' : 'keywords';\n                repoMap[repoName][key] = field.value.split(',').map(s => s.trim()).filter(Boolean);\n            } else if (fieldType === 'layerbonuses') {\n                try {\n                    repoMap[repoName]['layer_bonuses'] = field.value ? JSON.parse(field.value) : {};\n                } catch (e) {\n                    alert(`Invalid JSON for ${repoName} layer_bonuses: ${e.message}`);\n                    return null;\n                }\n            } else if (fieldType === 'path') {\n                repoMap[repoName]['path'] = field.value;\n            }\n        });\n\n        update.repos = Object.values(repoMap);\n        return update;\n    }\n\n    async function saveConfig() {\n        const body = gatherConfigForm();\n        if (!body) return;\n\n        try {\n            const r = await fetch(api('/api/config'), {\n                method: 'POST',\n                headers: { 'Content-Type': 'application/json' },\n                body: JSON.stringify(body)\n            });\n\n            if (!r.ok) {\n                alert('Save failed');\n                return;\n            }\n\n            const result = await r.json();\n            if (result.status === 'success') {\n                alert('Configuration updated successfully!');\n                await loadConfig(); // Reload to confirm\n            }\n        } catch (e) {\n            alert('Error saving config: ' + e.message);\n        }\n    }\n\n    // ---------------- Prices & Cost ----------------"}
{"id":230,"text":"async function loadPrices() {\n        try {\n            const r = await fetch(api('/api/prices'));\n            state.prices = await r.json();\n            populatePriceDatalists();\n        } catch (e) {\n            console.error('Failed to load prices:', e);\n        }\n    }\n\n    function unique(xs) { return Array.from(new Set(xs)); }\n\n    function populatePriceDatalists() {\n        if (!state.prices || !Array.isArray(state.prices.models)) return;\n\n        const models = state.prices.models;\n        const providers = unique(models.map(m => (m.provider || '').trim()).filter(Boolean));\n        const allModels = unique(models.map(m => (m.model || '').trim()).filter(Boolean));\n\n        const providerSelect = document.getElementById('cost-provider');\n        const modelList = document.getElementById('model-list');\n        const genList = document.getElementById('gen-model-list');\n        const rrList = document.getElementById('rerank-model-list');\n        const embList = document.getElementById('embed-model-list');\n\n        function setOpts(el, vals) {\n            if (!el) return;\n            el.innerHTML = '';\n            vals.forEach(v => {\n                const opt = document.createElement('option');\n                opt.value = v;"}
{"id":231,"text":"if (el.tagName === 'SELECT') opt.textContent = v;\n                el.appendChild(opt);\n            });\n        }\n\n        if (providerSelect && providerSelect.tagName === 'SELECT') {\n            // refill provider select only if empty, preserve user choice\n            if (providerSelect.options.length <= 1) setOpts(providerSelect, providers);\n        }\n        setOpts(modelList, allModels);\n        const genModels = unique(models\n            .filter(m => (m.family||'').includes('gen') || ['openai','anthropic','google','local','mistral','meta'].includes((m.provider||'').toLowerCase()))\n            .map(m => m.model));\n        const rrModels = unique(models\n            .filter(m => (m.family||'').includes('rerank') || ['cohere'].includes((m.provider||'').toLowerCase()) || (m.model||'').toLowerCase().includes('rerank'))\n            .map(m => m.model));\n        const embModels = unique(models\n            .filter(m => (m.family||'').includes('embed') || (m.embed_per_1k||0) > 0)\n            .map(m => m.model));\n        setOpts(genList, genModels);\n        setOpts(rrList, rrModels);\n        setOpts(embList, embModels);\n\n        if (!$('#cost-provider').value && providers.length) $('#cost-provider').value = providers[0];"}
{"id":232,"text":"if (!$('#cost-model').value && allModels.length) $('#cost-model').value = allModels[0];\n\n        // Filter model options when provider changes AND update the input value\n        const onProv = () => {\n            const modelInput = $('#cost-model');\n            if (!modelInput) return;\n\n            const p = $('#cost-provider').value.trim().toLowerCase();\n            const provModels = unique(models.filter(m => (m.provider||'').toLowerCase()===p).map(m => m.model));\n            const filtered = provModels.length ? provModels : allModels;\n\n            setOpts(modelList, filtered);\n\n            // Auto-select first model from this provider if current model doesn't match\n            if (!filtered.includes(modelInput.value)) {\n                modelInput.value = filtered[0] || '';\n            }\n        };\n\n        if (providerSelect) providerSelect.addEventListener('change', onProv);\n        onProv(); // Initialize\n    }\n\n    function buildCostPayload() {\n        const payload = {\n            provider: $('#cost-provider').value.trim(),\n            model: $('#cost-model').value.trim(),\n            tokens_in: parseInt($('#cost-in').value, 10) || 0,\n            tokens_out: parseInt($('#cost-out').value, 10) || 0,\n            embeds: parseInt($('#cost-embeds').value, 10) || 0,"}
{"id":233,"text":"reranks: parseInt($('#cost-rerank').value, 10) || 0,\n            requests_per_day: parseInt($('#cost-rpd').value, 10) || 0,\n        };\n        // Optional per-component providers/models for full pipeline costing\n        const ep = document.getElementById('cost-embed-provider');\n        const em = document.getElementById('cost-embed-model');\n        const rp = document.getElementById('cost-rerank-provider');\n        const rm = document.getElementById('cost-rerank-model');\n        if (ep && ep.value) payload.embed_provider = ep.value.trim();\n        if (em && em.value) payload.embed_model = em.value.trim();\n        if (rp && rp.value) payload.rerank_provider = rp.value.trim();\n        if (rm && rm.value) payload.rerank_model = rm.value.trim();\n        // MQ rewrites from current config (affects per-request embed/rerank cost)\n        const mq = parseInt((state.config?.env?.MQ_REWRITES)||'1', 10) || 1;\n        payload.mq_rewrites = mq;\n        return payload;\n    }\n\n    async function estimateCost() {\n        const basic = buildCostPayload();\n        // Pipeline payload includes gen model+provider and uses env to resolve embed/rerank (cohere/openai etc.)"}
{"id":234,"text":"const pipeline = {\n            gen_provider: basic.provider,\n            gen_model: basic.model,\n            tokens_in: basic.tokens_in,\n            tokens_out: basic.tokens_out,\n            embeds: basic.embeds,\n            reranks: basic.reranks,\n            requests_per_day: basic.requests_per_day,\n        };\n\n        try {\n            let r = await fetch(api('/api/cost/estimate_pipeline'), {\n                method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify(pipeline)\n            });\n            if (!r.ok) {\n                // Fallback to legacy single‑row estimator\n                r = await fetch(api('/api/cost/estimate'), { method:'POST', headers:{'Content-Type':'application/json'}, body: JSON.stringify(basic) });\n            }\n            const d = await r.json();\n            $('#cost-daily').textContent = `$${Number(d.daily||0).toFixed(4)}`;\n            $('#cost-monthly').textContent = `$${Number(d.monthly||0).toFixed(2)}`;\n        } catch (e) {\n            alert('Cost estimation failed: ' + e.message);\n        }\n    }\n\n    // ---------------- Hardware Scan & Profiles ----------------\n    function formatHardwareScan(data) {\n        if (!data || typeof data !== 'object') return 'No scan data';\n        const info = data.info || {};\n        const rt = data.runtimes || {};"}
{"id":235,"text":"const parts = [];\n\n        if (info.os) parts.push(`<div class=\"section\"><span class=\"key\">OS:</span> <span class=\"value\">${info.os}</span></div>`);\n        if (info.cpu_cores) parts.push(`<div class=\"section\"><span class=\"key\">CPU Cores:</span> <span class=\"value\">${info.cpu_cores}</span></div>`);\n        if (info.mem_gb) parts.push(`<div class=\"section\"><span class=\"key\">Memory:</span> <span class=\"value\">${info.mem_gb} GB</span></div>`);\n        if (info.gpu) parts.push(`<div class=\"section\"><span class=\"key\">GPU:</span> <span class=\"value\">${info.gpu}</span></div>`);\n\n        const activeRuntimes = Object.keys(rt).filter(k => rt[k]);\n        if (activeRuntimes.length) {\n            parts.push(`<div class=\"section\"><span class=\"key\">Runtimes:</span> <span class=\"value\">${activeRuntimes.join(', ')}</span></div>`);\n        }\n\n        return parts.join('');\n    }\n\n    async function scanHardware() {\n        try {\n            const r = await fetch(api('/api/scan-hw'), { method: 'POST' });\n            const d = await r.json();\n            const scanOut = $('#scan-out');\n            scanOut.innerHTML = formatHardwareScan(d);"}
{"id":236,"text":"scanOut.dataset.scanData = JSON.stringify(d);\n            updateWizardSummary();\n            return d;\n        } catch (e) {\n            alert('Hardware scan failed: ' + e.message);\n            return null;\n        }\n    }\n\n    function proposeProfile(scan, budget) {\n        // Budget-aware defaults (avoid paid providers at $0)\n        const hasLocal = scan?.runtimes?.ollama || scan?.runtimes?.coreml;\n        const rprov = (Number(budget) === 0) ? (hasLocal ? 'local' : 'none') : 'cohere';\n        const prof = {\n            GEN_MODEL: hasLocal && Number(budget) === 0 ? 'qwen3-coder:14b' : 'gpt-4o-mini',\n            EMBEDDING_TYPE: (Number(budget) === 0) ? (hasLocal ? 'local' : 'mxbai') : 'openai',\n            RERANK_BACKEND: rprov,\n            MQ_REWRITES: Number(budget) > 50 ? '6' : '3',\n            TOPK_SPARSE: '75',\n            TOPK_DENSE: '75',\n            FINAL_K: Number(budget) > 50 ? '20' : '10',\n            HYDRATION_MODE: 'lazy',\n        };\n        return prof;\n    }\n\n    function _tooltipHtmlForKey(k){\n        try{\n            const map = (window.Tooltips && window.Tooltips.buildTooltipMap && window.Tooltips.buildTooltipMap()) || {};\n            return map[k] || `<span class=\"tt-title\">${k}</span><div>No detailed tooltip available yet. See our docs.</div><div class=\"tt-links\"><a href=\"/files/README.md\" target=\"_blank\" rel=\"noopener\">Main README</a> <a href=\"/docs/README.md\" target=\"_blank\" rel=\"noopener\">Docs Index</a></div>`;"}
{"id":237,"text":"}catch{return `<span class=\"tt-title\">${k}</span><div>No details found.</div>`}\n    }\n\n    function formatProfile(prof) {\n        if (!prof || typeof prof !== 'object') return '(Preview will appear here)';\n        const parts = [];\n\n        const keyGroups = {\n            'Generation': ['GEN_MODEL', 'ENRICH_MODEL', 'ENRICH_MODEL_OLLAMA'],\n            'Embeddings': ['EMBEDDING_TYPE', 'VOYAGE_EMBED_DIM', 'EMBEDDING_DIM'],\n            'Reranking': ['RERANK_BACKEND', 'COHERE_RERANK_MODEL', 'RERANKER_MODEL'],\n            'Retrieval': ['MQ_REWRITES', 'FINAL_K', 'TOPK_SPARSE', 'TOPK_DENSE', 'HYDRATION_MODE'],\n        };\n\n        for (const [group, keys] of Object.entries(keyGroups)) {\n            const groupItems = keys.filter(k => prof[k] !== undefined).map(k => {\n                const tip = _tooltipHtmlForKey(k);\n                const val = String(prof[k]);\n                return `<div class=\"kv\">\n                    <span class=\"key\">${k}:</span>\n                    <span class=\"value\">${val}</span>\n                    <span class=\"tooltip-wrap\"><span class=\"help-icon\" tabindex=\"0\" aria-label=\"Help: ${k}\">?</span><div class=\"tooltip-bubble\">${tip}</div></span>\n                </div>`;\n            });\n            if (groupItems.length) {"}
{"id":238,"text":"parts.push(`<div class=\"section\"><strong style=\"color:#5b9dff;\">${group}</strong>${groupItems.join('')}</div>`);\n            }\n        }\n\n        if (prof.__estimate__) {\n            const est = prof.__estimate__;\n            parts.push(`<div class=\"section\"><strong style=\"color:#b794f6;\">Cost Estimate</strong><div><span class=\"key\">Daily:</span> <span class=\"value\">$${Number(est.daily||0).toFixed(4)}</span></div><div><span class=\"key\">Monthly:</span> <span class=\"value\">$${Number(est.monthly||0).toFixed(2)}</span></div></div>`);\n        }\n\n        return parts.join('');\n    }\n\n    function bindPreviewTooltips(){\n        const root = document.getElementById('profile-preview');\n        if (!root) return;\n        root.querySelectorAll('.kv .help-icon').forEach(icon => {\n            const wrap = icon.parentElement;\n            const bubble = wrap && wrap.querySelector('.tooltip-bubble');\n            if (!wrap || !bubble) return;\n            function show(){ bubble.classList.add('tooltip-visible'); }\n            function hide(){ bubble.classList.remove('tooltip-visible'); }\n            icon.addEventListener('mouseenter', show);"}
{"id":239,"text":"icon.addEventListener('mouseleave', hide);\n            icon.addEventListener('focus', show);\n            icon.addEventListener('blur', hide);\n            icon.addEventListener('click', (e)=>{ e.stopPropagation(); bubble.classList.toggle('tooltip-visible'); });\n            document.addEventListener('click', (evt)=>{ if (!wrap.contains(evt.target)) bubble.classList.remove('tooltip-visible'); });\n        });\n    }\n\n    async function generateProfileWizard() {\n        let scan = null;\n        const scanOut = $('#scan-out');\n        // Try to extract scan from data attribute or re-scan\n        if (scanOut.dataset.scanData) {\n            try { scan = JSON.parse(scanOut.dataset.scanData); } catch {}\n        }\n        if (!scan) scan = await scanHardware();\n        const budget = parseFloat($('#budget').value || '0');\n        const prof = (window.ProfileLogic && window.ProfileLogic.buildWizardProfile) ? window.ProfileLogic.buildWizardProfile(scan, budget) : {};\n\n        // Try a pipeline cost preview\n        const payload = (window.CostLogic && window.CostLogic.buildPayloadFromUI) ? window.CostLogic.buildPayloadFromUI() : {\n            gen_provider:'openai', gen_model:'gpt-4o-mini', tokens_in:0, tokens_out:0, embeds:0, reranks:0, requests_per_day:0"}
{"id":240,"text":"};\n        try {\n            const r = await fetch(api('/api/cost/estimate_pipeline'), { method:'POST', headers:{'Content-Type':'application/json'}, body: JSON.stringify(payload) });\n            const d = await r.json();\n            prof.__estimate__ = d;\n        } catch {}\n        $('#profile-preview').innerHTML = formatProfile(prof);\n        bindPreviewTooltips();\n        $('#profile-preview').dataset.profileData = JSON.stringify(prof);\n        updateWizardSummary();\n        return prof;\n    }\n\n    async function applyProfileWizard() {\n        let prof = null;\n        const preview = $('#profile-preview');\n        if (preview.dataset.profileData) {\n            try { prof = JSON.parse(preview.dataset.profileData); } catch {}\n        }\n        if (!prof || typeof prof !== 'object') prof = await generateProfileWizard();\n        // Remove cost estimate from applied profile\n        if (prof.__estimate__) delete prof.__estimate__;\n        try {\n            const r = await fetch(api('/api/profiles/apply'), { method:'POST', headers:{'Content-Type':'application/json'}, body: JSON.stringify({ profile: prof }) });\n            const d = await r.json();\n            alert(`Profile applied: ${d.applied_keys?.join(', ') || 'ok'}`);"}
{"id":241,"text":"await loadConfig();\n        } catch (e) { alert('Failed to apply profile: ' + e.message); }\n    }\n\n    // Tri-Candidate Generation (from docs)\n    function generateCandidates(scan, budget) {\n        const hasLocal = !!(scan?.runtimes?.ollama || scan?.runtimes?.coreml);\n        const mem = (scan?.info?.mem_gb || 8);\n        const budgetNum = Number(budget) || 0;\n\n        // Three baseline candidates\n        const local = {\n            name: 'local',\n            env: {\n                GEN_MODEL: hasLocal ? 'qwen3-coder:14b' : 'gpt-4o-mini',\n                EMBEDDING_TYPE: hasLocal ? 'local' : 'mxbai',\n                RERANK_BACKEND: hasLocal ? 'local' : 'none',\n                MQ_REWRITES: mem >= 32 ? '4' : '3',\n                FINAL_K: mem >= 32 ? '10' : '8',\n                TOPK_DENSE: '60', TOPK_SPARSE: '60', HYDRATION_MODE: 'lazy'\n            }\n        };\n        const cheapCloud = {\n            name: 'cheap_cloud',\n            env: {\n                GEN_MODEL: 'gpt-4o-mini', EMBEDDING_TYPE: 'openai', RERANK_BACKEND: 'local',\n                MQ_REWRITES: budgetNum > 25 ? '4' : '3',\n                FINAL_K: budgetNum > 25 ? '10' : '8',\n                TOPK_DENSE: '75', TOPK_SPARSE: '75', HYDRATION_MODE: 'lazy'\n            }\n        };\n        const premium = {\n            name: 'premium',\n            env: {\n                GEN_MODEL: 'gpt-4o-mini', EMBEDDING_TYPE: 'openai', RERANK_BACKEND: 'cohere',"}
{"id":242,"text":"MQ_REWRITES: budgetNum > 100 ? '6' : '4',\n                FINAL_K: budgetNum > 100 ? '20' : '12',\n                TOPK_DENSE: '120', TOPK_SPARSE: '120', HYDRATION_MODE: 'lazy'\n            }\n        };\n        return [local, cheapCloud, premium];\n    }\n\n    async function triCostSelect() {\n        // Use current Cost panel inputs for tokens and rpd\n        const base = {\n            tokens_in: parseInt($('#cost-in').value || '500', 10),\n            tokens_out: parseInt($('#cost-out').value || '800', 10),\n            embeds: parseInt($('#cost-embeds').value || '0', 10),\n            reranks: parseInt($('#cost-rerank').value || '0', 10),\n            requests_per_day: parseInt($('#cost-rpd').value || '100', 10)\n        };\n        const budget = parseFloat($('#budget').value || '0');\n        const scanOut = $('#scan-out');\n        let scan = null;\n        if (scanOut && scanOut.dataset.scanData) {\n            try { scan = JSON.parse(scanOut.dataset.scanData); } catch {}\n        }\n        if (!scan) scan = await scanHardware();\n\n        const cands = generateCandidates(scan, budget);\n\n        const rows = [];\n        for (const c of cands) {\n            // Decide provider/model from env for cost call\n            const provider = (c.env.GEN_MODEL || '').match(/:/) ? 'local' : 'openai';\n            const model = c.env.GEN_MODEL || 'gpt-4o-mini';"}
{"id":243,"text":"const payload = (window.CostLogic && window.CostLogic.buildPayloadFromUI) ? window.CostLogic.buildPayloadFromUI() : { gen_provider: provider, gen_model: model, ...base };\n            payload.gen_provider = provider; payload.gen_model = model;\n\n            // local electricity optional if provider==local\n            if (provider === 'local') {\n                const kwh = $('#cost-kwh')?.value;\n                const watts = $('#cost-watts')?.value;\n                const hours = $('#cost-hours')?.value;\n                if (kwh) payload.kwh_rate = parseFloat(kwh);\n                if (watts) payload.watts = parseInt(watts, 10);\n                if (hours) payload.hours_per_day = parseFloat(hours);\n            }\n            // Call cost API\n            const r = await fetch(api('/api/cost/estimate'), {\n                method: 'POST',\n                headers: {'Content-Type':'application/json'},\n                body: JSON.stringify(payload)\n            });\n            const d = await r.json();\n            rows.push({\n                name: c.name,\n                env: c.env,\n                provider,\n                model,\n                daily: d.daily,\n                monthly: d.monthly,\n                breakdown: d.breakdown\n            });\n        }\n\n        // Rank by monthly (ascending), then prefer cheaper that meet budget if budget>0\n        const ranked = rows.sort((a,b) => a.monthly - b.monthly);\n        let winner = ranked[0];"}
{"id":244,"text":"if (budget > 0) {\n            const within = ranked.filter(r => r.monthly <= budget);\n            if (within.length) winner = within[within.length - 1]; // Pick most expensive within budget\n        }\n\n        const triOut = $('#tri-out');\n        if (triOut) {\n            const lines = [];\n            ranked.forEach(r => {\n                const mark = r.name === winner.name ? '✓' : ' ';\n                const header = `${mark} ${r.name.toUpperCase().padEnd(15)} $${r.monthly.toFixed(2)}/mo`;\n                lines.push(header);\n                lines.push(`  Inference:  ${r.env.GEN_MODEL || '—'}`);\n                lines.push(`  Embedding:  ${r.env.EMBEDDING_TYPE || '—'}`);\n                lines.push(`  Rerank:     ${r.env.RERANK_BACKEND || 'none'}`);\n                lines.push(`  MQ:${r.env.MQ_REWRITES||'3'}  Final-K:${r.env.FINAL_K||'10'}  Sparse:${r.env.TOPK_SPARSE||'75'}  Dense:${r.env.TOPK_DENSE||'75'}`);\n                lines.push('');\n            });\n            triOut.textContent = lines.join('\\n').trim();\n        }\n\n        return { winner, ranked };\n    }\n\n    async function triChooseAndApply() {\n        console.log('[AUTO-PROFILE] Button clicked - starting triChooseAndApply');\n\n        // Show loading state\n        const placeholder = $('#profile-placeholder');\n        const resultsContent = $('#profile-results-content');"}
{"id":245,"text":"console.log('[AUTO-PROFILE] Elements found:', { placeholder: !!placeholder, resultsContent: !!resultsContent });\n\n        if (placeholder) placeholder.style.display = 'flex';\n        if (resultsContent) resultsContent.style.display = 'none';\n\n        // Add loading spinner to placeholder\n        if (placeholder) {\n            placeholder.innerHTML = `\n                <div style=\"display:flex;flex-direction:column;align-items:center;justify-content:center;\">\n                    <div style=\"width:48px;height:48px;border:3px solid #2a2a2a;border-top-color:#00ff88;border-radius:50%;animation:spin 1s linear infinite;margin-bottom:16px;\"></div>\n                    <p style=\"font-size:14px;color:#666;\">Analyzing hardware and generating profile...</p>\n                </div>\n                <style>@keyframes spin { to { transform: rotate(360deg); } }</style>\n            `;\n        }\n\n        const { winner, ranked } = await triCostSelect();\n        const budget = Number($('#budget')?.value || 0);\n\n        // Scan hardware if not already done\n        let scan = state.hwScan;\n        if (!scan) {\n            try {\n                const r = await fetch(api('/api/scan-hw'), { method: 'POST' });\n                scan = await r.json();\n                state.hwScan = scan;"}
{"id":246,"text":"} catch (e) {\n                console.error('HW scan failed:', e);\n                scan = null;\n            }\n        }\n\n        // Render rich profile display using ProfileRenderer\n        if (window.ProfileRenderer && resultsContent) {\n            try {\n                const html = window.ProfileRenderer.renderProfileResults(winner.env, scan, budget);\n                resultsContent.innerHTML = html;\n                // Bind tooltips inside the rendered preview\n                if (window.ProfileRenderer.bindTooltips) window.ProfileRenderer.bindTooltips(resultsContent);\n\n                // Hide placeholder, show results\n                if (placeholder) placeholder.style.display = 'none';\n                resultsContent.style.display = 'block';\n            } catch (err) {\n                console.error('ProfileRenderer error:', err);\n                // Fallback to simple display\n                if (resultsContent) {\n                    resultsContent.innerHTML = '<pre style=\"color:#ff6b6b;padding:20px;\">Error rendering profile: ' + err.message + '</pre>';\n                    resultsContent.style.display = 'block';\n                    if (placeholder) placeholder.style.display = 'none';\n                }\n            }\n        } else {\n            console.error('ProfileRenderer not available:', { hasRenderer: !!window.ProfileRenderer, hasContent: !!resultsContent });"}
{"id":247,"text":"// Fallback to old method\n            if (resultsContent) {\n                resultsContent.innerHTML = '<pre style=\"padding:20px;color:#aaa;\">' + JSON.stringify(winner.env, null, 2) + '</pre>';\n                resultsContent.style.display = 'block';\n                if (placeholder) placeholder.style.display = 'none';\n            }\n        }\n\n        // Wire up action buttons (always, regardless of renderer)\n        const applyBtn = document.getElementById('apply-profile-btn');\n        if (applyBtn) {\n            applyBtn.addEventListener('click', async () => {\n                const r = await fetch(api('/api/profiles/apply'), {\n                    method: 'POST',\n                    headers: { 'Content-Type':'application/json' },\n                    body: JSON.stringify({ profile: winner.env })\n                });\n                if (!r.ok) {\n                    alert('Apply failed');\n                    return;\n                }\n                alert(`✓ Applied: ${winner.name} ($${winner.monthly.toFixed(2)}/mo)\\n\\nSettings are now active. Refresh the page to see updated values.`);\n                await loadConfig();\n            });\n        }\n\n        const exportBtn = document.getElementById('export-profile-btn');\n        if (exportBtn) {\n            exportBtn.addEventListener('click', () => {\n                const blob = new Blob([JSON.stringify(winner.env, null, 2)], { type: 'application/json' });"}
{"id":248,"text":"const url = URL.createObjectURL(blob);\n                const a = document.createElement('a');\n                a.href = url;\n                a.download = `profile-${winner.name.toLowerCase().replace(/[^a-z0-9]/g, '-')}-${Date.now()}.json`;\n                a.click();\n                URL.revokeObjectURL(url);\n            });\n        }\n\n        const saveBtn = document.getElementById('save-profile-btn');\n        if (saveBtn) {\n            saveBtn.addEventListener('click', async () => {\n                const name = prompt('Profile name:', winner.name.toLowerCase().replace(/[^a-z0-9]/g, '-'));\n                if (!name) return;\n                const r = await fetch(api('/api/profiles/save'), {\n                    method: 'POST',\n                    headers: { 'Content-Type':'application/json' },\n                    body: JSON.stringify({ name, profile: winner.env })\n                });\n                if (r.ok) {\n                    alert(`✓ Saved as \"${name}\"`);\n                    await loadProfiles();\n                } else {\n                    alert('Save failed');\n                }\n            });\n        }\n    }\n\n    // Wizard helpers\n    function buildWizardProfile(scan, budget) {\n        // Legacy single-profile builder (kept for compatibility)\n        const hasLocal = scan?.runtimes?.ollama || scan?.runtimes?.coreml;\n        const budgetNum = Number(budget) || 0;\n        const defaultGen = hasLocal && budgetNum === 0 ? 'qwen3-coder:14b' : 'gpt-4o-mini';"}
{"id":249,"text":"const defaultEmb = budgetNum === 0 ? (hasLocal ? 'local' : 'mxbai') : 'openai';\n        const defaultRprov = budgetNum === 0 ? (hasLocal ? 'local' : 'none') : 'cohere';\n\n        const profile = {\n            GEN_MODEL: defaultGen,\n            EMBEDDING_TYPE: defaultEmb,\n            RERANK_BACKEND: defaultRprov,\n            MQ_REWRITES: budgetNum > 50 ? '6' : '3',\n            FINAL_K: budgetNum > 50 ? '20' : '10',\n            TOPK_SPARSE: budgetNum > 50 ? '120' : '75',\n            TOPK_DENSE: budgetNum > 50 ? '120' : '75',\n            HYDRATION_MODE: 'lazy',\n        };\n        return profile;\n    }\n\n    function seedWizardFromEnv(env) {\n        const wzGen = $('#wizard-gen-model');\n        if (wzGen && env.GEN_MODEL) wzGen.value = env.GEN_MODEL;\n        const wzEmb = $('#wizard-embed-provider');\n        if (wzEmb && env.EMBEDDING_TYPE) wzEmb.value = env.EMBEDDING_TYPE;\n        const wzRprov = $('#wizard-rerank-provider');\n        if (wzRprov && env.RERANK_BACKEND) wzRprov.value = env.RERANK_BACKEND;\n        const wzRmod = $('#wizard-rerank-model');\n        if (wzRmod && (env.COHERE_RERANK_MODEL || env.RERANKER_MODEL)) wzRmod.value = env.COHERE_RERANK_MODEL || env.RERANKER_MODEL;\n    }\n\n    function loadWizardFromEnv() {\n        const env = (state.config && state.config.env) || {};"}
{"id":250,"text":"seedWizardFromEnv(env);\n        updateWizardSummary();\n    }\n\n    function updateWizardSummary() {\n        const scanOut = $('#scan-out');\n        let hw = '';\n        if (scanOut && scanOut.dataset.scanData) {\n            try {\n                const s = JSON.parse(scanOut.dataset.scanData);\n                hw = `${s.info?.cpu_cores||'?'} cores, ${s.info?.mem_gb||'?'} GB RAM, runtimes: ${Object.keys(s.runtimes||{}).filter(k=>s.runtimes[k]).join(', ')||'none'}`;\n            } catch { hw = '(hardware not scanned)'; }\n        } else {\n            hw = '(hardware not scanned)';\n        }\n        const gen = ($('#wizard-gen-model')?.value || '(GEN_MODEL not set)');\n        const emb = ($('#wizard-embed-provider')?.value || (state.config?.env?.EMBEDDING_TYPE || '(use current)'));\n        const rprov = ($('#wizard-rerank-provider')?.value || (state.config?.env?.RERANK_BACKEND || '(use current)'));\n        const rmod = ($('#wizard-rerank-model')?.value || state.config?.env?.COHERE_RERANK_MODEL || state.config?.env?.RERANKER_MODEL || '');\n        const budget = $('#budget')?.value || '0';\n        const line = `Hardware: ${hw}\\nModels: gen=${gen}, emb=${emb}, rerank=${rprov}${rmod?`:${rmod}`:''}\\nBudget: $${budget}/mo`;"}
{"id":251,"text":"const el = $('#wizard-summary'); if (el) el.textContent = line;\n    }\n\n    // Keep summary in sync\n    ;['wizard-gen-model','wizard-embed-provider','wizard-rerank-provider','wizard-rerank-model','budget'].forEach(id => {\n        const el = document.getElementById(id); if (el) el.addEventListener('input', updateWizardSummary);\n    });\n\n    async function applyProfile() {\n        const scanText = $('#scan-out').textContent;\n        if (!scanText || scanText === '') {\n            alert('Please scan hardware first');\n            return;\n        }\n\n        const scan = JSON.parse(scanText);\n        const budget = parseFloat($('#budget').value || '0');\n        const prof = proposeProfile(scan, budget);\n\n        try {\n            const r = await fetch(api('/api/profiles/apply'), {\n                method: 'POST',\n                headers: { 'Content-Type': 'application/json' },\n                body: JSON.stringify({ profile: prof })\n            });\n\n            const d = await r.json();\n            alert(`Profile applied: ${d.applied_keys.join(', ')}`);\n            await loadConfig();\n        } catch (e) {\n            alert('Failed to apply profile: ' + e.message);\n        }\n    }\n\n    async function loadProfiles() {\n        try {\n            const r = await fetch(api('/api/profiles'));"}
{"id":252,"text":"const d = await r.json();\n            state.profiles = d.profiles || [];\n            state.defaultProfile = d.default || null;\n\n            const ul = $('#profiles-ul');\n            ul.innerHTML = '';\n            state.profiles.forEach((name) => {\n                const li = document.createElement('li');\n                li.textContent = name;\n                li.style.cssText = 'padding: 4px 0; color: #888;';\n                ul.appendChild(li);\n            });\n        } catch (e) {\n            console.error('Failed to load profiles:', e);\n        }\n    }\n\n    async function saveProfile() {\n        const name = $('#profile-name').value.trim();\n        if (!name) {\n            alert('Enter a profile name');\n            return;\n        }\n\n        // Prefer wizard preview if present; otherwise build from scan\n        let prof = null;\n        const preview = $('#profile-preview');\n        if (preview.dataset.profileData) {\n            try { prof = JSON.parse(preview.dataset.profileData); } catch {}\n        }\n        if (!prof) {\n            const scanOut = $('#scan-out');\n            if (!scanOut.dataset.scanData) { alert('Please scan hardware first'); return; }\n            const scan = JSON.parse(scanOut.dataset.scanData);\n            const budget = parseFloat($('#budget').value || '0');\n            prof = proposeProfile(scan, budget);\n        }\n        // Remove cost estimate before saving"}
{"id":253,"text":"if (prof.__estimate__) delete prof.__estimate__;\n\n        try {\n            const r = await fetch(api('/api/profiles/save'), {\n                method: 'POST',\n                headers: { 'Content-Type': 'application/json' },\n                body: JSON.stringify({ name, profile: prof })\n            });\n\n            if (!r.ok) {\n                alert('Save failed');\n                return;\n            }\n\n            await loadProfiles();\n            alert(`Saved profile: ${name}`);\n        } catch (e) {\n            alert('Failed to save profile: ' + e.message);\n        }\n    }\n\n    // ---------------- Secrets Ingest (Drag & Drop) ----------------\n    function bindDropzone() {\n        const dz = $('#dropzone');\n        const fi = $('#file-input');\n\n        function openPicker() {\n            fi.click();\n        }\n\n        dz.addEventListener('click', openPicker);\n\n        dz.addEventListener('dragover', (e) => {\n            e.preventDefault();\n            dz.style.background = '#111111';\n        });\n\n        dz.addEventListener('dragleave', (e) => {\n            dz.style.background = '';\n        });\n\n        dz.addEventListener('drop', async (e) => {\n            e.preventDefault();\n            dz.style.background = '';\n            const file = e.dataTransfer.files?.[0];\n            if (file) await ingestFile(file);\n        });\n\n        fi.addEventListener('change', async (e) => {\n            const file = e.target.files?.[0];"}
{"id":254,"text":"if (file) await ingestFile(file);\n            fi.value = '';\n        });\n    }\n\n    async function ingestFile(file) {\n        const persist = $('#persist-secrets').checked;\n        const fd = new FormData();\n        fd.append('file', file);\n        fd.append('persist', String(persist));\n\n        try {\n            const r = await fetch(api('/api/secrets/ingest'), {\n                method: 'POST',\n                body: fd\n            });\n\n            const d = await r.json();\n            $('#ingest-out').textContent = JSON.stringify(d, null, 2);\n            await loadConfig();\n        } catch (e) {\n            alert('Secrets ingest failed: ' + e.message);\n        }\n    }\n\n    // ---------------- Quick Action Helpers ----------------\n    function setButtonState(btn, state) {\n        if (!btn) return;\n        btn.classList.remove('loading', 'success', 'error');\n        if (state === 'loading') btn.classList.add('loading');\n        else if (state === 'success') btn.classList.add('success');\n        else if (state === 'error') btn.classList.add('error');\n    }\n\n    function showStatus(message, type = 'info') {\n        const status = document.getElementById('dash-index-status');\n        const bar = document.getElementById('dash-index-bar');\n        if (!status) return;\n\n        const timestamp = new Date().toLocaleTimeString();"}
{"id":255,"text":"const color = type === 'success' ? '#00ff88' : type === 'error' ? '#ff6b6b' : '#5b9dff';\n        const icon = type === 'success' ? '✓' : type === 'error' ? '✗' : '•';\n\n        status.innerHTML = `<span style=\"color:${color};\">${icon}</span> <span style=\"color:#666;\">[${timestamp}]</span> ${message}`;\n\n        if (bar) {\n            if (type === 'loading') {\n                bar.style.width = '50%';\n                bar.style.opacity = '0.6';\n            } else if (type === 'success') {\n                bar.style.width = '100%';\n                bar.style.opacity = '1';\n                setTimeout(() => { bar.style.width = '0%'; }, 2000);\n            } else if (type === 'error') {\n                bar.style.width = '100%';\n                bar.style.background = '#ff6b6b';\n                bar.style.opacity = '1';\n                setTimeout(() => {\n                    bar.style.width = '0%';\n                    bar.style.background = 'linear-gradient(90deg, #ff9b5e 0%, #ff6b9d 100%)';\n                }, 2000);\n            }\n        }\n    }\n\n    function bindQuickAction(btnId, handler) {\n        const btn = document.getElementById(btnId);\n        if (!btn) return;\n\n        btn.addEventListener('click', async (e) => {\n            e.preventDefault();\n            setButtonState(btn, 'loading');\n\n            try {\n                await handler();\n                setButtonState(btn, 'success');\n                setTimeout(() => setButtonState(btn, null), 1500);"}
{"id":256,"text":"} catch (err) {\n                console.error(`[${btnId}] Error:`, err);\n                setButtonState(btn, 'error');\n                setTimeout(() => setButtonState(btn, null), 2000);\n            }\n        });\n    }\n\n    // ---------------- Quick Actions ----------------\n    async function changeRepo() {\n        showStatus('Loading repositories...', 'loading');\n\n        try {\n            const response = await fetch(api('/api/config'));\n            const data = await response.json();\n            const repos = data.repos || [];\n            const currentRepo = data.config?.REPO || 'agro';\n\n            if (repos.length === 0) {\n                showStatus('No repositories configured', 'error');\n                return;\n            }\n\n            // Create a dialog-like selection UI\n            const repoHtml = repos.map((repo, idx) => {\n                const isActive = repo.slug === currentRepo;\n                return `\n                    <button\n                        class=\"small-button\"\n                        data-repo=\"${repo.slug}\"\n                        style=\"\n                            margin-bottom: 8px;\n                            background: ${isActive ? '#00ff88' : '#1a1a1a'};\n                            color: ${isActive ? '#000' : '#aaa'};\n                            border: 1px solid ${isActive ? '#00ff88' : '#2a2a2a'};\n                            width: 100%;\n                            text-align: left;\n                            padding: 12px;\n                            display: flex;\n                            justify-content: space-between;\n                            align-items: center;\n                        \"\n                    >\n                        <span>${repo.slug}</span>"}
{"id":257,"text":"${isActive ? '<span>✓ ACTIVE</span>' : ''}\n                    </button>\n                `;\n            }).join('');\n\n            const status = document.getElementById('dash-index-status');\n            if (status) {\n                status.innerHTML = `\n                    <div style=\"padding: 8px;\">\n                        <div style=\"margin-bottom: 12px; color: #00ff88; font-weight: 600;\">Select Repository:</div>\n                        ${repoHtml}\n                    </div>\n                `;\n\n                // Bind click handlers\n                repos.forEach(repo => {\n                    const btn = status.querySelector(`[data-repo=\"${repo.slug}\"]`);\n                    if (btn && repo.slug !== currentRepo) {\n                        btn.addEventListener('click', async () => {\n                            btn.disabled = true;\n                            btn.style.opacity = '0.6';\n                            showStatus(`Switching to ${repo.slug}...`, 'loading');\n\n                            try {\n                                const updateResponse = await fetch(api('/api/env/update'), {\n                                    method: 'POST',\n                                    headers: { 'Content-Type': 'application/json' },\n                                    body: JSON.stringify({ REPO: repo.slug })\n                                });\n\n                                if (updateResponse.ok) {\n                                    showStatus(`Switched to ${repo.slug}`, 'success');\n                                    setTimeout(() => refreshDashboard(), 500);\n                                } else {\n                                    showStatus(`Failed to switch to ${repo.slug}`, 'error');\n                                }\n                            } catch (err) {\n                                showStatus(`Error switching repo: ${err.message}`, 'error');"}
{"id":258,"text":"}\n                        });\n                    }\n                });\n            }\n        } catch (err) {\n            showStatus(`Error loading repos: ${err.message}`, 'error');\n        }\n    }\n\n    async function createKeywords() {\n        showStatus('Creating keywords...', 'loading');\n\n        try {\n            const response = await fetch(api('/api/config'));\n            const data = await response.json();\n            const repo = data.config?.REPO || 'agro';\n\n            // Call the keywords generation endpoint\n            const createResponse = await fetch(api('/api/keywords/generate'), {\n                method: 'POST',\n                headers: { 'Content-Type': 'application/json' },\n                body: JSON.stringify({ repo })\n            });\n\n            if (createResponse.ok) {\n                const result = await createResponse.json();\n                const count = result.count || 0;\n                showStatus(`Created ${count} keywords for ${repo}`, 'success');\n                await loadKeywords();\n            } else {\n                const error = await createResponse.text();\n                showStatus(`Failed to create keywords: ${error}`, 'error');\n            }\n        } catch (err) {\n            showStatus(`Error creating keywords: ${err.message}`, 'error');\n        }\n    }\n\n    async function reloadConfig() {\n        showStatus('Reloading configuration...', 'loading');\n\n        try {\n            const response = await fetch(api('/api/env/reload'), {"}
{"id":259,"text":"method: 'POST'\n            });\n\n            if (response.ok) {\n                showStatus('Configuration reloaded successfully', 'success');\n                await loadConfig();\n                await refreshDashboard();\n            } else {\n                const error = await response.text();\n                showStatus(`Failed to reload config: ${error}`, 'error');\n            }\n        } catch (err) {\n            showStatus(`Error reloading config: ${err.message}`, 'error');\n        }\n    }\n\n    // ---------------- Bindings ----------------\n    function bindActions() {\n        const btnHealth = $('#btn-health'); if (btnHealth) btnHealth.addEventListener('click', checkHealth);\n        const saveBtn = $('#save-btn'); if (saveBtn) saveBtn.addEventListener('click', saveConfig);\n        const btnEstimate = $('#btn-estimate'); if (btnEstimate) btnEstimate.addEventListener('click', estimateCost);\n        const btnScanHw = $('#btn-scan-hw'); if (btnScanHw) btnScanHw.addEventListener('click', scanHardware);\n        const legacyApply = document.getElementById('btn-apply-profile');\n        if (legacyApply) legacyApply.addEventListener('click', applyProfile);\n        const btnSaveProfile = $('#btn-save-profile'); if (btnSaveProfile) btnSaveProfile.addEventListener('click', saveProfile);"}
{"id":260,"text":"const genBtn = document.getElementById('btn-generate-profile');\n        if (genBtn) genBtn.addEventListener('click', generateProfileWizard);\n        const applyWizard = document.getElementById('btn-apply-wizard');\n        if (applyWizard) applyWizard.addEventListener('click', applyProfileWizard);\n        const oneClick = document.getElementById('btn-wizard-oneclick');\n        if (oneClick) oneClick.addEventListener('click', onWizardOneClick);\n        const loadCur = document.getElementById('btn-wizard-load-cur');\n        if (loadCur) loadCur.addEventListener('click', loadWizardFromEnv);\n\n        const addGen = document.getElementById('btn-add-gen-model');\n        if (addGen) addGen.addEventListener('click', addGenModelFlow);\n        const addEmb = document.getElementById('btn-add-embed-model');\n        if (addEmb) addEmb.addEventListener('click', addEmbedModelFlow);\n        const addRr = document.getElementById('btn-add-rerank-model');\n        if (addRr) addRr.addEventListener('click', addRerankModelFlow);\n        const addCost = document.getElementById('btn-add-cost-model');"}
{"id":261,"text":"if (addCost) addCost.addEventListener('click', addCostModelFlow);\n\n        const btnAuto = document.getElementById('btn-autotune-refresh');\n        if (btnAuto) btnAuto.addEventListener('click', refreshAutotune);\n        const cbAuto = document.getElementById('autotune-enabled');\n        if (cbAuto) cbAuto.addEventListener('change', setAutotuneEnabled);\n\n        const btnIndex = document.getElementById('btn-index-start');\n        if (btnIndex) btnIndex.addEventListener('click', startIndexing);\n        const btnCardsBuild = document.getElementById('btn-cards-build');\n        if (btnCardsBuild) btnCardsBuild.addEventListener('click', buildCards);\n        const btnCardsRefresh = document.getElementById('btn-cards-refresh');\n        if (btnCardsRefresh) btnCardsRefresh.addEventListener('click', refreshCards);\n        // Dashboard button bindings with enhanced feedback\n        bindQuickAction('dash-index-start', startIndexing);\n        bindQuickAction('dash-cards-refresh', refreshCards);\n        bindQuickAction('dash-change-repo', changeRepo);\n        bindQuickAction('dash-reload-config', reloadConfig);"}
{"id":262,"text":"// Keep cost panel in sync with wizard selections\n        const map = [\n            ['wizard-gen-model','cost-model'],\n            ['wizard-embed-provider','cost-embed-provider'],\n            ['wizard-rerank-provider','cost-rerank-provider'],\n            ['wizard-rerank-model','cost-rerank-model'],\n        ];\n        map.forEach(([a,b]) => { const elA = document.getElementById(a), elB = document.getElementById(b); if (elA && elB) elA.addEventListener('input', () => { elB.value = elA.value; }); });\n    }\n\n    // ---------------- Init ----------------\n    async function init() {\n        bindTabs();\n        bindActions();\n        bindGlobalSearchLive();\n        bindDropzone();\n        const hookBtn = document.getElementById('btn-install-hooks'); if (hookBtn) hookBtn.addEventListener('click', installHooks);\n\n        await Promise.all([\n            loadPrices(),\n            loadConfig(),\n            loadProfiles(),\n            loadKeywords()\n        ]);\n\n        await checkHealth();\n        await refreshAutotune();\n        await refreshDashboard();\n        await refreshHooksStatus();\n        addHelpTooltips();\n        // Note: comma formatting removed for cost-* fields since they are type=\"number\" inputs\n        wireDayConverters();"}
{"id":263,"text":"}\n\n    window.addEventListener('DOMContentLoaded', init);\n\n    // Decide v1 (client) vs v2 (server) auto-profile\n    async function onWizardOneClick(e){\n        try{\n            const v2 = document.getElementById('apv2-enabled');\n            if (v2 && v2.checked && window.AutoProfileV2 && typeof window.AutoProfileV2.run === 'function'){\n                e.preventDefault();\n                await window.AutoProfileV2.run();\n                return;\n            }\n        }catch{}\n        return triChooseAndApply();\n    }\n\n    // ---------------- Global Search (live) ----------------\n    function bindGlobalSearchLive() {\n        const box = document.getElementById('global-search');\n        if (!box) return;\n        const pop = document.getElementById('search-results');\n        let index = [];\n        let items = []; let cursor = -1;\n        function ensureIndex(){\n            if (index.length) return index;\n            const idx=[];\n            $$('.settings-section').forEach(sec=>{\n                const title = (sec.querySelector('h3')?.textContent||'').toLowerCase();\n                sec.querySelectorAll('.input-group').forEach(g=>{\n                    const label=(g.querySelector('label')?.textContent||'').trim();\n                    const input=g.querySelector('input,select,textarea');"}
{"id":264,"text":"if (!input) return;\n                    const name=input.name||input.id||''; const ph=input.getAttribute('placeholder')||'';\n                    const content=(title+' '+label+' '+name+' '+ph).toLowerCase();\n                    idx.push({label: `${label||name} — ${title}`, el: input, content});\n                });\n            });\n            index = idx; return idx;\n        }\n        function sectionGroupFor(el){\n            const tc = el.closest('.tab-content'); if (!tc) return 'dashboard';\n            const id = tc.id.replace('tab-','');\n            const map = { generation:'models', embeddings:'models', reranking:'models', retrieval:'retrieval', confidence:'retrieval', cards:'retrieval', repos:'repos', indexing:'repos', infra:'infra', calculator:'tools', eval:'tools', misc:'tools', dashboard:'dashboard' };\n            return map[id] || id;\n        }\n        function go(item){\n            const tab = sectionGroupFor(item.el); switchTab(tab);\n            item.el.classList.add('search-hit'); item.el.scrollIntoView({behavior:'smooth', block:'center'});\n            setTimeout(()=> item.el.classList.remove('search-hit'), 1200);\n            if (pop) pop.style.display='none';\n        }\n        function render(){"}
{"id":265,"text":"if (!pop) return; pop.innerHTML='';\n            if (!items.length){ pop.style.display='none'; return; }\n            items.slice(0,12).forEach((r,i)=>{\n                const div=document.createElement('div'); div.className='item'+(i===cursor?' active':'');\n                div.textContent=r.label; div.addEventListener('click',()=>go(r)); pop.appendChild(div);\n            });\n            pop.style.display='block';\n        }\n        function search(q){\n            const s=q.trim().toLowerCase(); if(!s){ items=[]; render(); return; }\n            ensureIndex(); items = index.filter(x=> x.content.includes(s)); cursor=0; render();\n        }\n        document.addEventListener('click', (e)=>{ if (pop && !pop.contains(e.target) && e.target!==box) pop.style.display='none'; });\n        box.addEventListener('keydown', (e)=>{ if ((e.ctrlKey||e.metaKey) && e.key.toLowerCase()==='k'){ e.preventDefault(); box.focus(); box.select(); }});\n        box.addEventListener('input', ()=> search(box.value));\n        box.addEventListener('keydown', (e)=>{\n            if (!pop || pop.style.display!=='block') return;\n            if (e.key==='ArrowDown'){ e.preventDefault(); cursor=Math.min(cursor+1, items.length-1); render(); }"}
{"id":266,"text":"else if (e.key==='ArrowUp'){ e.preventDefault(); cursor=Math.max(cursor-1,0); render(); }\n            else if (e.key==='Enter'){ e.preventDefault(); if(items[cursor]) go(items[cursor]); }\n        });\n    }\n\n    // ---------------- Autotune ----------------\n    async function refreshAutotune() {\n        try {\n            const r = await fetch(api('/api/autotune/status'));\n            if (!r.ok) {\n                if (r.status === 403 || r.status === 402) {\n                    $('#autotune-mode').textContent = 'Pro required (set Edition to pro)';\n                } else {\n                    $('#autotune-mode').textContent = '—';\n                }\n                $('#autotune-enabled').checked = false;\n                return;\n            }\n            const d = await r.json();\n            $('#autotune-enabled').checked = !!d.enabled;\n            $('#autotune-mode').textContent = d.current_mode || '—';\n        } catch (e) {\n            $('#autotune-mode').textContent = '—';\n        }\n    }\n\n    // ---------------- Dashboard Summary ----------------\n    async function refreshDashboard() {\n        try {\n            const c = state.config || (await (await fetch(api('/api/config'))).json());\n            const repo = (c.env && (c.env.REPO || c.default_repo)) || '(none)';\n            const reposCount = (c.repos || []).length;\n            const dr = document.getElementById('dash-repo'); if (dr) dr.textContent = `${repo} (${reposCount} repos)`;"}
{"id":267,"text":"} catch {}\n\n        try {\n            const h = await (await fetch(api('/health'))).json();\n            const dh = document.getElementById('dash-health'); if (dh) dh.textContent = `${h.status}${h.graph_loaded? ' (graph ready)':''}`;\n        } catch {}\n\n        try {\n            const a = await (await fetch(api('/api/autotune/status'))).json();\n            const da = document.getElementById('dash-autotune'); if (da) da.textContent = a.enabled ? (a.current_mode || 'enabled') : 'disabled';\n        } catch { const da = document.getElementById('dash-autotune'); if (da) da.textContent = 'Pro required'; }\n\n        try {\n            const cards = await (await fetch(api('/api/cards'))).json();\n            const dc = document.getElementById('dash-cards'); if (dc) dc.textContent = `${cards.count || 0} cards`;\n        } catch {}\n\n        try {\n            const env = (state.config && state.config.env) || {};\n            const host = env.MCP_HTTP_HOST || '0.0.0.0';\n            const port = env.MCP_HTTP_PORT || '8013';\n            const path = env.MCP_HTTP_PATH || '/mcp';\n            const dm = document.getElementById('dash-mcp'); if (dm) dm.textContent = `${host}:${port}${path}`;\n        } catch {}\n\n        // Load initial index status to show metadata"}
{"id":268,"text":"try {\n            await pollIndexStatus();\n        } catch {}\n    }\n\n    // ---------------- Help Tooltips ----------------\n    function addHelpTooltips() {\n        const HELP = {\n            // Generation\n            GEN_MODEL: 'Primary inference model for generation (e.g., gpt-4o-mini or qwen3-coder:14b).',\n            OPENAI_API_KEY: 'API key for OpenAI-compatible endpoints (generation/embeddings).',\n            OPENAI_BASE_URL: 'Optional OpenAI-compatible base URL (vLLM/proxy).',\n            OLLAMA_URL: 'Local model endpoint (Ollama or MLX serve).',\n            ENRICH_MODEL: 'Model used to enrich code chunks before embedding (text summaries).',\n            ENRICH_MODEL_OLLAMA: 'Local enrich model for Ollama/MLX.',\n            GEN_MODEL_HTTP: 'Override GEN_MODEL for HTTP server responses only.',\n            GEN_MODEL_MCP: 'Override GEN_MODEL for MCP tool responses only.',\n            GEN_MODEL_CLI: 'Override GEN_MODEL for CLI chat only.',\n            ENRICH_BACKEND: 'Force enrich backend (mlx or ollama).',\n\n            // Embeddings\n            EMBEDDING_TYPE: 'Embedding provider for dense vector search (openai, voyage, mxbai, local).',\n            VOYAGE_API_KEY: 'API key for Voyage embeddings.',\n            VOYAGE_EMBED_DIM: 'Output dimension for Voyage embeddings.',"}
{"id":269,"text":"EMBEDDING_DIM: 'Embedding dimension for MXBAI/local models.',\n            SKIP_DENSE: 'If 1, skip building dense vectors/Qdrant (sparse-only).',\n            ENRICH_CODE_CHUNKS: 'If true, store per-chunk summaries/keywords before embedding.',\n\n            // Reranking\n            RERANK_BACKEND: 'Cross-encoder reranking backend: local, hf, cohere, or none.',\n            RERANKER_MODEL: 'Local/HF cross-encoder model (e.g., BAAI/bge-reranker-v2-m3).',\n            COHERE_API_KEY: 'API key for Cohere reranking.',\n            COHERE_RERANK_MODEL: 'Cohere reranker model (e.g., rerank-3.5).',\n            TRANSFORMERS_TRUST_REMOTE_CODE: 'Allow HF models that require remote code.',\n\n            // Retrieval\n            MQ_REWRITES: 'Multi-query expansion count (more rewrites → better recall, more cost).',\n            FINAL_K: 'Final top-K after fusion + rerank (downstream consumers use these).',\n            TOPK_DENSE: 'Number of dense candidates (Qdrant) to fuse.',\n            TOPK_SPARSE: 'Number of sparse candidates (BM25) to fuse.',\n            HYDRATION_MODE: 'lazy: hydrate code snippets on demand; none: skip hydration.',\n            HYDRATION_MAX_CHARS: 'Max characters per hydrated code snippet.',"}
{"id":270,"text":"VENDOR_MODE: 'Prefer first-party or vendor paths when scoring files.',\n            project_PATH_BOOSTS: 'CSV of path substrings to boost (e.g., app/,lib/,config/).',\n            CARDS_MAX: 'Limit number of cards used for boosting (0 = all).',\n\n            // Confidence\n            CONF_TOP1: 'Accept answer if top-1 rerank score exceeds this threshold.',\n            CONF_AVG5: 'Accept if average of top-5 rerank scores exceeds this threshold.',\n            CONF_ANY: 'Accept if overall confidence exceeds this fallback threshold.',\n\n            // Infra\n            QDRANT_URL: 'Qdrant endpoint for vector search.',\n            REDIS_URL: 'Redis for LangGraph memory/checkpointer.',\n            REPO: 'Active repository tag for routing and output directories.',\n            COLLECTION_SUFFIX: 'Optional suffix to group collections in Qdrant.',\n            COLLECTION_NAME: 'Override Qdrant collection name.',\n            REPO_PATH: 'Fallback path when repos.json is absent.',\n            REPOS_FILE: 'Path to repos.json configuration file.',\n            OUT_DIR_BASE: 'Base output directory for per-repo data.',\n            RAG_OUT_BASE: 'Alternate env for OUT_DIR_BASE.',\n            MCP_HTTP_HOST: 'Host for MCP HTTP server.',\n            MCP_HTTP_PORT: 'Port for MCP HTTP server.',"}
{"id":271,"text":"MCP_HTTP_PATH: 'Path prefix for MCP HTTP server.',\n\n            // Misc\n            AGRO_EDITION: 'Edition gate (oss, pro, enterprise). Pro/Enterprise unlock Autotune/Compat.',\n            THREAD_ID: 'LangGraph thread id (http or cli-chat).',\n            PORT: 'Uvicorn port for serve entrypoints.',\n            PROJECT_PATH: 'Optional reference path used by some helpers.',\n            LANGCHAIN_TRACING_V2: 'Enable tracing for LangChain-compatible tooling.',\n            LANGCHAIN_PROJECT: 'Tracing project name.',\n            NETLIFY_API_KEY: 'Key for Netlify actions (if used).',\n            NETLIFY_DOMAINS: 'Comma-separated domains for Netlify deploy (if used).',\n        };\n        $$('.settings-section .input-group').forEach(g=>{\n            const label = g.querySelector('label'); const input = g.querySelector('input,select,textarea');\n            if (!label || !input) return; const key = input.name || input.id; const help = HELP[key];\n            if (!help) return; if (label.querySelector('.help')) return;\n            const tip = document.createElement('span'); tip.className='help'; tip.title = help; tip.textContent='?';\n            label.appendChild(tip);\n        });\n    }\n\n    // ---------- Numbers formatting + per‑day converters ----------"}
{"id":272,"text":"function getNum(id){ const v=document.getElementById(id); if (!v) return 0; return parseInt((v.value||'').toString().replace(/,/g,'').replace(/\\s/g,''),10)||0; }\n    function setNum(id, n){ const el=document.getElementById(id); if (!el) return; el.value = (Number(n)||0).toLocaleString('en-US'); }\n    function attachCommaFormatting(ids){ ids.forEach(id=>{ const el=document.getElementById(id); if(!el) return; el.addEventListener('focus',()=>{ el.value = el.value.replace(/,/g,''); }); el.addEventListener('blur',()=>{ const num=getNum(id); if(num >= 0) el.value = num.toLocaleString('en-US'); }); }); }\n    function wireDayConverters(){ const recalc=()=>{ const rpd=getNum('cost-rpd'); const inDay=getNum('cost-in-day'); const outDay=getNum('cost-out-day'); if(rpd>0){ if(inDay>0) setNum('cost-in', Math.floor(inDay/rpd)); if(outDay>0) setNum('cost-out', Math.floor(outDay/rpd)); } }; ['cost-in-day','cost-out-day','cost-rpd'].forEach(id=>{ const el=document.getElementById(id); if(el) el.addEventListener('input', recalc); }); recalc(); }"}
{"id":273,"text":"async function setAutotuneEnabled() {\n        try {\n            const enabled = document.getElementById('autotune-enabled').checked;\n            const r = await fetch(api('/api/autotune/status'), {\n                method: 'POST', headers: { 'Content-Type': 'application/json' },\n                body: JSON.stringify({ enabled, current_mode: null })\n            });\n            if (!r.ok) {\n                if (r.status === 403 || r.status === 402) {\n                    alert('Autotune is a Pro feature. Enable it by setting Edition to \"pro\" (Misc section) or PRO_ENABLED=1.');\n                    $('#autotune-enabled').checked = false;\n                    return;\n                }\n                throw new Error('HTTP ' + r.status);\n            }\n            await refreshAutotune();\n        } catch (e) {\n            alert('Failed to set Auto‑Tune: ' + e.message);\n        }\n    }\n\n    // ---------------- Keywords ----------------\n    async function loadKeywords() {\n        try {\n            const r = await fetch(api('/api/keywords'));\n            const d = await r.json();\n            state.keywordsCatalog = d;\n            const list = document.getElementById('keywords-list');\n            if (list) {\n                list.innerHTML = '';\n                (d.keywords || []).forEach(k => {\n                    const opt = document.createElement('option'); opt.value = k; list.appendChild(opt);\n                });\n            }\n            const kc = document.getElementById('keywords-count');"}
{"id":274,"text":"if (kc) kc.textContent = String((d.keywords||[]).length);\n            // repaint per-repo managers if present\n            ($$('#repos-section > div') || []).forEach(div => {\n                const srcSel = div.querySelector('[id^=\"kw-src-\"]');\n                const filter = div.querySelector('[id^=\"kw-filter-\"]');\n                const allSel = div.querySelector('[id^=\"kw-all-\"]');\n                const fld = div.querySelector('[name^=\"repo_keywords_\"]');\n                if (srcSel && filter && allSel && fld) {\n                    const cat = (srcSel.value||'all');\n                    const catMap = d; let base = cat==='all' ? (d.keywords||[]) : (d[cat]||[]);\n                    const f=(filter.value||'').toLowerCase(); const inRepo=new Set((fld.value||'').split(',').map(s=>s.trim()).filter(Boolean));\n                    allSel.innerHTML=''; base.filter(k=>!inRepo.has(k)&&(!f||k.toLowerCase().includes(f))).slice(0,500).forEach(k=>{const o=document.createElement('option');o.value=k;o.textContent=k;allSel.appendChild(o);});\n                }\n            });\n        } catch (e) { console.warn('keywords load failed', e); }\n    }\n\n    // ---------------- Indexing + Cards ----------------\n    let indexPoll = null;"}
{"id":275,"text":"function progressFromLog(lines) {\n        const text = (lines||[]).join(' ');\n        let pct = 5;\n        if (/Prepared \\d+ chunks/i.test(text)) pct = 20;\n        if (/BM25 index saved/i.test(text)) pct = 60;\n        if (/Indexed \\d+ chunks to Qdrant/i.test(text)) pct = 100;\n        return pct;\n    }\n\n    async function startIndexing() {\n        try {\n            showStatus('Starting indexer...', 'loading');\n            await fetch(api('/api/index/start'), { method: 'POST' });\n            if (indexPoll) clearInterval(indexPoll);\n            indexPoll = setInterval(pollIndexStatus, 800);\n            await pollIndexStatus();\n        } catch (e) {\n            showStatus('Failed to start indexer: ' + e.message, 'error');\n            throw e;\n        }\n    }\n\n    function formatBytes(bytes) {\n        if (!bytes || bytes === 0) return '0 B';\n        const k = 1024;\n        const sizes = ['B', 'KB', 'MB', 'GB'];\n        const i = Math.floor(Math.log(bytes) / Math.log(k));\n        return Math.round((bytes / Math.pow(k, i)) * 100) / 100 + ' ' + sizes[i];\n    }\n\n    function formatIndexStatus(lines, metadata) {\n        if (!metadata) {\n            if (!lines || !lines.length) return '<div style=\"color:#666;font-size:13px;\">Ready to index...</div>';\n            return `<div style=\"color:#aaa;font-size:12px;\">${lines.join('<br>')}</div>`;"}
{"id":276,"text":"}\n\n        // Enterprise-grade comprehensive display\n        const html = [];\n\n        // Header with repo/branch\n        html.push(`\n            <div style=\"display:flex;justify-content:space-between;align-items:center;margin-bottom:16px;padding-bottom:12px;border-bottom:1px solid #2a2a2a;\">\n                <div style=\"display:flex;align-items:center;gap:12px;\">\n                    <div style=\"width:6px;height:6px;border-radius:50%;background:#00ff88;box-shadow:0 0 8px #00ff88;\"></div>\n                    <div>\n                        <div style=\"font-size:16px;font-weight:600;color:#fff;letter-spacing:-0.3px;\">${metadata.current_repo}</div>\n                        <div style=\"font-size:11px;color:#666;text-transform:uppercase;letter-spacing:0.5px;margin-top:2px;\">\n                            Branch: <span style=\"color:#5b9dff;\">${metadata.current_branch}</span>\n                        </div>\n                    </div>\n                </div>\n                <div style=\"text-align:right;font-size:10px;color:#666;\">\n                    ${new Date(metadata.timestamp).toLocaleString()}\n                </div>\n            </div>\n        `);\n\n        // Configuration section\n        html.push(`\n            <div style=\"display:grid;grid-template-columns:1fr 1fr;gap:12px;margin-bottom:16px;\">"}
{"id":277,"text":"<div style=\"background:#0a0a0a;padding:12px;border-radius:6px;border:1px solid #2a2a2a;\">\n                    <div style=\"font-size:10px;color:#888;text-transform:uppercase;letter-spacing:0.5px;margin-bottom:6px;\">Embedding Model</div>\n                    <div style=\"font-size:14px;font-weight:600;color:#b794f6;font-family:'SF Mono',monospace;\">${metadata.embedding_model}</div>\n                </div>\n                <div style=\"background:#0a0a0a;padding:12px;border-radius:6px;border:1px solid #2a2a2a;\">\n                    <div style=\"font-size:10px;color:#888;text-transform:uppercase;letter-spacing:0.5px;margin-bottom:6px;\">Keywords</div>\n                    <div style=\"font-size:14px;font-weight:600;color:#ff9b5e;font-family:'SF Mono',monospace;\">${metadata.keywords_count.toLocaleString()}</div>\n                </div>\n            </div>\n        `);\n\n        // Index profiles section\n        if (metadata.repos && metadata.repos.length > 0) {\n            html.push(`<div style=\"margin-bottom:12px;\"><div style=\"font-size:11px;font-weight:600;color:#00ff88;text-transform:uppercase;letter-spacing:0.5px;margin-bottom:10px;\">Index Profiles</div>`);"}
{"id":278,"text":"metadata.repos.forEach(repo => {\n                const totalSize = (repo.sizes.chunks || 0) + (repo.sizes.bm25 || 0) + (repo.sizes.cards || 0);\n\n                html.push(`\n                    <div style=\"background:#0f0f0f;border:1px solid ${repo.has_cards ? '#006622' : '#2a2a2a'};border-radius:6px;padding:12px;margin-bottom:8px;\">\n                        <div style=\"display:flex;justify-content:space-between;align-items:start;margin-bottom:10px;\">\n                            <div>\n                                <div style=\"font-size:13px;font-weight:600;color:#fff;margin-bottom:4px;\">\n                                    ${repo.name} <span style=\"font-size:10px;color:#666;font-weight:400;\">/ ${repo.profile}</span>\n                                </div>\n                                <div style=\"font-size:11px;color:#666;\">\n                                    ${repo.chunk_count.toLocaleString()} chunks\n                                    ${repo.has_cards ? ' • <span style=\"color:#00ff88;\">✓ Cards</span>' : ' • <span style=\"color:#666;\">No cards</span>'}\n                                </div>\n                            </div>\n                            <div style=\"text-align:right;\">\n                                <div style=\"font-size:14px;font-weight:600;color:#00ff88;font-family:'SF Mono',monospace;\">\n                                    ${formatBytes(totalSize)}\n                                </div>\n                            </div>\n                        </div>\n                        <div style=\"display:grid;grid-template-columns:repeat(3,1fr);gap:8px;font-size:10px;\">"}
{"id":279,"text":"${repo.paths.chunks ? `\n                                <div style=\"background:#0a0a0a;padding:6px 8px;border-radius:4px;border:1px solid #1a1a1a;\">\n                                    <div style=\"color:#888;margin-bottom:2px;\">Chunks</div>\n                                    <div style=\"color:#5b9dff;font-family:'SF Mono',monospace;font-size:11px;\">${formatBytes(repo.sizes.chunks)}</div>\n                                </div>\n                            ` : ''}\n                            ${repo.paths.bm25 ? `\n                                <div style=\"background:#0a0a0a;padding:6px 8px;border-radius:4px;border:1px solid #1a1a1a;\">\n                                    <div style=\"color:#888;margin-bottom:2px;\">BM25 Index</div>\n                                    <div style=\"color:#ff9b5e;font-family:'SF Mono',monospace;font-size:11px;\">${formatBytes(repo.sizes.bm25)}</div>\n                                </div>\n                            ` : ''}\n                            ${repo.paths.cards ? `\n                                <div style=\"background:#0a0a0a;padding:6px 8px;border-radius:4px;border:1px solid #1a1a1a;\">\n                                    <div style=\"color:#888;margin-bottom:2px;\">Cards</div>\n                                    <div style=\"color:#00ff88;font-family:'SF Mono',monospace;font-size:11px;\">${formatBytes(repo.sizes.cards)}</div>\n                                </div>\n                            ` : ''}\n                        </div>\n                        ${repo.paths.chunks ? `\n                            <details style=\"margin-top:8px;\">"}
{"id":280,"text":"<summary style=\"cursor:pointer;font-size:10px;color:#666;padding:4px 0;\">\n                                    <span style=\"color:#5b9dff;\">▸</span> File Paths\n                                </summary>\n                                <div style=\"margin-top:6px;padding:8px;background:#0a0a0a;border-radius:4px;font-size:10px;font-family:'SF Mono',monospace;color:#888;\">\n                                    ${repo.paths.chunks ? `<div style=\"margin-bottom:2px;\">📄 ${repo.paths.chunks}</div>` : ''}\n                                    ${repo.paths.bm25 ? `<div style=\"margin-bottom:2px;\">📁 ${repo.paths.bm25}</div>` : ''}\n                                    ${repo.paths.cards ? `<div>🎴 ${repo.paths.cards}</div>` : ''}\n                                </div>\n                            </details>\n                        ` : ''}\n                    </div>\n                `);\n            });\n\n            html.push(`</div>`);\n        }\n\n        // Total storage footer\n        html.push(`\n            <div style=\"display:flex;justify-content:space-between;align-items:center;padding-top:12px;border-top:1px solid #2a2a2a;\">\n                <div style=\"font-size:12px;color:#888;text-transform:uppercase;letter-spacing:0.5px;\">Total Index Storage</div>\n                <div style=\"font-size:18px;font-weight:700;color:#00ff88;font-family:'SF Mono',monospace;\">\n                    ${formatBytes(metadata.total_storage)}"}
{"id":281,"text":"</div>\n            </div>\n        `);\n\n        return html.join('');\n    }\n\n    async function pollIndexStatus() {\n        try {\n            const r = await fetch(api('/api/index/status'));\n            const d = await r.json();\n            const box1 = document.getElementById('index-status');\n            const bar1 = document.getElementById('index-bar');\n            const box2 = document.getElementById('dash-index-status');\n            const bar2 = document.getElementById('dash-index-bar');\n\n            // Use the new comprehensive display if available\n            const formatted = (typeof window.formatIndexStatusDisplay === 'function')\n                ? window.formatIndexStatusDisplay(d.lines, d.metadata)\n                : formatIndexStatus(d.lines, d.metadata);\n\n            const pct = d.running ? 50 : (d.metadata ? 100 : 0);\n            if (box1) box1.innerHTML = formatted;\n            if (bar1) bar1.style.width = pct + '%';\n            if (box2) box2.innerHTML = formatted;\n            if (bar2) bar2.style.width = pct + '%';\n            if (!d.running && indexPoll) {\n                clearInterval(indexPoll);\n                indexPoll = null;\n                // Final complete animation\n                if (bar2) {\n                    setTimeout(() => { bar2.style.width = '0%'; }, 2000);\n                }\n            }\n        } catch (e) { /* ignore */ }\n    }\n\n    async function buildCards() {"}
{"id":282,"text":"try {\n            showStatus('Building cards...', 'loading');\n            await fetch(api('/api/cards/build'), { method: 'POST' });\n            await refreshCards();\n            showStatus('Cards built successfully', 'success');\n        } catch (e) {\n            showStatus('Failed to build cards: ' + e.message, 'error');\n            throw e;\n        }\n    }\n\n    async function refreshCards() {\n        try {\n            showStatus('Refreshing dashboard...', 'loading');\n            await refreshDashboard();\n            showStatus('Dashboard refreshed', 'success');\n        } catch (e) {\n            showStatus('Failed to refresh: ' + e.message, 'error');\n            throw e;\n        }\n    }\n\n    // ---------------- Add Model Flows ----------------\n    async function updateEnv(envUpdates) {\n        try {\n            await fetch(api('/api/config'), {\n                method: 'POST',\n                headers: { 'Content-Type': 'application/json' },\n                body: JSON.stringify({ env: envUpdates, repos: [] })\n            });\n        } catch (e) {\n            alert('Failed to update config: ' + e.message);\n        }\n    }\n\n    async function upsertPrice(entry) {\n        try {\n            await fetch(api('/api/prices/upsert'), {\n                method: 'POST',\n                headers: { 'Content-Type': 'application/json' },\n                body: JSON.stringify(entry)\n            });\n        } catch (e) {\n            console.warn('Price upsert failed:', e);"}
{"id":283,"text":"}\n    }\n\n    function promptStr(msg, defVal = '') {\n        const v = window.prompt(msg, defVal);\n        return v === null ? null : v.trim();\n    }\n\n    async function addGenModelFlow() {\n        const provider = promptStr('Provider (openai, anthropic, google, local)', 'openai');\n        if (!provider) return;\n        const model = promptStr('Model ID (e.g., gpt-4o-mini or qwen3-coder:14b)', 'gpt-4o-mini');\n        if (!model) return;\n        const baseUrl = promptStr('Base URL (optional; for proxies or local, e.g., http://127.0.0.1:11434)', '');\n        let apiKey = '';\n        if (provider !== 'local') {\n            apiKey = promptStr('API Key (optional; shown locally only)', '') || '';\n        }\n\n        // Update env\n        const env = { GEN_MODEL: model };\n        if (provider === 'openai') {\n            if (apiKey) env.OPENAI_API_KEY = apiKey;\n            if (baseUrl) env.OPENAI_BASE_URL = baseUrl;\n        } else if (provider === 'anthropic') {\n            if (apiKey) env.ANTHROPIC_API_KEY = apiKey;\n        } else if (provider === 'google') {\n            if (apiKey) env.GOOGLE_API_KEY = apiKey;\n        } else if (provider === 'local') {\n            if (baseUrl) env.OLLAMA_URL = baseUrl;\n        }\n        await updateEnv(env);\n        await loadConfig();\n\n        // Price entry (scaffold)"}
{"id":284,"text":"const entry = { provider, model, family: 'gen', base_url: baseUrl || undefined };\n        if (provider === 'local') entry.unit = 'request'; else entry.unit = '1k_tokens';\n        await upsertPrice(entry);\n        await loadPrices();\n        alert('Generation model added.');\n    }\n\n    async function addEmbedModelFlow() {\n        const provider = promptStr('Embedding provider (openai, voyage, local, mxbai)', 'openai');\n        if (!provider) return;\n        const model = promptStr('Embedding model ID (optional; depends on provider)', provider === 'openai' ? 'text-embedding-3-small' : '');\n        const baseUrl = promptStr('Base URL (optional)', '');\n        let apiKey = '';\n        if (provider !== 'local' && provider !== 'mxbai') {\n            apiKey = promptStr('API Key (optional)', '') || '';\n        }\n\n        const env = {};\n        if (provider === 'openai') {\n            env.EMBEDDING_TYPE = 'openai';\n            if (apiKey) env.OPENAI_API_KEY = apiKey;\n            if (baseUrl) env.OPENAI_BASE_URL = baseUrl;\n        } else if (provider === 'voyage') {\n            env.EMBEDDING_TYPE = 'voyage';\n            if (apiKey) env.VOYAGE_API_KEY = apiKey;\n        } else if (provider === 'mxbai') {\n            env.EMBEDDING_TYPE = 'mxbai';\n        } else if (provider === 'local') {"}
{"id":285,"text":"env.EMBEDDING_TYPE = 'local';\n        }\n        await updateEnv(env);\n        await loadConfig();\n\n        const entry = { provider, model: model || provider + '-embed', family: 'embed', base_url: baseUrl || undefined };\n        entry.unit = '1k_tokens';\n        await upsertPrice(entry);\n        await loadPrices();\n        alert('Embedding model added.');\n    }\n\n    async function addRerankModelFlow() {\n        const provider = promptStr('Rerank provider (cohere, local, hf)', 'cohere');\n        if (!provider) return;\n        let model = promptStr('Rerank model ID (e.g., rerank-3.5 or BAAI/bge-reranker-v2-m3)', provider === 'cohere' ? 'rerank-3.5' : 'BAAI/bge-reranker-v2-m3');\n        const baseUrl = promptStr('Base URL (optional)', '');\n        let apiKey = '';\n        if (provider === 'cohere') {\n            apiKey = promptStr('Cohere API Key (optional)', '') || '';\n        }\n\n        const env = {};\n        if (provider === 'cohere') {\n            env.RERANK_BACKEND = 'cohere';\n            env.COHERE_RERANK_MODEL = model;\n            if (apiKey) env.COHERE_API_KEY = apiKey;\n        } else if (provider === 'local') {\n            env.RERANK_BACKEND = 'local';\n            env.RERANKER_MODEL = model;\n        } else if (provider === 'hf') {\n            env.RERANK_BACKEND = 'hf';"}
{"id":286,"text":"env.RERANKER_MODEL = model;\n        }\n        await updateEnv(env);\n        await loadConfig();\n\n        const entry = { provider, model, family: 'rerank', base_url: baseUrl || undefined };\n        entry.unit = provider === 'cohere' ? '1k_tokens' : 'request';\n        await upsertPrice(entry);\n        await loadPrices();\n        alert('Rerank model added.');\n    }\n\n    async function addCostModelFlow() {\n        const provider = promptStr('Provider', 'openai');\n        if (!provider) return;\n        const model = promptStr('Model ID', 'gpt-4o-mini');\n        if (!model) return;\n        const baseUrl = promptStr('Base URL (optional)', '');\n        const unit = promptStr('Unit (1k_tokens or request)', provider === 'local' ? 'request' : '1k_tokens') || '1k_tokens';\n        await upsertPrice({ provider, model, family: 'misc', base_url: baseUrl || undefined, unit });\n        await loadPrices();\n        alert('Model added to pricing catalog.');\n    }\n})();"}
{"id":287,"text":"// Profile logic (algorithm only). Exported via window.ProfileLogic\n;(function(){\n  function proposeProfile(scan, budget){\n    const hasLocal = !!(scan && (scan.runtimes?.ollama || scan.runtimes?.coreml));\n    const rprov = (Number(budget) === 0) ? (hasLocal ? 'local' : 'cohere') : 'cohere';\n    return {\n      GEN_MODEL: hasLocal && Number(budget) === 0 ? 'qwen3-coder:14b' : 'gpt-4o-mini',\n      EMBEDDING_TYPE: (Number(budget) === 0) ? (hasLocal ? 'local' : 'openai') : 'openai',\n      RERANK_BACKEND: rprov,\n      MQ_REWRITES: Number(budget) > 50 ? '6' : '3',\n      TOPK_SPARSE: '75',\n      TOPK_DENSE: '75',\n      FINAL_K: Number(budget) > 50 ? '20' : '10',\n      HYDRATION_MODE: 'lazy',\n    };\n  }\n\n  function buildWizardProfile(scan, budget){\n    // Currently mirrors proposeProfile; kept separate for future tuning\n    return proposeProfile(scan, budget);\n  }\n\n  window.ProfileLogic = { proposeProfile, buildWizardProfile };\n})();"}
{"id":288,"text":";(function(){\n  function apiBase(){\n    try{\n      const u = new URL(window.location.href);\n      const q = new URLSearchParams(u.search);\n      const override = q.get('api');\n      if (override) return override.replace(/\\/$/, '');\n      if (u.port === '8012') return u.origin;\n      return 'http://127.0.0.1:8012';\n    }catch{ return 'http://127.0.0.1:8012'; }\n  }\n  function api(path){ return apiBase() + path; }\n  async function getConfig(){\n    try{ const r = await fetch(api('/api/config')); return await r.json(); }catch{ return { env:{}, repos:[] }; }\n  }\n  function csvToList(s){ return (String(s||'').split(',').map(x=>x.trim()).filter(Boolean)); }\n  function readAdvanced(){\n    const mode = document.getElementById('apv2-mode')?.value || 'balanced';\n    const budgetOverride = parseFloat(document.getElementById('apv2-budget')?.value || '');\n    const prov = Array.from(document.querySelectorAll('.apv2-prov'))\n      .filter(cb => cb.checked).map(cb => cb.value);\n    const regions = csvToList(document.getElementById('apv2-regions')?.value||'');\n    const compliance = csvToList(document.getElementById('apv2-compliance')?.value||'');"}
{"id":289,"text":"const wl = {\n      requests_per_day: parseInt(document.getElementById('apv2-rpd')?.value||'')||undefined,\n      tokens_in_per_req: parseInt(document.getElementById('apv2-tin')?.value||'')||undefined,\n      tokens_out_per_req: parseInt(document.getElementById('apv2-tout')?.value||'')||undefined,\n      mq_rewrites: parseInt(document.getElementById('apv2-mq')?.value||'')||undefined,\n      embed_tokens_per_req: parseInt(document.getElementById('apv2-embt')?.value||'')||undefined,\n      rerank_tokens_per_req: parseInt(document.getElementById('apv2-rrt')?.value||'')||undefined,\n    };\n    const slo = {\n      latency_target_ms: parseInt(document.getElementById('apv2-latency')?.value||'')||undefined,\n      min_qps: parseFloat(document.getElementById('apv2-minqps')?.value||'')||undefined,\n    };\n    return { mode, budgetOverride, prov, regions, compliance, workload: wl, slo };\n  }\n  function setPlaceholderLoading(){\n    const placeholder = document.getElementById('profile-placeholder');\n    const results = document.getElementById('profile-results-content');"}
{"id":290,"text":"if (placeholder) {\n      placeholder.style.display='flex';\n      placeholder.innerHTML = `\n        <div style=\"display:flex;flex-direction:column;align-items:center;justify-content:center;\">\n          <div style=\\\"width:48px;height:48px;border:3px solid #2a2a2a;border-top-color:#00ff88;border-radius:50%;animation:spin 1s linear infinite;margin-bottom:16px;\\\"></div>\n          <p style=\\\"font-size:14px;color:#666;\\\">Selecting profile with v2 engine...</p>\n        </div>\n        <style>@keyframes spin { to { transform: rotate(360deg); } }</style>`;\n    }\n    if (results) results.style.display='none';\n  }\n  function renderResult(env, reason, scan, budget){\n    const results = document.getElementById('profile-results-content');\n    const placeholder = document.getElementById('profile-placeholder');\n    if (window.ProfileRenderer && results) {\n      try{\n        const html = window.ProfileRenderer.renderProfileResults(env, scan, budget);\n        results.innerHTML = html;\n        if (window.ProfileRenderer.bindTooltips) window.ProfileRenderer.bindTooltips(results);\n        if (placeholder) placeholder.style.display='none';"}
{"id":291,"text":"results.style.display='block';\n      }catch(err){\n        results.innerHTML = '<pre style=\"color:#ff6b6b;padding:20px;\">'+(err?.message||String(err))+'</pre>';\n        results.style.display='block';\n        if (placeholder) placeholder.style.display='none';\n      }\n    }\n  }\n  async function ensureScan(){\n    try {\n      const out = document.getElementById('scan-out');\n      if (out && out.dataset.scanData){ return JSON.parse(out.dataset.scanData); }\n    }catch{}\n    try{ const r = await fetch(api('/api/scan-hw'), { method:'POST' }); return await r.json(); }catch{ return null; }\n  }\n\n  async function run(){\n    setPlaceholderLoading();\n    const cfg = await getConfig();\n    const env = (cfg && cfg.env) || {};\n    const scan = await ensureScan();\n    const budget = parseFloat(document.getElementById('budget')?.value||'0');\n    const adv = readAdvanced();\n    const payload = {\n      hardware: { runtimes: (scan && scan.runtimes) || {}, meta: (scan && scan.info) || {} },\n      policy: { providers_allowed: adv.prov.length? adv.prov : undefined, regions_allowed: adv.regions.length? adv.regions: undefined, compliance: adv.compliance.length? adv.compliance: undefined },"}
{"id":292,"text":"workload: Object.fromEntries(Object.entries(adv.workload).filter(([_,v])=> v!==undefined)),\n      objective: {\n        mode: adv.mode,\n        monthly_budget_usd: isNaN(adv.budgetOverride)? budget : adv.budgetOverride,\n        latency_target_ms: adv.slo.latency_target_ms,\n        min_qps: adv.slo.min_qps,\n      },\n      defaults: { gen_model: env.GEN_MODEL || '' }\n    };\n    try{\n      const r = await fetch(api('/api/profile/autoselect'), { method:'POST', headers:{'Content-Type':'application/json'}, body: JSON.stringify(payload) });\n      if (!r.ok){ const txt = await r.text(); throw new Error(txt || 'autoselect failed'); }\n      const data = await r.json();\n      renderResult(data.env, data.reason, scan, payload.objective.monthly_budget_usd || budget);\n    }catch(err){\n      const results = document.getElementById('profile-results-content');\n      const placeholder = document.getElementById('profile-placeholder');\n      if (results){ results.innerHTML = '<pre style=\"color:#ff6b6b;padding:20px;\">'+(err?.message||String(err))+'</pre>'; results.style.display='block'; }"}
{"id":293,"text":"if (placeholder) placeholder.style.display='none';\n    }\n  }\n\n  window.AutoProfileV2 = { run };\n})();"}
{"id":294,"text":"// GUI Tooltips: human-readable help + accurate links\n// Exposes window.Tooltips.{buildTooltipMap, attachTooltips}\n(function(){\n  function L(label, body, links, badges){\n    const linkHtml = (links||[]).map(([txt, href]) => `<a href=\"${href}\" target=\"_blank\" rel=\"noopener\">${txt}</a>`).join(' ');\n    const badgeHtml = (badges||[]).map(([txt, cls]) => `<span class=\"tt-badge ${cls||''}\">${txt}</span>`).join(' ');\n    const badgesBlock = badgeHtml ? `<div class=\"tt-badges\">${badgeHtml}</div>` : '';\n    return `<span class=\\\"tt-title\\\">${label}</span>${badgesBlock}<div>${body}</div>` + (links && links.length ? `<div class=\\\"tt-links\\\">${linkHtml}</div>` : '');\n  }\n\n  function buildTooltipMap(){\n    return {\n      // Infrastructure & routing\n      QDRANT_URL: L('Qdrant URL', 'HTTP URL for your Qdrant vector database. Used for dense vector queries during retrieval. If unavailable, retrieval still works via BM25 (sparse).', [\n        ['Qdrant Docs: Collections', 'https://qdrant.tech/documentation/concepts/collections/'],\n        ['Qdrant (GitHub)', 'https://github.com/qdrant/qdrant']"}
{"id":295,"text":"]),\n      REDIS_URL: L('Redis URL', 'Connection string for Redis, used for LangGraph checkpoints and optional session memory. The graph runs even if Redis is down (stateless mode).', [\n        ['Redis Docs', 'https://redis.io/docs/latest/']\n      ]),\n      REPO: L('Active Repository', 'Logical repository name for routing and indexing. MCP and CLI use this to scope retrieval.', [\n        ['Docs: MCP Quickstart', '/docs/QUICKSTART_MCP.md']\n      ]),\n      COLLECTION_NAME: L('Collection Name', 'Optional override for the Qdrant collection name. Defaults to code_chunks_{REPO}. Set this if you maintain multiple profiles.', [\n        ['Qdrant Docs: Collections', 'https://qdrant.tech/documentation/concepts/collections/']\n      ]),\n      COLLECTION_SUFFIX: L('Collection Suffix', 'Optional string appended to the default collection name for side-by-side comparisons.'),\n      REPOS_FILE: L('Repos File', 'Path to repos.json that defines repo names, paths, keywords, path boosts, and layer bonuses used for routing.', [\n        ['Local repos.json', '/files/repos.json']\n      ]),\n      REPO_PATH: L('Repo Path (fallback)', 'Absolute path to the active repo if repos.json is not available.'),"}
{"id":296,"text":"OUT_DIR_BASE: L('Out Dir Base', 'Where retrieval looks for indices (chunks.jsonl, bm25_index/). Use ./out.noindex-shared for one index across branches so MCP and local tools stay in sync. Symptom of mismatch: rag_search returns 0 results.', [\n        ['Docs: Shared Index', '/files/README.md']\n      ], [['Requires restart (MCP)','info']]),\n      RAG_OUT_BASE: L('RAG Out Base', 'Optional override for Out Dir Base; used by internal loaders if provided.'),\n      MCP_HTTP_HOST: L('MCP HTTP Host', 'Bind address for the HTTP MCP server (fast transport). Use 0.0.0.0 to listen on all interfaces.', [\n        ['Docs: Remote MCP', '/docs/REMOTE_MCP.md']\n      ]),\n      MCP_HTTP_PORT: L('MCP HTTP Port', 'TCP port for HTTP MCP server (default 8013).', [\n        ['Docs: Remote MCP', '/docs/REMOTE_MCP.md']\n      ]),\n      MCP_HTTP_PATH: L('MCP HTTP Path', 'URL path for the HTTP MCP endpoint (default /mcp).', [\n        ['Docs: Remote MCP', '/docs/REMOTE_MCP.md']\n      ]),\n\n      // Models / Providers\n      GEN_MODEL: L('Generation Model', 'Answer model. Local: qwen3-coder:14b via Ollama. Cloud: gpt-4o-mini, etc. Larger models cost more and can be slower; smaller ones are faster/cheaper.', ["}
{"id":297,"text":"['OpenAI Models', 'https://platform.openai.com/docs/models'],\n        ['Ollama API (GitHub)', 'https://github.com/ollama/ollama/blob/main/docs/api.md']\n      ], [['Affects latency','info']]),\n      OLLAMA_URL: L('Ollama URL', 'Local inference endpoint for Ollama (e.g., http://127.0.0.1:11434/api). Used when GEN_MODEL targets a local model.', [\n        ['Ollama API (GitHub)', 'https://github.com/ollama/ollama/blob/main/docs/api.md']\n      ]),\n      OPENAI_API_KEY: L('OpenAI API Key', 'API key used for OpenAI-based embeddings and/or generation.', [\n        ['OpenAI: API Keys', 'https://platform.openai.com/docs/quickstart/step-2-set-up-your-api-key'],\n        ['OpenAI Models', 'https://platform.openai.com/docs/models']\n      ]),\n      EMBEDDING_TYPE: L('Embedding Provider', 'Dense vectors (hybrid).\\n• openai — strong quality, paid\\n• voyage — strong retrieval, paid\\n• mxbai — OSS via SentenceTransformers\\n• local — any HF ST model', [\n        ['OpenAI Embeddings', 'https://platform.openai.com/docs/guides/embeddings'],\n        ['Voyage AI Embeddings', 'https://docs.voyageai.com/docs/embeddings'],"}
{"id":298,"text":"['Google Gemini Embeddings', 'https://ai.google.dev/gemini-api/docs/embeddings'],\n        ['SentenceTransformers Docs', 'https://www.sbert.net/']\n      ], [['Requires reindex','reindex']]),\n      VOYAGE_API_KEY: L('Voyage API Key', 'API key for Voyage AI embeddings when EMBEDDING_TYPE=voyage.', [\n        ['Voyage AI Docs', 'https://docs.voyageai.com/']\n      ]),\n      VOYAGE_EMBED_DIM: L('Voyage Embed Dim', 'Embedding vector dimension when using Voyage embeddings (provider‑specific). Larger dims can improve recall but increase Qdrant storage.', [], [['Requires reindex','reindex']]),\n\n      // Reranking\n      RERANK_BACKEND: L('Rerank Backend', 'Reranks fused candidates for better ordering.\\n• cohere — best quality, paid (COHERE_API_KEY)\\n• local/hf — no cost (ensure model installed)\\nDisable only to save cost.', [\n        ['Cohere Docs: Rerank', 'https://docs.cohere.com/reference/rerank'],\n        ['Cohere Python (GitHub)', 'https://github.com/cohere-ai/cohere-python']\n      ]),\n      COHERE_API_KEY: L('Cohere API Key', 'API key for Cohere reranking when RERANK_BACKEND=cohere.', ["}
{"id":299,"text":"['Cohere Dashboard: API Keys', 'https://dashboard.cohere.com/api-keys']\n      ]),\n      COHERE_RERANK_MODEL: L('Cohere Rerank Model', 'Cohere rerank model name (e.g., rerank-3.5). Check the provider docs for the latest list and pricing.', [\n        ['Cohere Docs: Models', 'https://docs.cohere.com/docs/models']\n      ]),\n      RERANKER_MODEL: L('Local Reranker (HF)', 'Name of local/HuggingFace reranker model when RERANK_BACKEND=local or hf.'),\n\n      // Retrieval tuning\n      MQ_REWRITES: L('Multi‑Query Rewrites', 'Rewrite the user query N times to broaden recall; then fuse + rerank. Start at 3–4; raise to 6 for “Where is X implemented?” questions.', [], [['Affects latency','info']]),\n      TOPK_DENSE: L('Top‑K Dense', 'Vector hits before fusion. Higher = better recall, more latency. Typical 60–120; start at 75.', [], [['Affects latency','info']]),\n      TOPK_SPARSE: L('Top‑K Sparse', 'BM25 hits before fusion. Higher = better recall, more latency. Typical 60–120; start at 75.', [\n        ['BM25S (GitHub)', 'https://github.com/xhluca/bm25s']\n      ], [['Affects latency','info']]),"}
{"id":300,"text":"FINAL_K: L('Final Top‑K', 'Results returned after rerank and boosts. Typical 10; increase for browsing, decrease for speed.'),\n      HYDRATION_MODE: L('Hydration Mode', 'Attach code bodies to results.\\n• lazy — on‑demand (recommended)\\n• none — skip hydration (lowest memory)'),\n      HYDRATION_MAX_CHARS: L('Hydration Max Chars', 'Max characters of code to attach per result. Lower to reduce RAM usage.'),\n\n      // Confidence\n      CONF_TOP1: L('Confidence Top‑1', 'Minimum score to accept top‑1 directly. Recommended ~0.60–0.65. Lower = more answers, more risk.'),\n      CONF_AVG5: L('Confidence Avg‑5', 'Average of top‑5; gate for rewriting loops. Recommended ~0.52–0.58.'),\n      CONF_ANY: L('Confidence Any', 'Proceed if any candidate exceeds this score (fallback).'),\n\n      // Netlify\n      NETLIFY_API_KEY: L('Netlify API Key', 'Key for the netlify_deploy MCP tool to trigger builds.', [\n        ['Netlify: Access Tokens', 'https://docs.netlify.com/api/get-started/#access-tokens']\n      ]),\n      NETLIFY_DOMAINS: L('Netlify Domains', 'Comma‑separated site domains you want to target with the netlify_deploy tool.'),"}
{"id":301,"text":"// Misc\n      THREAD_ID: L('Thread ID', 'Identifier for session state in LangGraph or CLI chat. Use a stable value to preserve memory across runs.', [\n        ['CLI Chat Docs', '/docs/CLI_CHAT.md']\n      ]),\n      TRANSFORMERS_TRUST_REMOTE_CODE: L('Transformers: trust_remote_code', 'Set to true only if you understand the security implications of loading remote model code.', [\n        ['Transformers: Security Notes', 'https://huggingface.co/docs/transformers/installation#security-notes']\n      ]),\n      LANGCHAIN_TRACING_V2: L('LangChain Tracing', 'Enable tracing with LangSmith (Tracing v2).', [\n        ['LangSmith Docs', 'https://docs.smith.langchain.com/']\n      ]),\n\n      GEN_MODEL_HTTP: L('HTTP Channel Model', 'Override generation model when serving via HTTP channel only. Useful to separate prod vs. local dev.'),\n      GEN_MODEL_MCP: L('MCP Channel Model', 'Override generation model when used by MCP tools only (e.g., choose a lighter model to reduce tool costs).'),\n      GEN_MODEL_CLI: L('CLI Channel Model', 'Override generation model for the CLI chat only.'),\n      NETLIFY_API_KEY: L('Netlify API Key', 'Token used by the netlify_deploy MCP tool to trigger builds.', ["}
{"id":302,"text":"['Netlify: Access Tokens', 'https://docs.netlify.com/api/get-started/#access-tokens']\n      ]),\n\n      // Additional providers\n      ANTHROPIC_API_KEY: L('Anthropic API Key', 'API key for Anthropic models (Claude family).', [\n        ['Anthropic: Getting Started', 'https://docs.anthropic.com/en/api/getting-started']\n      ]),\n      GOOGLE_API_KEY: L('Google API Key', 'API key for Google Gemini models and endpoints.', [\n        ['Gemini: API Keys', 'https://ai.google.dev/gemini-api/docs/api-key']\n      ]),\n      OPENAI_BASE_URL: L('OpenAI Base URL', 'Override API base URL for OpenAI‑compatible endpoints (advanced).', [\n        ['OpenAI Models', 'https://platform.openai.com/docs/models']\n      ]),\n\n      // Enrichment / Cards / Indexing\n      ENRICH_BACKEND: L('Enrichment Backend', 'Backend used for optional code/context enrichment (e.g., MLX or local workflows).'),\n      ENRICH_MODEL: L('Enrichment Model', 'Model used for enrichment when enabled (provider‑specific). Use a smaller local model for cost‑free summaries; cloud models improve quality.'),\n      ENRICH_MODEL_OLLAMA: L('Enrichment Model (Ollama)', 'Specific Ollama model to use for enrichment if ENRICH_BACKEND targets Ollama.'),"}
{"id":303,"text":"ENRICH_CODE_CHUNKS: L('Enrich Code Chunks', 'When enabled, stores per‑chunk summaries/keywords during indexing to support features like cards and improved reranking.', [\n        ['Cards Builder (source)', '/files/build_cards.py']\n      ]),\n      CARDS_MAX: L('Cards Max', 'Maximum number of summary cards to consider when boosting retrieval results.', [\n        ['Cards Builder (source)', '/files/build_cards.py']\n      ]),\n      SKIP_DENSE: L('Skip Dense Embeddings', 'When set, indexer skips dense embeddings/Qdrant upsert to build a fast BM25‑only index.'),\n      VENDOR_MODE: L('Vendor Mode', 'Bias for first‑party vs vendor‑origin code in reranking. Options: prefer_first_party | prefer_vendor.'),\n      EMBEDDING_DIM: L('Embedding Dimension', 'Vector length for MXBAI/local embeddings. Typical 384–768. Larger = better recall + larger Qdrant. Changing this requires full reindex.' , [], [['Requires reindex','reindex']]),\n      PORT: L('HTTP Port', 'HTTP server port for the GUI/API when running serve_rag.'),\n      AGRO_EDITION: L('Edition', 'Product edition flag (oss | pro | enterprise) to toggle advanced features in compatible deployments.'),"}
{"id":304,"text":"PORT: L('HTTP Port', 'HTTP server port for serve_rag (GUI/API). Change if port 8012 is in use.'),\n\n      // Repo editor (dynamic inputs)\n      repo_path: L('Repository Path', 'Absolute path to a repository that should be indexed for this logical name.'),\n      repo_keywords: L('Repository Keywords', 'Keywords that help route queries to this repository during retrieval. Add common terms users will ask for.'),\n      repo_pathboosts: L('Path Boosts', 'Directory substrings that should be boosted in ranking for this repository (e.g., app/, api/, server/).'),\n      repo_layerbonuses: L('Layer Bonuses', 'Per‑intent layer bonus map to tilt retrieval toward UI/server/integration code as needed.'),\n\n      // Evaluation\n      GOLDEN_PATH: L('Golden Questions Path', 'Path to your evaluation questions JSON (golden.json by default). Used by eval_loop to measure retrieval quality.', [\n        ['Eval Script', '/files/eval_loop.py'], ['Docs Index', '/docs/README.md']\n      ]),\n      BASELINE_PATH: L('Baseline Path', 'Where eval_loop saves baseline results for regression comparison.', [\n        ['Eval Script', '/files/eval_loop.py']"}
{"id":305,"text":"]),\n      EVAL_MULTI: L('Eval Multi‑Query', 'Whether eval uses multi‑query expansion (1=yes, 0=no). Turning on improves recall; increases latency.'),\n      EVAL_FINAL_K: L('Eval Final‑K', 'How many results eval considers as top‑K when scoring hits. Typical 5–10.'),\n\n      // Repo‑specific env overrides (legacy)\n      agro_PATH: L('agro PATH (legacy)', 'Legacy repo path override. Prefer REPO_PATH or repos.json configuration.' , [\n        ['Local repos.json', '/files/repos.json']\n      ]),\n      agro_PATH_BOOSTS: L('agro Path Boosts (CSV)', 'Comma‑separated path substrings to boost ranking for the agro repo (e.g., app/,lib/,config/). Mirrors per‑repo Path Boosts.'),\n      LANGCHAIN_agro: L('LangChain (agro)', 'Legacy/internal env key for tracing/metadata. Prefer LANGCHAIN_TRACING_V2 + project config.'),\n    };\n  }\n\n  function attachTooltips(){\n    const map = buildTooltipMap();\n    const fields = document.querySelectorAll('[name]');\n    fields.forEach((field) => {\n      const name = field.getAttribute('name');\n      const parent = field.closest('.input-group');\n      if (!name || !parent) return;"}
{"id":306,"text":"const label = parent.querySelector('label');\n      if (!label) return;\n      if (label.querySelector('.help-icon')) return;\n      let key = name;\n      if (name.startsWith('repo_')) {\n        const type = name.split('_')[1];\n        key = 'repo_' + type;\n      }\n      let html = map[key];\n      if (!html) {\n        html = `<span class=\\\"tt-title\\\">${name}</span><div>No detailed tooltip available yet. See our docs for related settings.</div><div class=\\\"tt-links\\\"><a href=\\\"/files/README.md\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\">Main README</a> <a href=\\\"/docs/README.md\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\">Docs Index</a></div>`;\n      }\n      const spanText = document.createElement('span');\n      spanText.className = 'label-text';\n      spanText.textContent = label.textContent;\n      label.textContent = '';\n      label.appendChild(spanText);\n      const wrap = document.createElement('span');\n      wrap.className = 'tooltip-wrap';\n      const icon = document.createElement('span');\n      icon.className = 'help-icon';\n      icon.setAttribute('tabindex', '0');\n      icon.setAttribute('aria-label', `Help: ${name}`);\n      icon.textContent = '?';"}
{"id":307,"text":"const bubble = document.createElement('div');\n      bubble.className = 'tooltip-bubble';\n      bubble.setAttribute('role', 'tooltip');\n      bubble.innerHTML = html;\n      wrap.appendChild(icon);\n      wrap.appendChild(bubble);\n      label.appendChild(wrap);\n      function show(){ bubble.classList.add('tooltip-visible'); }\n      function hide(){ bubble.classList.remove('tooltip-visible'); }\n      icon.addEventListener('mouseenter', show);\n      icon.addEventListener('mouseleave', hide);\n      icon.addEventListener('focus', show);\n      icon.addEventListener('blur', hide);\n      icon.addEventListener('click', (e) => {\n        e.stopPropagation();\n        bubble.classList.toggle('tooltip-visible');\n      });\n      document.addEventListener('click', (evt) => {\n        if (!wrap.contains(evt.target)) bubble.classList.remove('tooltip-visible');\n      });\n    });\n  }\n\n  window.Tooltips = { buildTooltipMap, attachTooltips };\n})();"}
{"id":308,"text":"// Enterprise-grade indexing status display\n// Matches storage calculator format with comprehensive metrics\nformatBytes(bytes) {\n    if (!bytes || bytes === 0) return '0 B';\n    const k = 1024;\n    const sizes = ['B', 'KB', 'MB', 'GB', 'TB'];\n    const i = Math.floor(Math.log(bytes) / Math.log(k));\n    return Math.round((bytes / Math.pow(k, i)) * 100) / 100 + ' ' + sizes[i];\n}"}
{"id":309,"text":"formatIndexStatusDisplay(lines, metadata) {\n    if (!metadata) {\n        if (!lines || !lines.length) return '<div style=\"color:#666;font-size:13px;\">Ready to index...</div>';\n        return `<div style=\"color:#aaa;font-size:12px;\">${lines.join('<br>')}</div>`;\n    }\n\n    const html = [];\n    const emb = metadata.embedding_config || {};\n    const storage = metadata.storage_breakdown || {};\n    const costs = metadata.costs || {};\n\n    // HEADER: Repo + Branch\n    html.push(`\n        <div style=\"display:flex;justify-content:space-between;align-items:center;margin-bottom:20px;padding-bottom:16px;border-bottom:2px solid #2a2a2a;\">\n            <div style=\"display:flex;align-items:center;gap:14px;\">\n                <div style=\"width:8px;height:8px;border-radius:50%;background:#00ff88;box-shadow:0 0 12px #00ff88;animation:pulse 2s ease-in-out infinite;\"></div>\n                <div>\n                    <div style=\"font-size:20px;font-weight:700;color:#fff;letter-spacing:-0.5px;\">${metadata.current_repo}</div>\n                    <div style=\"font-size:11px;color:#666;text-transform:uppercase;letter-spacing:0.8px;margin-top:4px;\">\n                        <span style=\"color:#888\">Branch:</span> <span style=\"color:#5b9dff;font-weight:600;\">${metadata.current_branch}</span>\n                    </div>\n                </div>\n            </div>\n            <div style=\"text-align:right;font-size:10px;color:#555;font-family:'SF Mono',monospace;\">\n                ${new Date(metadata.timestamp).toLocaleString()}\n            </div>\n        </div>\n    `);\n\n    // EMBEDDING CONFIGURATION\n    html.push(`\n        <div style=\"background:linear-gradient(135deg,#0a0a0a 0%,#0f0f0f 100%);padding:16px;border-radius:8px;border:1px solid #2a2a2a;margin-bottom:20px;\">\n            <div style=\"font-size:11px;font-weight:700;color:#b794f6;text-transform:uppercase;letter-spacing:1px;margin-bottom:12px;display:flex;align-items:center;gap:8px;\">\n                <svg width=\"14\" height=\"14\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"#b794f6\" stroke-width=\"2\">\n                    <circle cx=\"12\" cy=\"12\" r=\"10\"></circle>\n                    <path d=\"M12 6v6l4 2\"></path>\n                </svg>\n                Embedding Configuration\n            </div>\n            <div style=\"display:grid;grid-template-columns:repeat(3,1fr);gap:10px;\">\n                <div style=\"background:#0a0a0a;padding:10px;border-radius:6px;border:1px solid #1a1a1a;\">\n                    <div style=\"font-size:9px;color:#666;text-transform:uppercase;letter-spacing:0.5px;margin-bottom:4px;\">Model</div>\n                    <div style=\"font-size:13px;font-weight:700;color:#b794f6;font-family:'SF Mono',monospace;\">${emb.model || 'N/A'}</div>\n                </div>\n                <div style=\"background:#0a0a0a;padding:10px;border-radius:6px;border:1px solid #1a1a1a;\">\n                    <div style=\"font-size:9px;color:#666;text-transform:uppercase;letter-spacing:0.5px;margin-bottom:4px;\">Dimensions</div>\n                    <div style=\"font-size:13px;font-weight:700;color:#5b9dff;font-family:'SF Mono',monospace;\">${emb.dimensions ? emb.dimensions.toLocaleString() : 'N/A'}</div>\n                </div>\n                <div style=\"background:#0a0a0a;padding:10px;border-radius:6px;border:1px solid #1a1a1a;\">\n                    <div style=\"font-size:9px;color:#666;text-transform:uppercase;letter-spacing:0.5px;margin-bottom:4px;\">Precision</div>\n                    <div style=\"font-size:13px;font-weight:700;color:#ff9b5e;font-family:'SF Mono',monospace;\">${emb.precision || 'N/A'}</div>\n                </div>\n            </div>\n        </div>\n    `);\n\n    // COSTS (if available)\n    if (costs.total_tokens > 0) {\n        html.push(`\n            <div style=\"background:linear-gradient(135deg,#001a0f 0%,#0a0a0a 100%);padding:16px;border-radius:8px;border:1px solid #00442 2;margin-bottom:20px;\">\n                <div style=\"font-size:11px;font-weight:700;color:#00ff88;text-transform:uppercase;letter-spacing:1px;margin-bottom:12px;display:flex;align-items:center;gap:8px;\">\n                    <svg width=\"14\" height=\"14\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"#00ff88\" stroke-width=\"2\">\n                        <line x1=\"12\" y1=\"1\" x2=\"12\" y2=\"23\"></line>\n                        <path d=\"M17 5H9.5a3.5 3.5 0 0 0 0 7h5a3.5 3.5 0 0 1 0 7H6\"></path>\n                    </svg>\n                    Indexing Costs\n                </div>\n                <div style=\"display:grid;grid-template-columns:1fr 1fr;gap:10px;\">\n                    <div style=\"background:#0a0a0a;padding:10px;border-radius:6px;border:1px solid #003311;\">\n                        <div style=\"font-size:9px;color:#666;text-transform:uppercase;letter-spacing:0.5px;margin-bottom:4px;\">Total Tokens</div>\n                        <div style=\"font-size:15px;font-weight:700;color:#00ff88;font-family:'SF Mono',monospace;\">${costs.total_tokens.toLocaleString()}</div>\n                    </div>\n                    <div style=\"background:#0a0a0a;padding:10px;border-radius:6px;border:1px solid #003311;\">\n                        <div style=\"font-size:9px;color:#666;text-transform:uppercase;letter-spacing:0.5px;margin-bottom:4px;\">Embedding Cost</div>\n                        <div style=\"font-size:15px;font-weight:700;color:#00ff88;font-family:'SF Mono',monospace;\">$${costs.embedding_cost.toFixed(4)}</div>\n                    </div>\n                </div>\n            </div>\n        `);\n    }\n\n    // STORAGE BREAKDOWN (matching calculator format exactly)\n    html.push(`\n        <div style=\"background:linear-gradient(135deg,#0f0f0f 0%,#0a0a0a 100%);padding:18px;border-radius:8px;border:1px solid #2a2a2a;margin-bottom:20px;\">\n            <div style=\"font-size:11px;font-weight:700;color:#ff9b5e;text-transform:uppercase;letter-spacing:1px;margin-bottom:16px;display:flex;align-items:center;gap:8px;\">\n                <svg width=\"14\" height=\"14\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"#ff9b5e\" stroke-width=\"2\">\n                    <rect x=\"2\" y=\"3\" width=\"20\" height=\"18\" rx=\"2\" ry=\"2\"></rect>\n                    <line x1=\"2\" y1=\"9\" x2=\"22\" y2=\"9\"></line>\n                    <line x1=\"2\" y1=\"15\" x2=\"22\" y2=\"15\"></line>\n                </svg>\n                Storage Requirements\n            </div>\n            <div style=\"display:grid;grid-template-columns:repeat(2,1fr);gap:10px;\">\n                <div style=\"background:#0a0a0a;padding:12px;border-radius:6px;border:1px solid #1a1a1a;display:flex;justify-content:space-between;align-items:center;\">\n                    <span style=\"font-size:10px;color:#888;text-transform:uppercase;letter-spacing:0.5px;\">Chunks JSON</span>\n                    <span style=\"font-size:13px;font-weight:700;color:#5b9dff;font-family:'SF Mono',monospace;\">${formatBytes(storage.chunks_json)}</span>\n                </div>\n                <div style=\"background:#0a0a0a;padding:12px;border-radius:6px;border:1px solid #1a1a1a;display:flex;justify-content:space-between;align-items:center;\">\n                    <span style=\"font-size:10px;color:#888;text-transform:uppercase;letter-spacing:0.5px;\">Raw Embeddings</span>\n                    <span style=\"font-size:13px;font-weight:700;color:#b794f6;font-family:'SF Mono',monospace;\">${formatBytes(storage.embeddings_raw)}</span>\n                </div>\n                <div style=\"background:#0a0a0a;padding:12px;border-radius:6px;border:1px solid #1a1a1a;display:flex;justify-content:space-between;align-items:center;\">\n                    <span style=\"font-size:10px;color:#888;text-transform:uppercase;letter-spacing:0.5px;\">Qdrant (w/overhead)</span>\n                    <span style=\"font-size:13px;font-weight:700;color:#ff9b5e;font-family:'SF Mono',monospace;\">${formatBytes(storage.embeddings_raw + storage.qdrant_overhead)}</span>\n                </div>\n                <div style=\"background:#0a0a0a;padding:12px;border-radius:6px;border:1px solid #1a1a1a;display:flex;justify-content:space-between;align-items:center;\">\n                    <span style=\"font-size:10px;color:#888;text-transform:uppercase;letter-spacing:0.5px;\">BM25 Index</span>\n                    <span style=\"font-size:13px;font-weight:700;color:#00ff88;font-family:'SF Mono',monospace;\">${formatBytes(storage.bm25_index)}</span>\n                </div>\n                <div style=\"background:#0a0a0a;padding:12px;border-radius:6px;border:1px solid #1a1a1a;display:flex;justify-content:space-between;align-items:center;\">\n                    <span style=\"font-size:10px;color:#888;text-transform:uppercase;letter-spacing:0.5px;\">Cards/Summary</span>\n                    <span style=\"font-size:13px;font-weight:700;color:#ff6b9d;font-family:'SF Mono',monospace;\">${formatBytes(storage.cards)}</span>\n                </div>\n                <div style=\"background:#0a0a0a;padding:12px;border-radius:6px;border:1px solid #1a1a1a;display:flex;justify-content:space-between;align-items:center;\">\n                    <span style=\"font-size:10px;color:#888;text-transform:uppercase;letter-spacing:0.5px;\">Reranker Cache</span>\n                    <span style=\"font-size:13px;font-weight:700;color:#ffaa00;font-family:'SF Mono',monospace;\">${formatBytes(storage.reranker_cache)}</span>\n                </div>\n                <div style=\"background:#0a0a0a;padding:12px;border-radius:6px;border:1px solid #1a1a1a;display:flex;justify-content:space-between;align-items:center;\">\n                    <span style=\"font-size:10px;color:#888;text-transform:uppercase;letter-spacing:0.5px;\">Redis Cache</span>\n                    <span style=\"font-size:13px;font-weight:700;color:#7db3ff;font-family:'SF Mono',monospace;\">${formatBytes(storage.redis)}</span>\n                </div>\n                <div style=\"background:#0a0a0a;padding:12px;border-radius:6px;border:1px solid#1a1a1a;display:flex;justify-content:space-between;align-items:center;\">\n                    <span style=\"font-size:10px;color:#888;text-transform:uppercase;letter-spacing:0.5px;\">Keywords</span>\n                    <span style=\"font-size:13px;font-weight:700;color:#ffcf66;font-family:'SF Mono',monospace;\">${metadata.keywords_count.toLocaleString()}</span>\n                </div>\n            </div>\n        </div>\n    `);\n\n    // INDEX PROFILES (collapsible)\n    if (metadata.repos && metadata.repos.length > 0) {\n        html.push(`\n            <details style=\"margin-bottom:20px;\">\n                <summary style=\"cursor:pointer;font-size:11px;font-weight:700;color:#888;text-transform:uppercase;letter-spacing:1px;padding:12px;background:#0a0a0a;border-radius:6px;border:1px solid #2a2a2a;\">\n                    <span style=\"color:#5b9dff;\">▸</span> Index Profiles (${metadata.repos.length})\n                </summary>\n                <div style=\"margin-top:12px;padding:12px;background:#0a0a0a;border-radius:6px;border:1px solid #1a1a1a;\">\n        `);\n\n        metadata.repos.forEach(repo => {\n            const totalSize = (repo.sizes.chunks || 0) + (repo.sizes.bm25 || 0) + (repo.sizes.cards || 0);\n            html.push(`\n                <div style=\"padding:10px;margin-bottom:8px;background:#0f0f0f;border-radius:4px;border:1px solid ${repo.has_cards ? '#003311' : '#1a1a1a'};\">\n                    <div style=\"display:flex;justify-content:space-between;margin-bottom:6px;\">\n                        <div style=\"font-size:12px;font-weight:600;color:#fff;\">${repo.name} <span style=\"color:#666;font-weight:400;\">/ ${repo.profile}</span></div>\n                        <div style=\"font-size:12px;font-weight:700;color:#00ff88;font-family:'SF Mono',monospace;\">${formatBytes(totalSize)}</div>\n                    </div>\n                    <div style=\"font-size:10px;color:#666;\">${repo.chunk_count.toLocaleString()} chunks ${repo.has_cards ? '• <span style=\"color:#00ff88;\">✓ Cards</span>' : ''}</div>\n                </div>\n            `);\n        });\n\n        html.push(`</div></details>`);\n    }\n\n    // TOTAL FOOTER\n    html.push(`\n        <div style=\"display:flex;justify-content:space-between;align-items:center;padding:18px;background:linear-gradient(135deg,#0f1f0f 0%,#0a0a0a 100%);border-radius:8px;border:2px solid #00ff88;\">\n            <div style=\"font-size:13px;font-weight:700;color:#888;text-transform:uppercase;letter-spacing:1px;\">Total Index Storage</div>\n            <div style=\"font-size:24px;font-weight:900;color:#00ff88;font-family:'SF Mono',monospace;text-shadow:0 0 20px rgba(0,255,136,0.3);\">\n                ${formatBytes(metadata.total_storage)}\n            </div>\n        </div>\n    `);\n\n    return html.join('');\n}\n\n// Export for use in app.js\nif (typeof window !== 'undefined') {\n    window.formatIndexStatusDisplay = formatIndexStatusDisplay;\n}"}
{"id":310,"text":"// Cost calculator logic. Exported via window.CostLogic\n;(function(){\n  function readInt(id, d){ const el=document.getElementById(id); const v=el?el.value:''; const n=parseInt(v||'',10); return Number.isFinite(n)?n:(d||0); }\n  function readStr(id, d){ const el=document.getElementById(id); const v=el?el.value:''; return (v||d||'').toString(); }\n\n  function buildBase(){\n    return {\n      tokens_in: readInt('cost-in', 500),\n      tokens_out: readInt('cost-out', 800),\n      embeds: readInt('cost-embeds', 0),\n      reranks: readInt('cost-rerank', 0),\n      requests_per_day: readInt('cost-rpd', 100),\n    };\n  }\n\n  function buildPayloadFromUI(){\n    const base = buildBase();\n    const gen_provider = readStr('cost-provider','openai').trim();\n    const gen_model = readStr('cost-model','gpt-4o-mini').trim();\n    const embed_provider = readStr('cost-embed-provider','openai').trim();\n    const embed_model = readStr('cost-embed-model','text-embedding-3-small').trim();\n    const rerank_provider = readStr('cost-rerank-provider','cohere').trim();\n    const rerank_model = readStr('cost-rerank-model','rerank-3.5').trim();"}
{"id":311,"text":"return { gen_provider, gen_model, embed_provider, embed_model, rerank_provider, rerank_model, ...base };\n  }\n\n  window.CostLogic = { buildBase, buildPayloadFromUI };\n})();"}
{"id":312,"text":"// Profile Renderer - Rich, professional display of auto-generated profiles\n;(function(){\n  \n  // Setting metadata with explanations\n  const SETTING_INFO = {\n    GEN_MODEL: {\n      name: 'Generation Model',\n      description: 'The AI model used to generate answers from retrieved code. This is the \"brain\" that synthesizes information.',\n      category: 'Generation',\n      icon: '🧠'\n    },\n    EMBEDDING_TYPE: {\n      name: 'Embedding Provider',\n      description: 'Creates vector representations of your code for semantic search. Higher quality embeddings find more relevant results.',\n      category: 'Retrieval',\n      icon: '🔍'\n    },\n    RERANK_BACKEND: {\n      name: 'Reranking Engine',\n      description: 'Re-scores retrieved results for precision. This is your quality filter that ensures the best results rise to the top.',\n      category: 'Retrieval',\n      icon: '⚡'\n    },\n    MQ_REWRITES: {\n      name: 'Multi-Query Expansion',\n      description: 'Number of query variations generated to cast a wider search net. More rewrites = better recall but higher cost.',\n      category: 'Search Strategy',\n      icon: '🎯',\n      valueExplainer: (v) => v + ' variations per query'"}
{"id":313,"text":"},\n    TOPK_SPARSE: {\n      name: 'BM25 Candidates',\n      description: 'Number of keyword-based matches to retrieve. BM25 is excellent for exact terms and technical names.',\n      category: 'Search Strategy',\n      icon: '📝',\n      valueExplainer: (v) => 'Top ' + v + ' keyword matches'\n    },\n    TOPK_DENSE: {\n      name: 'Vector Candidates',\n      description: 'Number of semantic matches to retrieve. Vector search excels at conceptual similarity.',\n      category: 'Search Strategy',\n      icon: '🎨',\n      valueExplainer: (v) => 'Top ' + v + ' semantic matches'\n    },\n    FINAL_K: {\n      name: 'Final Results',\n      description: 'After hybrid fusion and reranking, this many results are sent to generation. Balance between context and cost.',\n      category: 'Search Strategy',\n      icon: '🎁',\n      valueExplainer: (v) => v + ' final results'\n    },\n    HYDRATION_MODE: {\n      name: 'Code Hydration',\n      description: 'How full code is loaded. \"Lazy\" fetches on-demand for efficiency. \"Eager\" pre-loads everything.',\n      category: 'Performance',\n      icon: '💧'\n    }\n  };\n\n  const TIER_INFO = {\n    0: { name: 'Free Tier', color: '#4ecdc4', badge: 'LOCAL ONLY' },\n    10: { name: 'Starter', color: '#5b9dff', badge: 'BUDGET FRIENDLY' },"}
{"id":314,"text":"50: { name: 'Professional', color: '#b794f6', badge: 'BALANCED' },\n    200: { name: 'Enterprise', color: '#00ff88', badge: 'MAXIMUM PERFORMANCE' }\n  };\n\n  function renderProfileResults(profile, scan, budget) {\n    const tierInfo = TIER_INFO[budget] || { name: 'Custom', color: '#999', badge: 'CUSTOM CONFIG' };\n    \n    let html = '<div style=\"margin-bottom:24px;padding-bottom:20px;border-bottom:1px solid #2a2a2a;\">';\n    html += '<div style=\"display:flex;align-items:center;justify-content:space-between;margin-bottom:12px;\">';\n    html += '<div>';\n    html += '<h4 style=\"font-size:18px;font-weight:700;color:#fff;margin-bottom:4px;\">' + tierInfo.name + ' Profile</h4>';\n    html += '<span style=\"font-size:11px;color:' + tierInfo.color + ';font-weight:600;letter-spacing:0.8px;\">' + tierInfo.badge + '</span>';\n    html += '</div>';\n    html += '<div style=\"font-size:28px;font-weight:800;color:' + tierInfo.color + ';\">$' + budget + '/mo</div>';\n    html += '</div>';\n    \n    html += '<div style=\"background:#111;border:1px solid #2a2a2a;border-radius:6px;padding:14px;margin-top:12px;\">';"}
{"id":315,"text":"html += '<p style=\"font-size:13px;color:#aaa;line-height:1.6;margin:0;\">';\n    html += '<strong style=\"color:#00ff88;\">Baseline Configuration</strong> — ';\n    html += 'This profile gives you a strong starting point optimized for your hardware and budget. ';\n    html += 'You can fine-tune any setting in the Models, Retrieval, or Infrastructure tabs. ';\n    html += 'Consider saving multiple profiles for different use cases (e.g., \"dev-fast\" vs \"prod-quality\").';\n    html += '</p></div></div>';\n\n    html += '<div style=\"display:flex;flex-direction:column;gap:16px;margin-bottom:24px;\">';\n\n    // Group settings by category\n    const categories = {};\n    Object.keys(profile).forEach(key => {\n      const info = SETTING_INFO[key];\n      if (!info) return;\n      \n      const cat = info.category;\n      if (!categories[cat]) categories[cat] = [];\n      categories[cat].push({ key: key, value: profile[key], info: info });\n    });\n\n    // Render each category\n    Object.entries(categories).forEach(([catName, settings]) => {\n      html += '<div style=\"background:#0a0a0a;border:1px solid #2a2a2a;border-radius:6px;padding:16px;\">';"}
{"id":316,"text":"html += '<h5 style=\"font-size:12px;color:#999;text-transform:uppercase;letter-spacing:0.8px;font-weight:600;margin-bottom:14px;\">';\n      html += catName + '</h5>';\n      html += '<div style=\"display:flex;flex-direction:column;gap:12px;\">';\n\n      settings.forEach(({ key, value, info }) => {\n        const displayValue = info.valueExplainer ? info.valueExplainer(value) : value;\n        html += '<div style=\"display:flex;gap:12px;\">';\n        html += '<div style=\"font-size:20px;flex-shrink:0;width:32px;height:32px;display:flex;align-items:center;justify-content:center;background:#1a1a1a;border-radius:6px;\">';\n        html += info.icon + '</div>';\n        html += '<div style=\"flex:1;\">';\n        html += '<div style=\"display:flex;align-items:center;justify-content:space-between;margin-bottom:4px;\">';\n        html += '<span style=\"font-size:13px;font-weight:600;color:#fff;\">' + info.name + '</span>';\n        html += '<code style=\"font-size:12px;color:#00ff88;background:#0a0a0a;padding:2px 8px;border-radius:4px;font-family:\\'SF Mono\\',monospace;\">';"}
{"id":317,"text":"html += displayValue + '</code></div>';\n        html += '<p style=\"font-size:12px;color:#888;line-height:1.5;margin:0;\">' + info.description + '</p>';\n        html += '</div></div>';\n      });\n\n      html += '</div></div>';\n    });\n\n    html += '</div>';\n\n    html += '<div style=\"display:flex;gap:12px;padding-top:16px;border-top:1px solid #2a2a2a;\">';\n    html += '<button id=\"apply-profile-btn\" class=\"small-button\" style=\"flex:1;background:#00ff88;color:#000;border:none;padding:12px;font-weight:700;\">';\n    html += 'Apply This Profile</button>';\n    html += '<button id=\"export-profile-btn\" class=\"small-button\" style=\"background:#1a1a1a;border:1px solid #2a2a2a;color:#aaa;padding:12px;\">';\n    html += 'Export JSON</button>';\n    html += '<button id=\"save-profile-btn\" class=\"small-button\" style=\"background:#1a1a1a;border:1px solid #2a2a2a;color:#aaa;padding:12px;\">';\n    html += 'Save As...</button></div>';\n\n    html += '<div style=\"margin-top:20px;padding:14px;background:#0a0a0a;border:1px solid #2a2a2a;border-radius:6px;\">';\n    html += '<div style=\"font-size:11px;color:#666;line-height:1.6;\">';"}
{"id":318,"text":"html += '<strong style=\"color:#888;\">Hardware Detected:</strong> ';\n    html += (scan && scan.info && scan.info.os) || 'Unknown';\n    html += ' • ';\n    html += (scan && scan.info && scan.info.arch) || 'Unknown';\n    html += ' • ';\n    html += (scan && scan.info && scan.info.cpu_cores) || '?';\n    html += ' cores • ';\n    html += (scan && scan.info && scan.info.mem_gb) ? scan.info.mem_gb + 'GB RAM' : 'RAM unknown';\n    if (scan && scan.runtimes && scan.runtimes.ollama) html += ' • Ollama available';\n    if (scan && scan.runtimes && scan.runtimes.cuda) html += ' • CUDA available';\n    html += '</div></div>';\n\n    return html;\n  }\n\n  window.ProfileRenderer = { renderProfileResults: renderProfileResults };\n  window.ProfileRenderer.bindTooltips = function bindTooltips(root){\n    if (!root) return;\n    const icons = root.querySelectorAll('.help-icon');\n    icons.forEach(icon => {\n      const wrap = icon.parentElement;\n      const bubble = wrap && wrap.querySelector('.tooltip-bubble');\n      if (!wrap || !bubble) return;\n      function show(){ bubble.classList.add('tooltip-visible'); }\n      function hide(){ bubble.classList.remove('tooltip-visible'); }"}
{"id":319,"text":"icon.addEventListener('mouseenter', show);\n      icon.addEventListener('mouseleave', hide);\n      icon.addEventListener('focus', show);\n      icon.addEventListener('blur', hide);\n      icon.addEventListener('click', (e)=>{ e.stopPropagation(); bubble.classList.toggle('tooltip-visible'); });\n      document.addEventListener('click', (evt)=>{ if (!wrap.contains(evt.target)) bubble.classList.remove('tooltip-visible'); });\n    });\n  }\n})();"}
{"id":320,"text":"// AGRO Storage Calculator v1.2 - JavaScript Logic\n// This file contains all the calculation logic for the storage calculator\n\n// Improved formatBytes function with consistent formattingformatBytes(bytes) {\n    if (!isFinite(bytes) || bytes === 0) return '0 B';\n    const abs = Math.abs(bytes);\n    const KB = 1024;\n    const MB = KB * 1024;\n    const GB = MB * 1024;\n    const TB = GB * 1024;\n    const nf = new Intl.NumberFormat('en-US', { maximumFractionDigits: 3 });\n\n    if (abs < KB) return `${bytes.toFixed(0)} B`;\n    if (abs < MB) return `${nf.format(bytes / KB)} KiB`;\n    if (abs < GB) return `${nf.format(bytes / MB)} MiB`;\n    if (abs < TB) return `${nf.format(bytes / GB)} GiB`;\n    return `${nf.format(bytes / TB)} TiB`;\n}\nformatNumber(num) {\n    return new Intl.NumberFormat('en-US').format(num);\n}\n\n// Calculator 1: Full Storage Requirements"}
{"id":321,"text":"calculateStorage1() {\n    const R = parseFloat(document.getElementById('calc1-repoSize').value) *\n             parseFloat(document.getElementById('calc1-repoUnit').value);\n    const C = parseFloat(document.getElementById('calc1-chunkSize').value) *\n             parseFloat(document.getElementById('calc1-chunkUnit').value);\n\n    // Guard against invalid chunk size\n    if (!C || C <= 0) {\n        console.warn(\"Chunk size must be > 0\");\n        return;\n    }\n\n    const D = parseFloat(document.getElementById('calc1-embDim').value);\n    const B = parseFloat(document.getElementById('calc1-precision').value);\n    const Q = parseFloat(document.getElementById('calc1-qdrant').value);\n    const hydrationPct = parseFloat(document.getElementById('calc1-hydration').value) / 100;\n    const redisBytes = parseFloat(document.getElementById('calc1-redis').value) * 1048576;\n    const replFactor = parseFloat(document.getElementById('calc1-replication').value);\n\n    // Calculate\n    const N = Math.ceil(R / C);\n    const E = N * D * B;\n    const Q_bytes = E * Q;\n    const BM25 = 0.20 * R;\n    const CARDS = 0.10 * R;\n    const HYDR = hydrationPct * R;\n    const RER = 0.5 * E;\n\n    // Update display\n    document.getElementById('calc1-chunks').textContent = formatNumber(N);\n    document.getElementById('calc1-embeddings').textContent = formatBytes(E);\n    document.getElementById('calc1-qdrantSize').textContent = formatBytes(Q_bytes);\n    document.getElementById('calc1-bm25').textContent = formatBytes(BM25);\n    document.getElementById('calc1-cards').textContent = formatBytes(CARDS);\n    document.getElementById('calc1-hydr').textContent = formatBytes(HYDR);\n    document.getElementById('calc1-reranker').textContent = formatBytes(RER);\n    document.getElementById('calc1-redisSize').textContent = formatBytes(redisBytes);\n\n    // Totals\n    const singleTotal = E + Q_bytes + BM25 + CARDS + HYDR + RER + redisBytes;\n    const criticalComponents = E + Q_bytes + HYDR + CARDS + RER;\n    const replicatedTotal = singleTotal + (replFactor - 1) * criticalComponents;\n\n    document.getElementById('calc1-single').textContent = formatBytes(singleTotal);\n    document.getElementById('calc1-replicated').textContent = formatBytes(replicatedTotal);\n    document.getElementById('calc1-repFactor').textContent = replFactor;\n}\n\n// Calculator 2: Optimization & Fitting (corrected version)"}
{"id":322,"text":"calculateStorage2() {\n    // Read base values (uses same unit semantics as calc1)\n    const R = parseFloat(document.getElementById('calc2-repoSize').value) *\n              parseFloat(document.getElementById('calc2-repoUnit').value);\n\n    const targetBytes = parseFloat(document.getElementById('calc2-targetSize').value) *\n                        parseFloat(document.getElementById('calc2-targetUnit').value);\n\n    const C = parseFloat(document.getElementById('calc2-chunkSize').value) *\n              parseFloat(document.getElementById('calc2-chunkUnit').value);\n\n    // Guard against invalid chunk size\n    if (!C || C <= 0) {\n        console.warn(\"Chunk size must be > 0\");\n        return;\n    }\n\n    const D = parseFloat(document.getElementById('calc2-embDim').value);\n    const bm25Pct = parseFloat(document.getElementById('calc2-bm25pct').value) / 100;\n    const cardsPct = parseFloat(document.getElementById('calc2-cardspct').value) / 100;\n\n    // Try to reuse calc1 inputs if present (keeps both calculators consistent)\n    const qdrantMultiplier = (document.getElementById('calc1-qdrant') ? parseFloat(document.getElementById('calc1-qdrant').value) : 1.5);\n    const hydrationPct = (document.getElementById('calc1-hydration') ? (parseFloat(document.getElementById('calc1-hydration').value) / 100) : 1.0);\n    const redisBytesInput = (document.getElementById('calc1-redis') ? parseFloat(document.getElementById('calc1-redis').value) * 1048576 : 390 * 1048576);\n    const replicationFactor = (document.getElementById('calc1-replication') ? parseFloat(document.getElementById('calc1-replication').value) : 3);\n\n    // Derived values\n    const N = Math.ceil(R / C);\n    const E_float32 = N * D * 4;\n    const E_float16 = E_float32 / 2;\n    const E_int8 = E_float32 / 4;\n    const E_pq8 = E_float32 / 8;\n\n    const BM25 = bm25Pct * R;\n    const CARDS = cardsPct * R;\n\n    // Update display\n    document.getElementById('calc2-chunks').textContent = formatNumber(N);\n    document.getElementById('calc2-baseStorage').textContent = formatBytes(R);\n    document.getElementById('calc2-float32').textContent = formatBytes(E_float32);\n    document.getElementById('calc2-float16').textContent = formatBytes(E_float16);\n    document.getElementById('calc2-int8').textContent = formatBytes(E_int8);\n    document.getElementById('calc2-pq8').textContent = formatBytes(E_pq8);\n\n    // Aggressive plan: PQ 8x, no local hydration (hydrate = 0)\n    const aggressiveEmbedding = E_pq8;\n    const aggressiveQ = E_pq8 * qdrantMultiplier;\n    const aggressiveRer = 0.5 * E_pq8; // reranker scaled with PQ embedding bytes\n    const aggressiveTotal = aggressiveEmbedding + aggressiveQ + BM25 + CARDS + redisBytesInput + aggressiveRer;\n    const aggressiveCritical = aggressiveEmbedding + aggressiveQ + CARDS + aggressiveRer; // no hydration\n    const aggressiveReplicated = aggressiveTotal + (replicationFactor - 1) * aggressiveCritical;\n    const aggressiveFits = aggressiveTotal <= targetBytes;\n\n    document.getElementById('calc2-aggressive-total').textContent = formatBytes(aggressiveTotal);\n    document.getElementById('calc2-aggressive-replicated').textContent = formatBytes(aggressiveReplicated);\n    document.getElementById('calc2-aggressive-plan').className = 'plan-card ' + (aggressiveFits ? 'fits' : 'exceeds');\n\n    // Conservative plan: float16 precision, full hydration\n    const conservativeEmbedding = E_float16;\n    const conservativeQ = conservativeEmbedding * qdrantMultiplier;\n    const conservativeRer = 0.5 * conservativeEmbedding;\n    const conservativeHydration = hydrationPct * R;\n    const conservativeTotal = conservativeEmbedding + conservativeQ + conservativeHydration + BM25 + CARDS + conservativeRer + redisBytesInput;\n    const conservativeCritical = conservativeEmbedding + conservativeQ + conservativeHydration + CARDS + conservativeRer;\n    const conservativeReplicated = conservativeTotal + (replicationFactor - 1) * conservativeCritical;\n    const conservativeFits = conservativeTotal <= targetBytes;\n\n    document.getElementById('calc2-conservative-total').textContent = formatBytes(conservativeTotal);\n    document.getElementById('calc2-conservative-replicated').textContent = formatBytes(conservativeReplicated);\n    document.getElementById('calc2-conservative-plan').className = 'plan-card ' + (conservativeFits ? 'fits' : 'exceeds');\n\n    // Update replication factor display\n    document.getElementById('calc2-aggRepFactor').textContent = replicationFactor;\n    document.getElementById('calc2-consRepFactor').textContent = replicationFactor;\n\n    // Update hydration info display\n    const hydrationInfoEl = document.getElementById('hydrationInfo');\n    if (hydrationInfoEl) {\n        hydrationInfoEl.textContent = Math.round(hydrationPct * 100) + '%';\n    }\n\n    // Status message\n    const statusEl = document.getElementById('calc2-status');\n    if (aggressiveFits && conservativeFits) {\n        statusEl.className = 'success';\n        statusEl.textContent = '✓ Both configurations fit within your ' + formatBytes(targetBytes) + ' limit';\n    } else if (aggressiveFits) {\n        statusEl.className = 'warning';\n        statusEl.textContent = '⚠ Only Minimal config fits. Low Latency config needs ' + formatBytes(conservativeTotal - targetBytes) + ' more storage.';\n    } else {\n        statusEl.className = 'warning';\n        statusEl.textContent = '⚠ Both exceed limit. Minimal needs ' + formatBytes(aggressiveTotal - targetBytes) + ' more. Consider larger chunks or stronger compression.';\n    }\n}\n\n// Initialize event listeners when DOM is loaded"}
{"id":323,"text":"initStorageCalculator() {\n    // Event listeners for Calculator 1\n    ['calc1-repoSize', 'calc1-repoUnit', 'calc1-chunkSize', 'calc1-chunkUnit',\n     'calc1-embDim', 'calc1-precision', 'calc1-qdrant', 'calc1-hydration',\n     'calc1-redis', 'calc1-replication'].forEach(id => {\n        const element = document.getElementById(id);\n        if (element) {\n            element.addEventListener('input', () => {\n                calculateStorage1();\n                calculateStorage2(); // Recalc calc2 when calc1 shared params change\n            });\n        }\n    });\n\n    // Event listeners for Calculator 2\n    ['calc2-repoSize', 'calc2-repoUnit', 'calc2-targetSize', 'calc2-targetUnit',\n     'calc2-chunkSize', 'calc2-chunkUnit', 'calc2-embDim', 'calc2-bm25pct',\n     'calc2-cardspct'].forEach(id => {\n        const element = document.getElementById(id);\n        if (element) {\n            element.addEventListener('input', calculateStorage2);\n        }\n    });\n\n    // Initial calculations\n    calculateStorage1();\n    calculateStorage2();\n}\n\n// Export functions for external use\nif (typeof module !== 'undefined' && module.exports) {\n    module.exports = {\n        formatBytes,\n        formatNumber,\n        calculateStorage1,\n        calculateStorage2,\n        initStorageCalculator\n    };\n}"}
{"id":324,"text":"// Storage Calculator HTML Template\n// This function generates the HTML structure for the storage calculator"}
{"id":325,"text":"getStorageCalculatorHTML() {\n    return `\n        <div class=\"storage-calc-wrapper\">\n            <div class=\"storage-calc-header\">\n                <h1><span class=\"brand\">AGRO</span> Storage Calculator Suite</h1>\n                <p class=\"subtitle\">Another Good RAG Option • Enterprise Memory Planning</p>\n                <div class=\"info-box\">\n                    <p>\n                        <strong>Left:</strong> Calculate exact storage needs for your configuration.<br>\n                        <strong>Right:</strong> See if your data fits within a target limit using different strategies.\n                    </p>\n                </div>\n            </div>\n\n            <div class=\"calculators-grid\">\n                <!-- Calculator 1: Comprehensive Storage Requirements -->\n                <div class=\"calculator\">\n                    <div class=\"calculator-title\">\n                        Storage Requirements\n                        <span class=\"calculator-badge\">Full Stack</span>\n                    </div>\n\n                    <p style=\"font-size: 12px; color: #888; margin-bottom: 20px; line-height: 1.5;\">\n                        Calculate total storage for your chosen configuration with all components.\n                    </p>\n\n                    <div class=\"input-section\">\n                        <div class=\"input-row\">\n                            <div class=\"input-group\">\n                                <label>\n                                    <div class=\"label-with-tooltip\">\n                                        Repository Size\n                                        <span class=\"tooltip\" title=\"Total size of your data/documents to index\">?</span>\n                                    </div>\n                                </label>\n                                <div class=\"unit-input\">\n                                    <input type=\"number\" id=\"calc1-repoSize\" value=\"5\" step=\"0.1\" min=\"0.1\" aria-label=\"Repository size value\">\n                                    <select id=\"calc1-repoUnit\" aria-label=\"Repository size unit\">\n                                        <option value=\"1073741824\" selected>GiB</option>\n                                        <option value=\"1099511627776\">TiB</option>\n                                        <option value=\"1048576\">MiB</option>\n                                    </select>\n                                </div>\n                            </div>\n                            <div class=\"input-group\">\n                                <label>\n                                    <div class=\"label-with-tooltip\">\n                                        Chunk Size\n                                        <span class=\"tooltip\" title=\"Size of text chunks for embedding. Typically 1-8 KiB\">?</span>\n                                    </div>\n                                </label>\n                                <div class=\"unit-input\">\n                                    <input type=\"number\" id=\"calc1-chunkSize\" value=\"4\" step=\"1\" min=\"0.001\" aria-label=\"Chunk size value\">\n                                    <select id=\"calc1-chunkUnit\" aria-label=\"Chunk size unit\">\n                                        <option value=\"1024\" selected>KiB</option>\n                                        <option value=\"1048576\">MiB</option>\n                                    </select>\n                                </div>\n                            </div>\n                        </div>\n\n                        <div class=\"input-row\">\n                            <div class=\"input-group\">\n                                <label>\n                                    <div class=\"label-with-tooltip\">\n                                        Embedding Dimension\n                                        <span class=\"tooltip\" title=\"Vector size: 512 (small), 768 (BERT), 1536 (OpenAI)\">?</span>\n                                    </div>\n                                </label>\n                                <input type=\"number\" id=\"calc1-embDim\" value=\"512\" step=\"1\" min=\"1\" aria-label=\"Embedding dimension\">\n                            </div>\n                            <div class=\"input-group\">\n                                <label>\n                                    <div class=\"label-with-tooltip\">\n                                        Precision\n                                        <span class=\"tooltip\" title=\"float32: full precision, float16: half size, int8: quarter size\">?</span>\n                                    </div>\n                                </label>\n                                <select id=\"calc1-precision\" aria-label=\"Data precision\">\n                                    <option value=\"4\" selected>float32</option>\n                                    <option value=\"2\">float16</option>\n                                    <option value=\"1\">int8</option>\n                                </select>\n                            </div>\n                        </div>\n\n                        <div class=\"input-row\">\n                            <div class=\"input-group\">\n                                <label>\n                                    <div class=\"label-with-tooltip\">\n                                        Qdrant Overhead\n                                        <span class=\"tooltip\" title=\"Vector DB index overhead. Typically 1.5x embedding size\">?</span>\n                                    </div>\n                                </label>\n                                <input type=\"number\" id=\"calc1-qdrant\" value=\"1.5\" step=\"0.1\" min=\"1\" aria-label=\"Qdrant overhead multiplier\">\n                            </div>\n                            <div class=\"input-group\">\n                                <label>\n                                    <div class=\"label-with-tooltip\">\n                                        Hydration %\n                                        <span class=\"tooltip\" title=\"% of raw data kept in RAM for instant retrieval. 0% = fetch from disk, 100% = everything in memory\">?</span>\n                                    </div>\n                                </label>\n                                <input type=\"number\" id=\"calc1-hydration\" value=\"100\" step=\"10\" min=\"0\" max=\"100\" aria-label=\"Hydration percentage\">\n                            </div>\n                        </div>\n\n                        <div class=\"input-row\">\n                            <div class=\"input-group\">\n                                <label>\n                                    <div class=\"label-with-tooltip\">\n                                        Redis Cache (MiB)\n                                        <span class=\"tooltip\" title=\"Session/chat memory storage\">?</span>\n                                    </div>\n                                </label>\n                                <input type=\"number\" id=\"calc1-redis\" value=\"400\" step=\"50\" min=\"0\" aria-label=\"Redis cache size\">\n                            </div>\n                            <div class=\"input-group\">\n                                <label>\n                                    <div class=\"label-with-tooltip\">\n                                        Replication Factor\n                                        <span class=\"tooltip\" title=\"Number of copies for HA/scaling\">?</span>\n                                    </div>\n                                </label>\n                                <input type=\"number\" id=\"calc1-replication\" value=\"3\" step=\"1\" min=\"1\" aria-label=\"Replication factor\">\n                            </div>\n                        </div>\n                    </div>\n\n                    <div class=\"results\">\n                        <div class=\"result-grid\">\n                            <div class=\"result-item\">\n                                <span class=\"result-label\">Chunks</span>\n                                <span class=\"result-value\" id=\"calc1-chunks\">-</span>\n                            </div>\n                            <div class=\"result-item\">\n                                <span class=\"result-label\">Raw Embeddings</span>\n                                <span class=\"result-value\" id=\"calc1-embeddings\">-</span>\n                            </div>\n                            <div class=\"result-item\">\n                                <span class=\"result-label\">Qdrant</span>\n                                <span class=\"result-value\" id=\"calc1-qdrantSize\">-</span>\n                            </div>\n                            <div class=\"result-item\">\n                                <span class=\"result-label\">BM25 Index</span>\n                                <span class=\"result-value\" id=\"calc1-bm25\">-</span>\n                            </div>\n                            <div class=\"result-item\">\n                                <span class=\"result-label\">Cards/Summary</span>\n                                <span class=\"result-value\" id=\"calc1-cards\">-</span>\n                            </div>\n                            <div class=\"result-item\">\n                                <span class=\"result-label\">Hydration</span>\n                                <span class=\"result-value\" id=\"calc1-hydr\">-</span>\n                            </div>\n                            <div class=\"result-item\">\n                                <span class=\"result-label\">Reranker</span>\n                                <span class=\"result-value\" id=\"calc1-reranker\">-</span>\n                            </div>\n                            <div class=\"result-item\">\n                                <span class=\"result-label\">Redis</span>\n                                <span class=\"result-value\" id=\"calc1-redisSize\">-</span>\n                            </div>\n                        </div>\n\n                        <div class=\"total-row\">\n                            <div class=\"result-item\">\n                                <span class=\"result-label\">Single Instance</span>\n                                <span class=\"result-value\" id=\"calc1-single\">-</span>\n                            </div>\n                            <div class=\"result-item\">\n                                <span class=\"result-label\">Replicated (×<span id=\"calc1-repFactor\">3</span>)</span>\n                                <span class=\"result-value\" id=\"calc1-replicated\">-</span>\n                            </div>\n                        </div>\n                    </div>\n                </div>\n\n                <!-- Calculator 2: Optimization & Fitting -->\n                <div class=\"calculator\">\n                    <div class=\"calculator-title\">\n                        Optimization Planner\n                        <span class=\"calculator-badge\">Fit Analysis</span>\n                    </div>\n\n                    <p style=\"font-size: 12px; color: #888; margin-bottom: 20px; line-height: 1.5;\">\n                        Compare two strategies: <strong>Minimal</strong> (smallest footprint, fetches data on-demand) vs <strong>Low Latency</strong> (everything in RAM for instant access).\n                    </p>\n\n                    <div class=\"input-section\">\n                        <div class=\"input-row\">\n                            <div class=\"input-group\">\n                                <label>\n                                    <div class=\"label-with-tooltip\">\n                                        Repository Size\n                                        <span class=\"tooltip\" title=\"Same as left calculator - your total data\">?</span>\n                                    </div>\n                                </label>\n                                <div class=\"unit-input\">\n                                    <input type=\"number\" id=\"calc2-repoSize\" value=\"5\" step=\"0.1\" min=\"0.1\" aria-label=\"Repository size value\">\n                                    <select id=\"calc2-repoUnit\" aria-label=\"Repository size unit\">\n                                        <option value=\"1073741824\" selected>GiB</option>\n                                        <option value=\"1099511627776\">TiB</option>\n                                        <option value=\"1048576\">MiB</option>\n                                    </select>\n                                </div>\n                            </div>\n                            <div class=\"input-group\">\n                                <label>\n                                    <div class=\"label-with-tooltip\">\n                                        Target Limit\n                                        <span class=\"tooltip\" title=\"Max storage you want to use\">?</span>\n                                    </div>\n                                </label>\n                                <div class=\"unit-input\">\n                                    <input type=\"number\" id=\"calc2-targetSize\" value=\"5\" step=\"0.5\" min=\"0.1\" aria-label=\"Target storage limit\">\n                                    <select id=\"calc2-targetUnit\" aria-label=\"Target limit unit\">\n                                        <option value=\"1073741824\" selected>GiB</option>\n                                        <option value=\"1099511627776\">TiB</option>\n                                    </select>\n                                </div>\n                            </div>\n                        </div>\n\n                        <div class=\"input-row\">\n                            <div class=\"input-group\">\n                                <label>\n                                    <div class=\"label-with-tooltip\">\n                                        Chunk Size\n                                        <span class=\"tooltip\" title=\"Smaller chunks = more vectors = more storage\">?</span>\n                                    </div>\n                                </label>\n                                <div class=\"unit-input\">\n                                    <input type=\"number\" id=\"calc2-chunkSize\" value=\"4\" step=\"1\" min=\"0.001\" aria-label=\"Chunk size value\">\n                                    <select id=\"calc2-chunkUnit\" aria-label=\"Chunk size unit\">\n                                        <option value=\"1024\" selected>KiB</option>\n                                        <option value=\"1048576\">MiB</option>\n                                    </select>\n                                </div>\n                            </div>\n                            <div class=\"input-group\">\n                                <label>\n                                    <div class=\"label-with-tooltip\">\n                                        Embedding Dims\n                                        <span class=\"tooltip\" title=\"Must match your model choice\">?</span>\n                                    </div>\n                                </label>\n                                <input type=\"number\" id=\"calc2-embDim\" value=\"512\" step=\"1\" min=\"1\" aria-label=\"Embedding dimension\">\n                            </div>\n                        </div>\n\n                        <div class=\"input-row\">\n                            <div class=\"input-group\">\n                                <label>\n                                    <div class=\"label-with-tooltip\">\n                                        BM25 Overhead %\n                                        <span class=\"tooltip\" title=\"Text search index, typically 20% of data\">?</span>\n                                    </div>\n                                </label>\n                                <input type=\"number\" id=\"calc2-bm25pct\" value=\"20\" step=\"5\" min=\"0\" max=\"100\" aria-label=\"BM25 overhead percentage\">\n                            </div>\n                            <div class=\"input-group\">\n                                <label>\n                                    <div class=\"label-with-tooltip\">\n                                        Cards/Summary %\n                                        <span class=\"tooltip\" title=\"Metadata/summaries, typically 10% of data\">?</span>\n                                    </div>\n                                </label>\n                                <input type=\"number\" id=\"calc2-cardspct\" value=\"10\" step=\"5\" min=\"0\" max=\"100\" aria-label=\"Cards/summary percentage\">\n                            </div>\n                        </div>\n                    </div>\n\n                    <div class=\"results\">\n                        <div class=\"result-grid\">\n                            <div class=\"result-item\">\n                                <span class=\"result-label\">Chunks</span>\n                                <span class=\"result-value\" id=\"calc2-chunks\">-</span>\n                            </div>\n                            <div class=\"result-item\">\n                                <span class=\"result-label\">Repository</span>\n                                <span class=\"result-value\" id=\"calc2-baseStorage\">-</span>\n                            </div>\n                        </div>\n\n                        <div class=\"plan-title\">Embedding Size by Precision (raw vectors only)</div>\n                        <div class=\"result-grid\">\n                            <div class=\"result-item\">\n                                <span class=\"result-label\">float32 (baseline)</span>\n                                <span class=\"result-value\" id=\"calc2-float32\">-</span>\n                            </div>\n                            <div class=\"result-item\">\n                                <span class=\"result-label\">float16 (half size)</span>\n                                <span class=\"result-value\" id=\"calc2-float16\">-</span>\n                            </div>\n                            <div class=\"result-item\">\n                                <span class=\"result-label\">int8 (quarter size)</span>\n                                <span class=\"result-value\" id=\"calc2-int8\">-</span>\n                            </div>\n                            <div class=\"result-item\">\n                                <span class=\"result-label\">\n                                    Product Quantization\n                                    <span class=\"tooltip\" title=\"Aggressive compression: 8× smaller but ~5% accuracy loss\" style=\"margin-left: 4px;\">?</span>\n                                </span>\n                                <span class=\"result-value\" id=\"calc2-pq8\">-</span>\n                            </div>\n                        </div>\n\n                        <div class=\"plans-section\">\n                            <div class=\"plan-title\">Configuration Plans</div>\n                            <div class=\"plan-grid\">\n                                <div class=\"plan-card\" id=\"calc2-aggressive-plan\">\n                                    <div class=\"plan-name\">Minimal (No Hydration)</div>\n                                    <div class=\"plan-details\" id=\"calc2-aggressive-details\" style=\"line-height: 1.8;\">\n                                        <strong>Includes:</strong><br>\n                                        • Product Quantized vectors<br>\n                                        • Qdrant index<br>\n                                        • BM25 search<br>\n                                        • Cards/metadata<br>\n                                        • Reranker cache<br>\n                                        • Redis<br>\n                                        <strong>Excludes:</strong><br>\n                                        • Raw data (fetched on-demand)\n                                    </div>\n                                    <div class=\"plan-total\" id=\"calc2-aggressive-total\">-</div>\n                                </div>\n                                <div class=\"plan-card\" id=\"calc2-conservative-plan\">\n                                    <div class=\"plan-name\">Low Latency (Full Cache)</div>\n                                    <div class=\"plan-details\" id=\"calc2-conservative-details\" style=\"line-height: 1.8;\">\n                                        <strong>Includes:</strong><br>\n                                        • float16 vectors<br>\n                                        • Qdrant index<br>\n                                        • BM25 search<br>\n                                        • Cards/metadata<br>\n                                        • Reranker cache<br>\n                                        • Redis<br>\n                                        • <span style=\"color: #ffaa00;\">Data in RAM (per left hydration %)</span>\n                                    </div>\n                                    <div class=\"plan-total\" id=\"calc2-conservative-total\">-</div>\n                                </div>\n                            </div>\n\n                            <p style=\"font-size: 11px; color: #666; margin: 16px 0 8px; padding: 12px; background: #0a0a0a; border-radius: 4px; line-height: 1.5;\">\n                                💡 <strong>Why the big difference?</strong> Low Latency keeps data in RAM based on hydration % from left panel (currently adding <span id=\"hydrationInfo\">100%</span> of repo size). Minimal only stores compressed vectors and indexes, fetching actual data from disk when needed.\n                            </p>\n\n                            <div class=\"total-row\" style=\"margin-top: 20px;\">\n                                <div class=\"result-item\">\n                                    <span class=\"result-label\">Minimal × <span id=\"calc2-aggRepFactor\">3</span> replicas</span>\n                                    <span class=\"result-value\" id=\"calc2-aggressive-replicated\">-</span>\n                                </div>\n                                <div class=\"result-item\">\n                                    <span class=\"result-label\">Low Latency × <span id=\"calc2-consRepFactor\">3</span> replicas</span>\n                                    <span class=\"result-value\" id=\"calc2-conservative-replicated\">-</span>\n                                </div>\n                            </div>\n\n                            <div id=\"calc2-status\" style=\"margin-top: 12px;\"></div>\n                        </div>\n                    </div>\n                </div>\n            </div>\n\n            <div class=\"storage-calc-footer\">\n                <p>AGRO (Another Good RAG Option) • Enterprise Storage Calculator v1.2</p>\n                <p>Precision calculations for vector search infrastructure</p>\n            </div>\n        </div>\n    `;\n}\n\n// Export for use in main app\nif (typeof module !== 'undefined' && module.exports) {\n    module.exports = { getStorageCalculatorHTML };\n}"}
