{"id":0,"text":"from fastapi import FastAPI, Query\nfrom pydantic import BaseModel\nfrom typing import Optional\nfrom langgraph_app import build_graph\nfrom hybrid_search import search_routed_multi\n\napp = FastAPI(title=\"PROJECT/PROJECT RAG\")\n\n_graph = Noneget_graph():\n    global _graph\n    if _graph is None:\n        _graph = build_graph()\n    return _graph\n\nCFG = {\"configurable\": {\"thread_id\": \"http\"}}\nAnswer(BaseModel):\n    answer: str\n\n@app.get(\"/health\")health():\n    try:\n        g = get_graph()\n        return {\"status\": \"healthy\", \"graph_loaded\": g is not None}\n    except Exception as e:\n        return {\"status\": \"error\", \"detail\": str(e)}\n\n@app.get(\"/answer\", response_model=Answer)"}
{"id":1,"text":"answer(\n    q: str = Query(..., description=\"Question\"),\n    repo: Optional[str] = Query(None, description=\"Repository override: project|project\")\n):\n    \"\"\"Answer a question using strict per-repo routing.\n\n    If `repo` is provided, retrieval and the answer header will use that repo.\n    Otherwise, a lightweight router selects the repo from the query content.\n    \"\"\"\n    g = get_graph()\n    state = {\"question\": q, \"documents\": [], \"generation\":\"\", \"iteration\":0, \"confidence\":0.0, \"repo\": (repo.strip() if repo else None)}\n    res = g.invoke(state, CFG)\n    return {\"answer\": res[\"generation\"]}\n\n@app.get(\"/search\")"}
{"id":2,"text":"search(\n    q: str = Query(..., description=\"Question\"),\n    repo: Optional[str] = Query(None, description=\"Repository override: project|project\"),\n    top_k: int = Query(10, description=\"Number of results to return\")\n):\n    \"\"\"Search for relevant code locations without generation.\n\n    Returns file paths, line ranges, and rerank scores for the most relevant code chunks.\n    \"\"\"\n    docs = search_routed_multi(q, repo_override=repo, m=4, final_k=top_k)\n    results = [\n        {\n            \"file_path\": d.get(\"file_path\", \"\"),\n            \"start_line\": d.get(\"start_line\", 0),\n            \"end_line\": d.get(\"end_line\", 0),\n            \"language\": d.get(\"language\", \"\"),\n            \"rerank_score\": float(d.get(\"rerank_score\", 0.0) or 0.0),\n            \"repo\": d.get(\"repo\", repo),\n        }\n        for d in docs\n    ]\n    return {\"results\": results, \"repo\": repo, \"count\": len(results)}"}
{"id":3,"text":"import math\nimport os\nfrom typing import List, Dict\nfrom rerankers import Reranker\nfrom typing import Optional\n\n_HF_PIPE = None  # optional transformers pipeline for models that require trust_remote_code\n\n_RERANKER = None\n\n# Default to a strong open-source cross-encoder; allow env override\nDEFAULT_MODEL = os.getenv('RERANKER_MODEL', 'BAAI/bge-reranker-v2-m3')\n# Default backend: local (set RERANK_BACKEND=cohere + COHERE_API_KEY to use Cohere API)\nRERANK_BACKEND = (os.getenv('RERANK_BACKEND', 'local') or 'local').lower()\n# Default Cohere model (override via COHERE_RERANK_MODEL). Accepts 'rerank-3.5' or 'rerank-2.5'.\nCOHERE_MODEL = os.getenv('COHERE_RERANK_MODEL', 'rerank-3.5')\n\n_sigmoid(x: float) -> float:\n    try:\n        return 1.0 / (1.0 + math.exp(-float(x)))\n    except Exception:\n        return 0.0\n\n_normalize(score: float, model_name: str) -> float:\n    if any(k in model_name.lower() for k in ['bge-reranker', 'cross-encoder', 'mxbai', 'jina-reranker']):\n        return _sigmoid(score)\n    return float(score)"}
{"id":4,"text":"_maybe_init_hf_pipeline(model_name: str) -> Optional[object]:\n    global _HF_PIPE\n    if _HF_PIPE is not None:\n        return _HF_PIPE\n    try:\n        if 'jinaai/jina-reranker' in model_name.lower():\n            # Use HF pipeline directly to guarantee trust_remote_code is honored\n            os.environ.setdefault('TRANSFORMERS_TRUST_REMOTE_CODE', '1')\n            from transformers import pipeline\n            _HF_PIPE = pipeline(\n                task='text-classification',\n                model=model_name,\n                tokenizer=model_name,\n                trust_remote_code=True,\n                device_map='auto'\n            )\n            return _HF_PIPE\n    except Exception:\n        _HF_PIPE = None\n    return _HF_PIPE\n\nget_reranker() -> Reranker:\n    global _RERANKER\n    if _RERANKER is None:\n        model_name = DEFAULT_MODEL\n        # First try a direct HF pipeline for models with custom code\n        if _maybe_init_hf_pipeline(model_name):\n            return None  # Signal to use HF pipeline path\n        # Otherwise, fall back to rerankers\n        os.environ.setdefault('TRANSFORMERS_TRUST_REMOTE_CODE', '1')\n        _RERANKER = Reranker(model_name, model_type='cross-encoder', trust_remote_code=True)\n    return _RERANKER"}
{"id":5,"text":"rerank_results(query: str, results: List[Dict], top_k: int = 10) -> List[Dict]:\n    if not results:\n        return []\n    model_name = DEFAULT_MODEL\n    # Optional Cohere backend (remote API)\n    if RERANK_BACKEND == 'cohere':\n        try:\n            import cohere  # type: ignore\n            api_key = os.getenv('COHERE_API_KEY')\n            if not api_key:\n                raise RuntimeError('COHERE_API_KEY not set')\n            client = cohere.Client(api_key=api_key)\n            docs = []\n            for r in results:\n                file_ctx = r.get('file_path', '')\n                code_snip = (r.get('code') or r.get('text') or '')[:700]\n                docs.append(f\"{file_ctx}\\n\\n{code_snip}\")\n            rr = client.rerank(model=COHERE_MODEL, query=query, documents=docs, top_n=len(docs))\n            # Normalize scores into 0..1\n            scores = [getattr(x, 'relevance_score', 0.0) for x in rr.results]\n            max_s = max(scores) if scores else 1.0\n            for item in rr.results:\n                idx = int(getattr(item, 'index', 0))\n                score = float(getattr(item, 'relevance_score', 0.0))\n                results[idx]['rerank_score'] = (score / max_s) if max_s else 0.0\n            results.sort(key=lambda x: x.get('rerank_score', 0.0), reverse=True)\n            return results[:top_k]\n        except Exception:\n            # Fall back to local reranker paths below\n            pass\n    # HF pipeline path (e.g., Jina reranker)\n    pipe = _maybe_init_hf_pipeline(model_name)\n    if pipe is not None:\n        pairs = []\n        for r in results:\n            code_snip = (r.get('code') or r.get('text') or '')[:700]\n            pairs.append({'text': query, 'text_pair': code_snip})\n        try:\n            out = pipe(pairs, truncation=True)\n            for i, o in enumerate(out):\n                score = float(o.get('score', 0.0))\n                results[i]['rerank_score'] = score\n            results.sort(key=lambda x: x.get('rerank_score', 0.0), reverse=True)\n            return results[:top_k]\n        except Exception:\n            # Fall back to rerankers path below\n            pass\n    # rerankers path\n    docs = []\n    for r in results:\n        file_ctx = r.get('file_path', '')\n        code_snip = (r.get('code') or r.get('text') or '')[:600]\n        docs.append(f\"{file_ctx}\\n\\n{code_snip}\")\n    rr = get_reranker()\n    if rr is None and _maybe_init_hf_pipeline(model_name) is not None:\n        # HF pipeline already used above; should not reach here\n        return results[:top_k]\n    ranked = rr.rank(query=query, docs=docs, doc_ids=list(range(len(docs))))\n    for res in ranked.results:\n        idx = res.document.doc_id\n        results[idx]['rerank_score'] = _normalize(res.score, model_name)\n    results.sort(key=lambda x: x.get('rerank_score', 0.0), reverse=True)\n    return results[:top_k]"}
{"id":6,"text":"import os\nimport json\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Tuple\n\n\n_CACHE: Dict[str, Any] = {}\n\n_repos_file_path() -> Path:\n    # Allow override via env; default to repos.json in repo root\n    env_path = os.getenv(\"REPOS_FILE\")\n    if env_path:\n        return Path(env_path).expanduser().resolve()\n    return Path(__file__).resolve().parent / \"repos.json\""}
{"id":7,"text":"load_repos() -> Dict[str, Any]:\n    \"\"\"\n    Load repository configuration.\n\n    Format:\n      {\n        \"default_repo\": \"example-1\",\n        \"repos\": [\n          {\"name\": \"example-1\", \"path\": \"/abs/path/example-1\",\n           \"keywords\": [\"auth\", \"backend\"],\n           \"path_boosts\": [\"src/server/\", \"api/\"],\n           \"layer_bonuses\": {\"server\": {\"kernel\": 0.10}, \"ui\": {\"ui\": 0.12}}\n          },\n          {\"name\": \"example-2\", \"path\": [\"/abs/path/example-2\", \"/abs/alt\"]}\n        ]\n      }\n\n    Fallbacks (if repos.json missing):\n      - If REPO and REPO_PATH env set, synthesize a single-repo config.\n      - Else raise a clear error when a function requires config.\n    \"\"\"\n    global _CACHE\n    if \"config\" in _CACHE:\n        return _CACHE[\"config\"]\n\n    p = _repos_file_path()\n    if p.exists():\n        try:\n            data = json.loads(p.read_text())\n            if isinstance(data, dict) and isinstance(data.get(\"repos\"), list):\n                _CACHE[\"config\"] = data\n                return data\n        except Exception:\n            pass\n\n    # Fallback to environment-only single repo\n    env_repo = (os.getenv(\"REPO\") or \"default\").strip()\n    env_path = os.getenv(\"REPO_PATH\") or os.getenv(f\"REPO_{env_repo.upper()}_PATH\")\n    if env_path:\n        cfg = {\n            \"default_repo\": env_repo,\n            \"repos\": [\n                {\"name\": env_repo, \"path\": env_path}\n            ]\n        }\n        _CACHE[\"config\"] = cfg\n        return cfg\n\n    # Last resort minimal placeholder (no repos) to allow help text rendering\n    cfg = {\"default_repo\": None, \"repos\": []}\n    _CACHE[\"config\"] = cfg\n    return cfg"}
{"id":8,"text":"list_repos() -> List[str]:\n    cfg = load_repos()\n    return [str(r.get(\"name\")) for r in cfg.get(\"repos\", []) if r.get(\"name\")]\n\nget_default_repo() -> str:\n    cfg = load_repos()\n    # 1) explicit default\n    if cfg.get(\"default_repo\"):\n        return str(cfg[\"default_repo\"]).strip()\n    # 2) first repo\n    repos = cfg.get(\"repos\", [])\n    if repos:\n        return str(repos[0].get(\"name\"))\n    # 3) env REPO or sentinel\n    return (os.getenv(\"REPO\") or \"default\").strip()\n\n_find_repo(name: str) -> Optional[Dict[str, Any]]:\n    name_low = (name or \"\").strip().lower()\n    if not name_low:\n        return None\n    for r in load_repos().get(\"repos\", []):\n        if (r.get(\"name\") or \"\").strip().lower() == name_low:\n            return r\n    return None\n\nget_repo_paths(name: str) -> List[str]:\n    r = _find_repo(name)\n    if not r:\n        raise ValueError(f\"Unknown repo: {name}. Known: {', '.join(list_repos()) or '[]'}\")\n    p = r.get(\"path\")\n    if isinstance(p, list):\n        return [str(Path(x).expanduser()) for x in p]\n    if isinstance(p, str):\n        return [str(Path(p).expanduser())]\n    raise ValueError(f\"Repo `{name}` missing 'path' in repos.json\")"}
{"id":9,"text":"out_dir(name: str) -> str:\n    base = Path(__file__).resolve().parent / \"out\" / name\n    return str(base)\n\nget_repo_keywords(name: str) -> List[str]:\n    r = _find_repo(name)\n    if not r:\n        return []\n    kws = r.get(\"keywords\") or []\n    return [str(k).lower() for k in kws if isinstance(k, str)]\n\npath_boosts(name: str) -> List[str]:\n    r = _find_repo(name)\n    if not r:\n        return []\n    lst = r.get(\"path_boosts\") or []\n    return [str(x) for x in lst if isinstance(x, str)]\n\nlayer_bonuses(name: str) -> Dict[str, Dict[str, float]]:\n    r = _find_repo(name)\n    if not r:\n        return {}\n    lb = r.get(\"layer_bonuses\") or {}\n    # Normalize numeric values\n    out: Dict[str, Dict[str, float]] = {}\n    for intent, d in (lb.items() if isinstance(lb, dict) else []):\n        if not isinstance(d, dict):\n            continue\n        out[intent] = {k: float(v) for k, v in d.items() if isinstance(v, (int, float))}\n    return out"}
{"id":10,"text":"choose_repo_from_query(query: str, default: Optional[str] = None) -> str:\n    q = (query or \"\").lower().strip()\n    # Allow explicit prefix: \"<name>: question\"\n    if \":\" in q:\n        cand, _ = q.split(\":\", 1)\n        cand = cand.strip()\n        if cand in [r.lower() for r in list_repos()]:\n            return cand\n    # Keyword voting across repos\n    best = None\n    best_hits = 0\n    for name in list_repos():\n        hits = 0\n        for kw in get_repo_keywords(name):\n            if kw and kw in q:\n                hits += 1\n        if hits > best_hits:\n            best = name\n            best_hits = hits\n    if best:\n        return best\n    return (default or get_default_repo())"}
{"id":11,"text":"import os\nfrom dotenv import load_dotenv\n\nload_dotenv('env/project.env')\nfrom serve_rag import app\n\nif __name__ == '__main__':\n    import uvicorn\n    os.environ['COLLECTION_NAME'] = os.environ.get('COLLECTION_NAME', f\"{os.environ['REPO']}_{os.environ.get('COLLECTION_SUFFIX','default')}\")\n    port = int(os.environ.get('PORT', '8014'))\n    uvicorn.run(app, host='127.0.0.1', port=port)"}
{"id":12,"text":"import os\nimport json\nimport collections\nfrom typing import List, Dict\nfrom pathlib import Path\nfrom config_loader import choose_repo_from_query, get_default_repo\nfrom dotenv import load_dotenv, find_dotenv\nfrom qdrant_client import QdrantClient, models\nimport bm25s\nfrom bm25s.tokenization import Tokenizer\nfrom Stemmer import Stemmer\nfrom rerank import rerank_results as ce_rerank\nfrom env_model import generate_text\n\n# Query intent â†’ layer preferences"}
{"id":13,"text":"_classify_query(q:str)->str:\n    ql=(q or '').lower()\n    if any(k in ql for k in ['ui','react','component','tsx','page','frontend','render','css']):\n        return 'ui'\n    if any(k in ql for k in ['notification','pushover','apprise','hubspot','provider','integration','adapter','webhook']):\n        return 'integration'\n    if any(k in ql for k in ['diagnostic','health','event log','phi','mask','hipaa','middleware','auth','token','oauth','hmac']):\n        return 'server'\n    if any(k in ql for k in ['sdk','client library','python sdk','node sdk']):\n        return 'sdk'\n    if any(k in ql for k in ['infra','asterisk','sip','t.38','ami','freeswitch','egress','cloudflared']):\n        return 'infra'\n    return 'server'"}
{"id":14,"text":"_project_layer_bonus(layer:str,intent:str)->float:\n    layer_lower=(layer or '').lower()\n    intent_lower=(intent or 'server').lower()\n    table={'server':{'kernel':0.10,'plugin':0.04,'ui':0.00,'docs':0.00,'tests':0.00,'infra':0.02},\n           'integration':{'integration':0.12,'kernel':0.04,'ui':0.00,'docs':0.00,'tests':0.00,'infra':0.00},\n           'ui':{'ui':0.12,'docs':0.06,'kernel':0.02,'plugin':0.02,'tests':0.00,'infra':0.00},\n           'sdk':{'kernel':0.04,'docs':0.02},\n           'infra':{'infra':0.12,'kernel':0.04}}\n    return table.get(intent_lower,{}).get(layer_lower,0.0)"}
{"id":15,"text":"_project_layer_bonus(layer:str,intent:str)->float:\n    layer_lower=(layer or '').lower()\n    intent_lower=(intent or 'server').lower()\n    table={'server':{'server':0.10,'integration':0.06,'fax':0.30,'admin console':0.10,'sdk':0.00,'infra':0.00,'docs':0.02},\n           'integration':{'provider':0.12,'traits':0.10,'server':0.06,'ui':0.00,'sdk':0.00,'infra':0.02,'docs':0.00},\n           'ui':{'ui':0.12,'docs':0.06,'server':0.02,'hipaa':0.20},\n           'sdk':{'sdk':0.12,'server':0.04,'docs':0.02},\n           'infra':{'infra':0.12,'server':0.04,'provider':0.04}}\n    return table.get(intent_lower,{}).get(layer_lower,0.0)\n_provider_plugin_hint(fp:str, code:str)->float:\n    fp=(fp or '').lower()\n    code=(code or '').lower()\n    keys=['provider','providers','integration','adapter','webhook','pushover','apprise','hubspot']\n    return 0.06 if any(k in fp or k in code for k in keys) else 0.0"}
{"id":16,"text":"_origin_bonus(origin:str, mode:str)->float:\n    origin = (origin or '').lower()\n    mode=(mode or 'prefer_first_party').lower()\n    if mode == 'prefer_first_party':\n        return 0.06 if origin=='first_party' else (-0.08 if origin=='vendor' else 0.0)\n    if mode == 'prefer_vendor':\n        return 0.06 if origin=='vendor' else 0.0\n    return 0.0\n_feature_bonus(query:str, fp:str, code:str)->float:\n    ql = (query or '').lower()\n    fp = (fp or '').lower()\n    code=(code or '').lower()\n    bumps = 0.0\n    if any(k in ql for k in ['diagnostic','health','event log','phi','hipaa']):\n        if ('diagnostic' in fp) or ('diagnostic' in code) or ('event' in fp and 'log' in fp):\n            bumps += 0.06\n    return bumps\n_card_bonus(chunk_id: str, card_chunk_ids: set) -> float:\n    \"\"\"Boost chunks that matched via card-based retrieval.\"\"\"\n    return 0.08 if str(chunk_id) in card_chunk_ids else 0.0\n\n# Path-aware bonus to tilt results toward likely server/auth code"}
{"id":17,"text":"_path_bonus(fp: str) -> float:\n    fp = (fp or '').lower()\n    bonus = 0.0\n    for sfx, b in [\n        ('/identity/', 0.12),\n        ('/auth/', 0.12),\n        ('/server', 0.10),\n        ('/backend', 0.10),\n        ('/api/', 0.08),\n    ]:\n        if sfx in fp:\n            bonus += b\n    return bonus\n\n# Additional PROJECT-only path boosts (env-tunable)"}
{"id":18,"text":"_project_path_boost(fp: str, repo_tag: str) -> float:\n    import os as _os\n    if (repo_tag or '').lower() != 'project':\n        return 0.0\n    cfg = _os.getenv('project_PATH_BOOSTS', 'app/,lib/,config/,scripts/,server/,api/,api/app,app/services,app/routers,api/admin_ui,app/plugins')\n    tokens = [t.strip().lower() for t in cfg.split(',') if t.strip()]\n    s = (fp or '').lower()\n    bonus = 0.0\n    for tok in tokens:\n        if tok and tok in s:\n            bonus += 0.06\n    return min(bonus, 0.18)\n\n# Load environment from repo root .env without hard-coded paths\ntry:\n    load_dotenv(override=False)\n    repo_root = Path(__file__).resolve().parent\n    env_path = repo_root / \".env\"\n    if env_path.exists():\n        load_dotenv(dotenv_path=env_path, override=False)\n    else:\n        alt = find_dotenv(usecwd=True)\n        if alt:\n            load_dotenv(dotenv_path=alt, override=False)\nexcept Exception:\n    pass\nQDRANT_URL = os.getenv('QDRANT_URL','http://127.0.0.1:6333')\nREPO = os.getenv('REPO','project')\nVENDOR_MODE = os.getenv('VENDOR_MODE','prefer_first_party')\n# Allow explicit collection override (for versioned collections per embedding config)\nCOLLECTION = os.getenv('COLLECTION_NAME', f'code_chunks_{REPO}')\n\n# --- Embeddings provider (openai | voyage | local) ---"}
{"id":19,"text":"_lazy_import_openai():\n    from openai import OpenAI\n    return OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n_lazy_import_voyage():\n    import voyageai\n    return voyageai.Client(api_key=os.getenv(\"VOYAGE_API_KEY\"))\n\n_local_embed_model = None"}
{"id":20,"text":"_get_embedding(text: str, kind: str = \"query\") -> list[float]:\n    et = (os.getenv(\"EMBEDDING_TYPE\", \"openai\") or \"openai\").lower()\n    if et == \"voyage\":\n        vo = _lazy_import_voyage()\n        out = vo.embed([text], model=\"voyage-code-3\", input_type=kind, output_dimension=512)\n        return out.embeddings[0]\n    if et == \"local\":\n        global _local_embed_model\n        if _local_embed_model is None:\n            from sentence_transformers import SentenceTransformer\n            _local_embed_model = SentenceTransformer('BAAI/bge-small-en-v1.5')\n        # Normalize embeddings for cosine distance\n        return _local_embed_model.encode([text], normalize_embeddings=True, show_progress_bar=False)[0].tolist()\n    # default openai\n    client = _lazy_import_openai()\n    resp = client.embeddings.create(input=text, model=\"text-embedding-3-large\")\n    return resp.data[0].embedding"}
{"id":21,"text":"rrf(\n    dense: list,\n    sparse: list,\n    k: int = 10,\n    kdiv: int = 60\n) -> list:\n    \"\"\"\n    Reciprocal Rank Fusion (RRF) for combining dense and sparse retrieval results.\n\n    Args:\n        dense (List): Ranked list of IDs from dense retrieval.\n        sparse (List): Ranked list of IDs from sparse retrieval.\n        k (int, optional): Number of top results to return. Defaults to 10.\n        kdiv (int, optional): RRF constant to dampen rank impact. Defaults to 60.\n\n    Returns:\n        List: Top-k fused IDs by RRF score.\n    \"\"\"\n    score: dict = collections.defaultdict(float)\n    for rank, pid in enumerate(dense, start=1):\n        score[pid] += 1.0 / (kdiv + rank)\n    for rank, pid in enumerate(sparse, start=1):\n        score[pid] += 1.0 / (kdiv + rank)\n    ranked = sorted(score.items(), key=lambda x: x[1], reverse=True)\n    return [pid for pid, _ in ranked[:k]]_load_chunks(repo: str) -> List[Dict]:\n    p = os.path.join(os.path.dirname(__file__), 'out', repo, 'chunks.jsonl')\n    chunks = []\n    if os.path.exists(p):\n        with open(p, 'r', encoding='utf-8') as f:\n            for line in f:\n                chunks.append(json.loads(line))\n    return chunks"}
{"id":22,"text":"_load_bm25_map(idx_dir: str):\n    # Prefer point IDs (UUID strings) aligned with Qdrant\n    pid_json = os.path.join(idx_dir, 'bm25_point_ids.json')\n    if os.path.exists(pid_json):\n        m = json.load(open(pid_json))\n        return [m[str(i)] for i in range(len(m))]\n    # Fallback to chunk_ids.txt (string chunk IDs)\n    map_path = os.path.join(idx_dir, 'chunk_ids.txt')\n    if os.path.exists(map_path):\n        with open(map_path, 'r', encoding='utf-8') as f:\n            ids = [line.strip() for line in f if line.strip()]\n        return ids\n    return None\n_load_cards_bm25(repo: str):\n    idx_dir = os.path.join(os.path.dirname(__file__), 'out', repo, 'bm25_cards')\n    try:\n        import bm25s\n        retr = bm25s.BM25.load(idx_dir)\n        return retr\n    except Exception:\n        return None"}
{"id":23,"text":"_load_cards_map(repo: str) -> Dict:\n    \"\"\"Load cards to get chunk ID mapping. Returns dict with card index -> chunk_id and chunk_id -> card data.\"\"\"\n    cards_file = os.path.join(os.path.dirname(__file__), 'out', repo, 'cards.jsonl')\n    cards_by_idx = {}  # card corpus index -> chunk_id\n    cards_by_chunk_id = {}  # chunk_id -> card metadata\n    try:\n        with open(cards_file, 'r', encoding='utf-8') as f:\n            for idx, line in enumerate(f):\n                card = json.loads(line)\n                chunk_id = str(card.get('id', ''))\n                if chunk_id:\n                    cards_by_idx[idx] = chunk_id\n                    cards_by_chunk_id[chunk_id] = card\n        return {'by_idx': cards_by_idx, 'by_chunk_id': cards_by_chunk_id}\n    except Exception:\n        return {'by_idx': {}, 'by_chunk_id': {}}"}
{"id":24,"text":"search(query: str, repo: str, topk_dense: int = 75, topk_sparse: int = 75, final_k: int = 10) -> List[Dict]:\n    chunks = _load_chunks(repo)\n    if not chunks:\n        return []\n\n    # ---- Dense (Qdrant) ----\n    dense_pairs = []\n    qc = QdrantClient(url=QDRANT_URL)\n    coll = os.getenv('COLLECTION_NAME', f'code_chunks_{repo}')\n    try:\n        e = _get_embedding(query, kind=\"query\")\n    except Exception:\n        e = []\n    try:\n        dres = qc.query_points(\n            collection_name=coll,\n            query=e,\n            using='dense',\n            limit=topk_dense,\n            with_payload=models.PayloadSelectorInclude(include=['file_path','start_line','end_line','language','layer','repo','hash','id'])\n        )\n        points = getattr(dres, 'points', dres)\n        dense_pairs = [(str(p.id), dict(p.payload)) for p in points]  # type: ignore\n    except Exception:\n        dense_pairs = []\n\n    # ---- Sparse (BM25S) ----\n    idx_dir = os.path.join(os.path.dirname(__file__), 'out', repo, 'bm25_index')\n    retriever = bm25s.BM25.load(idx_dir)\n    tokenizer = Tokenizer(stemmer=Stemmer('english'), stopwords='en')\n    tokens = tokenizer.tokenize([query])\n    ids, _ = retriever.retrieve(tokens, k=topk_sparse)\n    # ids shaped (1, k)\n    ids = ids.tolist()[0] if hasattr(ids, 'tolist') else list(ids[0])\n    id_map = _load_bm25_map(idx_dir)\n    by_chunk_id = {str(c['id']): c for c in chunks}\n    sparse_pairs = []\n    for i in ids:\n        if id_map is not None:\n            if 0 <= i < len(id_map):\n                pid_or_cid = id_map[i]\n                key = str(pid_or_cid)\n                if key in by_chunk_id:\n                    # id_map contained chunk id\n                    sparse_pairs.append((key, by_chunk_id[key]))\n                else:\n                    # Fallback to corpus order alignment\n                    if 0 <= i < len(chunks):\n                        sparse_pairs.append((str(chunks[i]['id']), chunks[i]))\n        else:\n            # fallback to corpus order alignment\n            if 0 <= i < len(chunks):\n                sparse_pairs.append((str(chunks[i]['id']), chunks[i]))\n\n    # Card-based BM25 boosting: retrieve cards and boost matching chunks\n    card_chunk_ids: set = set()\n    cards_retr = _load_cards_bm25(repo)\n    if cards_retr is not None:\n        try:\n            cards_map = _load_cards_map(repo)\n            tokens = tokenizer.tokenize([query])\n            c_ids, _ = cards_retr.retrieve(tokens, k=min(topk_sparse, 30))\n            # Map card indices to chunk IDs\n            c_ids_flat = c_ids[0] if hasattr(c_ids, '__getitem__') else c_ids\n            for card_idx in c_ids_flat:\n                chunk_id = cards_map['by_idx'].get(int(card_idx))\n                if chunk_id:\n                    card_chunk_ids.add(str(chunk_id))\n        except Exception:\n            pass\n\n    # Fuse\n    dense_ids = [pid for pid,_ in dense_pairs]\n    sparse_ids = [pid for pid,_ in sparse_pairs]\n    fused = rrf(dense_ids, sparse_ids, k=max(final_k, 2*final_k)) if dense_pairs else sparse_ids[:final_k]\n    by_id = {pid: p for pid,p in (dense_pairs + sparse_pairs)}\n    docs = [by_id[pid] for pid in fused if pid in by_id]\n    # Hydrate code bodies from local cache before CE rerank\n    cache = _load_code_cache(repo)\n    for d in docs:\n        if not d.get('code'):\n            h = d.get('hash')\n            cid = str(d.get('id',''))\n            d['code'] = cache['by_hash'].get(h) or cache['by_id'].get(cid) or ''\n    docs = ce_rerank(query, docs, top_k=final_k)\n    # Apply path + layer intent + provider + feature + card + (optional) origin bonuses, then resort\n    intent = _classify_query(query)\n    for d in docs:\n        layer_bonus = _project_layer_bonus(d.get('layer',''), intent) if repo=='project' else _project_layer_bonus(d.get('layer',''), intent)\n        origin_bonus = _origin_bonus(d.get('origin',''), VENDOR_MODE) if 'VENDOR_MODE' in os.environ else 0.0\n        repo_tag = d.get('repo', repo)\n        chunk_id = str(d.get('id', ''))\n        d['rerank_score'] = float(\n            d.get('rerank_score', 0.0)\n            + _path_bonus(d.get('file_path', ''))\n            + _project_path_boost(d.get('file_path',''), repo_tag)\n            + layer_bonus\n            + _provider_plugin_hint(d.get('file_path', ''), d.get('code', '')[:1000])\n            + _feature_bonus(query, d.get('file_path',''), d.get('code','')[:800])\n            + _card_bonus(chunk_id, card_chunk_ids)\n            + origin_bonus\n        )\n    docs.sort(key=lambda x: x.get('rerank_score', 0.0), reverse=True)\n    return docs[:final_k]\n\n# Local code cache to hydrate code bodies from chunks.jsonl instead of Qdrant payloads\n_code_cache_by_repo: dict[str, dict] = {}"}
{"id":25,"text":"_load_code_cache(repo: str):\n    import json\n    if repo in _code_cache_by_repo:\n        return _code_cache_by_repo[repo]\n    jl = os.path.join(os.path.dirname(__file__), 'out', repo, 'chunks.jsonl')\n    cache: dict[str, dict[str, str]] = {'by_hash': {}, 'by_id': {}}\n    try:\n        with open(jl, 'r', encoding='utf-8') as f:\n            for line in f:\n                try:\n                    o = json.loads(line)\n                except Exception:\n                    continue\n                h = o.get('hash')\n                cid = str(o.get('id', ''))\n                code = o.get('code', '')\n                if h:\n                    cache['by_hash'][h] = code\n                if cid:\n                    cache['by_id'][cid] = code\n    except FileNotFoundError:\n        pass\n    _code_cache_by_repo[repo] = cache\n    return cache\n\n# --- filename/path boosts applied post-rerank ---"}
{"id":26,"text":"_apply_filename_boosts(docs: list[dict], question: str) -> None:\n    terms = set((question or '').lower().replace('/', ' ').replace('-', ' ').split())\n    for d in docs:\n        fp = (d.get('file_path') or '').lower()\n        fn = os.path.basename(fp)\n        parts = fp.split('/')\n        score = float(d.get('rerank_score', 0.0) or 0.0)\n        if any(t and t in fn for t in terms):\n            score *= 1.5\n        if any(t and t in p for t in terms for p in parts):\n            score *= 1.2\n        d['rerank_score'] = score\n    docs.sort(key=lambda x: x.get('rerank_score', 0.0), reverse=True)\n\n# --- Strict per-repo routing helpers (no fusion) ---"}
{"id":27,"text":"route_repo(query: str, default_repo: str | None = None) -> str:\n    \"\"\"Route to a repo using repos.json config and lightweight prefixing.\n\n    - Supports explicit prefix: \"<name>: question\"\n    - Falls back to keyword voting as configured in repos.json\n    - Defaults to configured default_repo (repos.json) or env REPO\n    \"\"\"\n    try:\n        # Prefer config-driven choice (handles prefixes + keywords)\n        return choose_repo_from_query(query, default=(default_repo or get_default_repo()))\n    except Exception:\n        # Very safe fallback\n        q = (query or '').lower().strip()\n        if ':' in q:\n            cand, _ = q.split(':', 1)\n            cand = cand.strip()\n            if cand:\n                return cand\n        return (default_repo or os.getenv('REPO', 'project') or 'project').strip()\nsearch_routed(query: str, repo_override: str | None = None, final_k: int = 10):\n    repo = (repo_override or route_repo(query, default_repo=os.getenv('REPO', 'project')) or os.getenv('REPO', 'project')).strip()\n    return search(query, repo=repo, final_k=final_k)\n\n# Multi-query expansion (cheap) and routed search"}
{"id":28,"text":"expand_queries(query: str, m: int = 4) -> list[str]:\n    try:\n        sys = \"Rewrite a developer query into multiple search-friendly variants without changing meaning.\"\n        user = f\"Count: {m}\\nQuery: {query}\\nOutput one variant per line, no numbering.\"\n        text, _ = generate_text(user_input=user, system_instructions=sys, reasoning_effort=None)\n        lines = [ln.strip('- ').strip() for ln in (text or '').splitlines() if ln.strip()]\n        uniq = []\n        for ln in lines:\n            if ln and ln not in uniq:\n                uniq.append(ln)\n        return (uniq or [query])[:m]\n    except Exception:\n        return [query]"}
{"id":29,"text":"search_routed_multi(query: str, repo_override: str | None = None, m: int = 4, final_k: int = 10):\n    repo = (repo_override or route_repo(query) or os.getenv('REPO','project')).strip()\n    variants = expand_queries(query, m=m)\n    all_docs = []\n    for qv in variants:\n        docs = search(qv, repo=repo, final_k=final_k)\n        all_docs.extend(docs)\n    # Deduplicate by file_path + line span\n    seen = set()\n    uniq = []\n    for d in all_docs:\n        key = (d.get('file_path'), d.get('start_line'), d.get('end_line'))\n        if key in seen:\n            continue\n        seen.add(key)\n        uniq.append(d)\n    # Rerank union\n    try:\n        from rerank import rerank_results as ce_rerank\n        reranked = ce_rerank(query, uniq, top_k=final_k)\n        _apply_filename_boosts(reranked, query)\n        return reranked\n    except Exception:\n        return uniq[:final_k]"}
{"id":30,"text":"# coding: utf-8\n\nPRUNE_DIRS = {\n    '.git', '.worktrees', '.venv', 'venv', 'env', '.venv_ci',\n    'node_modules', 'vendor', 'dist', 'build',\n    '.next', '.turbo', '.svelte-kit', 'coverage',\n    'site', '_site', '__pycache__', '.pytest_cache', '.mypy_cache', '.cache'\n}\n\nVALID_EXTS = (\n    '.py', '.ts', '.tsx', '.js', '.jsx', '.go', '.rs', '.java', '.c', '.cpp',\n    '.md', '.mdx', '.yaml', '.yml', '.toml', '.json'\n)\n\nSKIP_EXTS = ('.map', '.pyc', '.ds_store')\n_should_index_file(name):\n    n = name.lower()\n    if n.endswith(SKIP_EXTS):\n        return False\n    return n.endswith(VALID_EXTS)\n_prune_dirs_in_place(dirs):\n    # remove noisy dirs without descending into them\n    dirs[:] = [d for d in dirs if d not in PRUNE_DIRS]"}
{"id":31,"text":"import os\nimport json\nfrom typing import Optional, Dict, Any, Tuple\n\ntry:\n    from openai import OpenAI\nexcept Exception as e:\n    raise RuntimeError(\"openai>=1.x is required for Responses API\") from e\n\n# Model pin (Responses API): default to OpenAI gpt-4o-mini\n# Users can override with GEN_MODEL (e.g., gpt-4.1, o4-mini, gpt-4o)\n# Avoid local defaults; prefer OpenAI to reduce confusion.\n_DEFAULT_MODEL = os.getenv(\"GEN_MODEL\", os.getenv(\"ENRICH_MODEL\", \"gpt-4o-mini\"))\n\n_client = None\n\n# MLX backend (lazy-loaded for Apple Silicon)\n_mlx_model = None\n_mlx_tokenizer = None\n_get_mlx_model():\n    \"\"\"Lazy-load MLX model for Apple Silicon generation\"\"\"\n    global _mlx_model, _mlx_tokenizer\n    if _mlx_model is None:\n        from mlx_lm import load\n        model_name = os.getenv(\"GEN_MODEL\", \"mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit\")\n        _mlx_model, _mlx_tokenizer = load(model_name)\n    return _mlx_model, _mlx_tokenizer"}
{"id":32,"text":"client() -> OpenAI:\n    global _client\n    if _client is None:\n        # Let OPENAI_API_KEY and OPENAI_BASE_URL (if set) drive the target\n        # This allows using OpenAI-hosted models or an OpenAI-compatible server (e.g., vLLM/Ollama proxy)\n        _client = OpenAI()\n    return _client\n\n_extract_text(resp: Any) -> str:\n    # Prefer .output_text if present (library convenience), else parse the structure\n    txt = \"\"\n    if hasattr(resp, \"output_text\") and isinstance(getattr(resp, \"output_text\"), str):\n        txt = resp.output_text\n        if txt:\n            return txt\n    try:\n        # Fallback path: resp.output[0].content[0].text\n        out = getattr(resp, \"output\", None)\n        if out and len(out) > 0:\n            cont = getattr(out[0], \"content\", None)\n            if cont and len(cont) > 0 and hasattr(cont[0], \"text\"):\n                return cont[0].text or \"\"\n    except Exception:\n        pass\n    return txt or \"\""}
{"id":33,"text":"generate_text(\n    user_input: str,\n    *,\n    system_instructions: Optional[str] = None,\n    model: Optional[str] = None,\n    reasoning_effort: Optional[str] = None,  # e.g., \"low\" | \"medium\" | \"high\"\n    response_format: Optional[Dict[str, Any]] = None,  # e.g., {\"type\":\"json_object\"}\n    store: bool = False,\n    previous_response_id: Optional[str] = None,\n    extra: Optional[Dict[str, Any]] = None,\n) -> Tuple[str, Any]:\n    \"\"\"\n    Minimal wrapper over Responses API:\n      - Uses 'instructions' for system prompt and 'input' for user text\n      - Supports response_format (JSON mode / structured)\n      - Leaves tool-calling to upstream if needed (MCP/file_search handled elsewhere)\n    \"\"\"\n    mdl = model or _DEFAULT_MODEL\n    kwargs: Dict[str, Any] = {\n        \"model\": mdl,\n        \"input\": user_input,\n        \"store\": store,\n    }\n    if system_instructions:\n        kwargs[\"instructions\"] = system_instructions\n    if reasoning_effort:\n        kwargs[\"reasoning\"] = {\"effort\": reasoning_effort}\n    if response_format:\n        kwargs[\"response_format\"] = response_format\n    if previous_response_id:\n        kwargs[\"previous_response_id\"] = previous_response_id\n    if extra:\n        kwargs.update(extra)\n\n    # Check for MLX backend (Apple Silicon, highest priority for local models)\n    ENRICH_BACKEND = os.getenv(\"ENRICH_BACKEND\", \"\").lower()\n    is_mlx_model = mdl.startswith(\"mlx-community/\") if mdl else False\n    prefer_mlx = (ENRICH_BACKEND == \"mlx\") or is_mlx_model\n\n    if prefer_mlx:\n        try:\n            from mlx_lm import generate\n            model, tokenizer = _get_mlx_model()\n\n            # Build prompt with system instructions if provided\n            sys_text = (system_instructions or \"\").strip()\n            prompt = (f\"<system>{sys_text}</system>\\n\" if sys_text else \"\") + user_input\n\n            # Generate with MLX\n            text = generate(\n                model,\n                tokenizer,\n                prompt=prompt,\n                max_tokens=2048,  # More tokens for answer generation\n                verbose=False\n            )\n            return text, {\"response\": text, \"backend\": \"mlx\"}\n        except Exception as e:\n            # Fall through to Ollama/OpenAI on MLX failure\n            pass\n\n    # If using local Qwen via Ollama, prefer its API first\n    OLLAMA_URL = os.getenv(\"OLLAMA_URL\")\n    prefer_ollama = bool(OLLAMA_URL) and (\"qwen\" in (mdl or \"\").lower())\n    if prefer_ollama:\n        try:\n            import requests, json as _json, time\n            sys_text = (system_instructions or \"\").strip()\n            prompt = (f\"<system>{sys_text}</system>\\n\" if sys_text else \"\") + user_input\n            url = OLLAMA_URL.rstrip(\"/\") + \"/generate\"\n\n            # Retry configuration (production hardening)\n            max_retries = 2\n            chunk_timeout = 60  # 60s per chunk\n            total_timeout = 300  # 5min hard cap\n\n            for attempt in range(max_retries + 1):\n                start_time = time.time()\n                try:\n                    # Prefer streaming to avoid long blocking on large models\n                    with requests.post(url, json={\n                        \"model\": mdl,\n                        \"prompt\": prompt,\n                        \"stream\": True,\n                        \"options\": {\"temperature\": 0.2, \"num_ctx\": 8192},\n                    }, timeout=chunk_timeout, stream=True) as r:\n                        r.raise_for_status()\n                        buf = []\n                        last = None\n                        for line in r.iter_lines(decode_unicode=True):\n                            # Check total timeout\n                            if time.time() - start_time > total_timeout:\n                                # Return partial output on timeout\n                                partial = (\"\".join(buf) or \"\").strip()\n                                if partial:\n                                    return partial + \" [TIMEOUT]\", {\"response\": partial, \"timeout\": True}\n                                break\n\n                            if not line:\n                                continue\n                            try:\n                                obj = _json.loads(line)\n                            except Exception:\n                                continue\n                            if isinstance(obj, dict):\n                                seg = (obj.get(\"response\") or \"\")\n                                if seg:\n                                    buf.append(seg)\n                                last = obj\n                                if obj.get(\"done\") is True:\n                                    break\n\n                        text = (\"\".join(buf) or \"\").strip()\n                        if text:\n                            return text, (last or {\"response\": text})\n\n                    # Fallback to non-stream if streaming returned empty\n                    resp = requests.post(url, json={\n                        \"model\": mdl,\n                        \"prompt\": prompt,\n                        \"stream\": False,\n                        \"options\": {\"temperature\": 0.2, \"num_ctx\": 8192},\n                    }, timeout=total_timeout)\n                    resp.raise_for_status()\n                    data = resp.json()\n                    text = (data.get(\"response\") or \"\").strip()\n                    if text:\n                        return text, data\n\n                except (requests.Timeout, requests.ConnectionError) as e:\n                    # Retry on network errors\n                    if attempt < max_retries:\n                        backoff = 2 ** attempt  # Exponential backoff: 1s, 2s\n                        time.sleep(backoff)\n                        continue\n                    # Last attempt failed, fall through to OpenAI\n                    pass\n                except Exception:\n                    # Other errors, don't retry\n                    break\n        except Exception:\n            pass\n    # Try OpenAI Responses API\n    try:\n        resp = client().responses.create(**kwargs)\n        text = _extract_text(resp)\n        return text, resp\n    except Exception:\n        # Fallback to Chat Completions for OpenAI-compatible servers that don't expose Responses API\n        try:\n            messages = []\n            if system_instructions:\n                messages.append({\"role\": \"system\", \"content\": system_instructions})\n            messages.append({\"role\": \"user\", \"content\": user_input})\n            ckwargs: Dict[str, Any] = {\"model\": mdl, \"messages\": messages}\n            if response_format and isinstance(response_format, dict):\n                ckwargs[\"response_format\"] = response_format\n            cc = client().chat.completions.create(**ckwargs)\n            text = (cc.choices[0].message.content if getattr(cc, \"choices\", []) else \"\") or \"\"\n            return text, cc\n        except Exception as e:\n            raise RuntimeError(f\"Generation failed for model={mdl}: {e}\")"}
{"id":34,"text":"#!/usr/bin/env python3\n\"\"\"\nMinimal eval loop with regression tracking.\n\nUsage:\n  python eval_loop.py                    # Run once\n  python eval_loop.py --watch            # Run on file changes\n  python eval_loop.py --baseline         # Save current results as baseline\n  python eval_loop.py --compare          # Compare against baseline\n\"\"\"\nimport os\nimport sys\nimport json\nimport time\nimport argparse\nfrom pathlib import Path\nfrom typing import Dict, List, Any\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Import eval logic\nfrom eval_rag import main as run_eval, hit, GOLDEN_PATH, USE_MULTI, FINAL_K\nfrom hybrid_search import search_routed, search_routed_multi\n\n\nBASELINE_PATH = os.getenv('BASELINE_PATH', 'eval_baseline.json')"}
{"id":35,"text":"run_eval_with_results() -> Dict[str, Any]:\n    \"\"\"Run eval and return detailed results.\"\"\"\n    if not os.path.exists(GOLDEN_PATH):\n        return {\"error\": f\"No golden file at {GOLDEN_PATH}\"}\n\n    gold = json.load(open(GOLDEN_PATH))\n    total = len(gold)\n    hits_top1 = 0\n    hits_topk = 0\n    results = []\n\n    t0 = time.time()\n    for i, row in enumerate(gold, 1):\n        q = row['q']\n        repo = row.get('repo') or os.getenv('REPO', 'project')\n        expect = row.get('expect_paths') or []\n\n        if USE_MULTI:\n            docs = search_routed_multi(q, repo_override=repo, m=4, final_k=FINAL_K)\n        else:\n            docs = search_routed(q, repo_override=repo, final_k=FINAL_K)\n\n        paths = [d.get('file_path', '') for d in docs]\n        top1_hit = hit(paths[:1], expect) if paths else False\n        topk_hit = hit(paths, expect) if paths else False\n\n        if top1_hit:\n            hits_top1 += 1\n        if topk_hit:\n            hits_topk += 1\n\n        results.append({\n            \"question\": q,\n            \"repo\": repo,\n            \"expect_paths\": expect,\n            \"top1_path\": paths[:1],\n            \"top1_hit\": top1_hit,\n            \"topk_hit\": topk_hit,\n            \"top_paths\": paths[:FINAL_K]\n        })\n\n    dt = time.time() - t0\n\n    return {\n        \"total\": total,\n        \"top1_hits\": hits_top1,\n        \"topk_hits\": hits_topk,\n        \"top1_accuracy\": round(hits_top1 / max(1, total), 3),\n        \"topk_accuracy\": round(hits_topk / max(1, total), 3),\n        \"final_k\": FINAL_K,\n        \"use_multi\": USE_MULTI,\n        \"duration_secs\": round(dt, 2),\n        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n        \"results\": results\n    }"}
{"id":36,"text":"save_baseline(results: Dict[str, Any]):\n    \"\"\"Save current results as baseline.\"\"\"\n    with open(BASELINE_PATH, 'w') as f:\n        json.dump(results, f, indent=2)\n    print(f\"âœ“ Baseline saved to {BASELINE_PATH}\")"}
{"id":37,"text":"compare_with_baseline(current: Dict[str, Any]):\n    \"\"\"Compare current results with baseline.\"\"\"\n    if not os.path.exists(BASELINE_PATH):\n        print(f\"âš  No baseline found at {BASELINE_PATH}\")\n        print(f\"  Run with --baseline to create one\")\n        return\n\n    with open(BASELINE_PATH) as f:\n        baseline = json.load(f)\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"REGRESSION CHECK: Current vs Baseline\")\n    print(\"=\"*60)\n\n    curr_top1 = current[\"top1_accuracy\"]\n    base_top1 = baseline[\"top1_accuracy\"]\n    curr_topk = current[\"topk_accuracy\"]\n    base_topk = baseline[\"topk_accuracy\"]\n\n    delta_top1 = curr_top1 - base_top1\n    delta_topk = curr_topk - base_topk\n\n    print(f\"\\nTop-1 Accuracy:\")\n    print(f\"  Baseline: {base_top1:.3f}\")\n    print(f\"  Current:  {curr_top1:.3f}\")\n    print(f\"  Delta:    {delta_top1:+.3f} {'âœ“' if delta_top1 >= 0 else 'âœ—'}\")\n\n    print(f\"\\nTop-{FINAL_K} Accuracy:\")\n    print(f\"  Baseline: {base_topk:.3f}\")\n    print(f\"  Current:  {curr_topk:.3f}\")\n    print(f\"  Delta:    {delta_topk:+.3f} {'âœ“' if delta_topk >= 0 else 'âœ—'}\")\n\n    # Check for regressions per-question\n    regressions = []\n    improvements = []\n\n    for i, (curr_res, base_res) in enumerate(zip(current[\"results\"], baseline[\"results\"])):\n        if curr_res[\"question\"] != base_res[\"question\"]:\n            continue  # skip if questions don't align\n\n        if base_res[\"top1_hit\"] and not curr_res[\"top1_hit\"]:\n            regressions.append((i+1, curr_res[\"question\"], curr_res[\"repo\"]))\n        elif not base_res[\"top1_hit\"] and curr_res[\"top1_hit\"]:\n            improvements.append((i+1, curr_res[\"question\"], curr_res[\"repo\"]))\n\n    if regressions:\n        print(f\"\\nâš  REGRESSIONS ({len(regressions)} questions):\")\n        for idx, q, repo in regressions:\n            print(f\"  [{idx}] {repo}: {q}\")\n\n    if improvements:\n        print(f\"\\nâœ“ IMPROVEMENTS ({len(improvements)} questions):\")\n        for idx, q, repo in improvements:\n            print(f\"  [{idx}] {repo}: {q}\")\n\n    if not regressions and delta_top1 >= -0.05 and delta_topk >= -0.05:\n        print(\"\\nâœ“ No significant regressions detected\")\n        return True\n    else:\n        print(\"\\nâœ— Regressions detected!\")\n        return False"}
{"id":38,"text":"watch_mode():\n    \"\"\"Watch for file changes and re-run eval.\"\"\"\n    print(\"â± Watch mode: monitoring for changes...\")\n    print(f\"   Watching: {GOLDEN_PATH}, hybrid_search.py, langgraph_app.py\")\n\n    files_to_watch = [\n        GOLDEN_PATH,\n        \"hybrid_search.py\",\n        \"langgraph_app.py\",\n        \"index_repo.py\",\n        \"rerank.py\"\n    ]\n\n    last_mtimes = {}\n    for fp in files_to_watch:\n        if os.path.exists(fp):\n            last_mtimes[fp] = os.path.getmtime(fp)\n\n    while True:\n        time.sleep(5)\n        changed = False\n        for fp in files_to_watch:\n            if not os.path.exists(fp):\n                continue\n            mtime = os.path.getmtime(fp)\n            if fp not in last_mtimes or mtime > last_mtimes[fp]:\n                print(f\"\\nðŸ”„ Change detected: {fp}\")\n                last_mtimes[fp] = mtime\n                changed = True\n\n        if changed:\n            print(\"\\n\" + \"=\"*60)\n            print(\"Running eval...\")\n            print(\"=\"*60)\n            results = run_eval_with_results()\n            if \"error\" in results:\n                print(f\"Error: {results['error']}\")\n            else:\n                print(json.dumps({\n                    \"top1_accuracy\": results[\"top1_accuracy\"],\n                    \"topk_accuracy\": results[\"topk_accuracy\"],\n                    \"total\": results[\"total\"],\n                    \"duration_secs\": results[\"duration_secs\"]\n                }, indent=2))"}
{"id":39,"text":"main():\n    parser = argparse.ArgumentParser(description=\"RAG eval loop with regression tracking\")\n    parser.add_argument(\"--baseline\", action=\"store_true\", help=\"Save current results as baseline\")\n    parser.add_argument(\"--compare\", action=\"store_true\", help=\"Compare current results with baseline\")\n    parser.add_argument(\"--watch\", action=\"store_true\", help=\"Watch for file changes and re-run\")\n    parser.add_argument(\"--json\", action=\"store_true\", help=\"Output results as JSON\")\n\n    args = parser.parse_args()\n\n    if args.watch:\n        watch_mode()\n        return\n\n    print(\"Running eval...\")\n    results = run_eval_with_results()\n\n    if \"error\" in results:\n        print(f\"Error: {results['error']}\", file=sys.stderr)\n        sys.exit(1)\n\n    if args.json:\n        print(json.dumps(results, indent=2))\n    else:\n        print(\"\\n\" + \"=\"*60)\n        print(\"EVAL RESULTS\")\n        print(\"=\"*60)\n        print(f\"Total questions: {results['total']}\")\n        print(f\"Top-1 accuracy:  {results['top1_accuracy']:.1%} ({results['top1_hits']}/{results['total']})\")\n        print(f\"Top-{FINAL_K} accuracy: {results['topk_accuracy']:.1%} ({results['topk_hits']}/{results['total']})\")\n        print(f\"Duration:        {results['duration_secs']}s\")\n        print(f\"Timestamp:       {results['timestamp']}\")\n\n        # Show failures\n        failures = [r for r in results[\"results\"] if not r[\"topk_hit\"]]\n        if failures:\n            print(f\"\\nâš  Failures ({len(failures)}):\")\n            for r in failures:\n                print(f\"  [{r['repo']}] {r['question']}\")\n                print(f\"    Expected: {r['expect_paths']}\")\n                print(f\"    Got: {r['top_paths'][:3]}\")\n\n    if args.baseline:\n        save_baseline(results)\n    elif args.compare:\n        compare_with_baseline(results)\n\n\nif __name__ == \"__main__\":\n    main()"}
{"id":40,"text":"import os, json\nfrom typing import Dict\nfrom dotenv import load_dotenv\nfrom env_model import generate_text\n\nload_dotenv()\nREPO = os.getenv('REPO','project').strip()\nMAX_CHUNKS = int(os.getenv('CARDS_MAX','0') or '0')\nBASE = os.path.join(os.path.dirname(__file__), 'out', REPO)\nCHUNKS = os.path.join(BASE, 'chunks.jsonl')\nCARDS = os.path.join(BASE, 'cards.jsonl')\nCARDS_TXT = os.path.join(BASE, 'cards.txt')\nINDEX_DIR = os.path.join(BASE, 'bm25_cards')\n\nPROMPT = (\n    \"Summarize this code chunk for retrieval as a JSON object with keys: \"\n    \"symbols (array of names: functions/classes/components/routes), purpose (short sentence), \"\n    \"routes (array of route paths if any). Respond with only the JSON.\\n\\n\"\n)\niter_chunks():\n    with open(CHUNKS, 'r', encoding='utf-8') as f:\n        for line in f:\n            o = json.loads(line)\n            yield o"}
{"id":41,"text":"main():\n    os.makedirs(BASE, exist_ok=True)\n    # Responses API via env_model.generate_text\n    n = 0\n    with open(CARDS, 'w', encoding='utf-8') as out_json, open(CARDS_TXT, 'w', encoding='utf-8') as out_txt:\n        for ch in iter_chunks():\n            code = ch.get('code','')\n            fp = ch.get('file_path','')\n            snippet = code[:2000]\n            msg = PROMPT + snippet\n            try:\n                text, _ = generate_text(user_input=msg, system_instructions=None, reasoning_effort=None, response_format={\"type\": \"json_object\"})\n                content = (text or '').strip()\n                card: Dict = json.loads(content) if content else {\"symbols\": [], \"purpose\": \"\", \"routes\": []}\n            except Exception:\n                card = {\"symbols\": [], \"purpose\": \"\", \"routes\": []}\n            card['file_path'] = fp\n            card['id'] = ch.get('id')\n            out_json.write(json.dumps(card, ensure_ascii=False) + '\\n')\n            # Text for BM25\n            text = ' '.join(card.get('symbols', [])) + '\\n' + card.get('purpose','') + '\\n' + ' '.join(card.get('routes', [])) + '\\n' + fp\n            out_txt.write(text.replace('\\n',' ') + '\\n')\n            n += 1\n            if MAX_CHUNKS and n >= MAX_CHUNKS:\n                break\n    # Build BM25 over cards text\n    try:\n        import bm25s\n        from bm25s.tokenization import Tokenizer\n        from Stemmer import Stemmer\n        stemmer = Stemmer('english'); tok = Tokenizer(stemmer=stemmer, stopwords='en')\n        with open(CARDS_TXT,'r',encoding='utf-8') as f:\n            docs = [line.strip() for line in f if line.strip()]\n        tokens = tok.tokenize(docs)\n        retriever = bm25s.BM25(method='lucene', k1=1.2, b=0.65)\n        retriever.index(tokens)\n        # Workaround: ensure JSON-serializable vocab keys\n        try:\n            retriever.vocab_dict = {str(k): v for k, v in retriever.vocab_dict.items()}\n        except Exception:\n            pass\n        os.makedirs(INDEX_DIR, exist_ok=True)\n        retriever.save(INDEX_DIR, corpus=docs)\n        tok.save_vocab(save_dir=INDEX_DIR)\n        tok.save_stopwords(save_dir=INDEX_DIR)\n        print(f\"Built cards BM25 index with {len(docs)} docs at {INDEX_DIR}\")\n    except Exception as e:\n        print('BM25 build failed:', e)\n\nif __name__ == '__main__':\n    main()"}
{"id":42,"text":"from __future__ import annotations\nimport os, json\nfrom typing import Literal, Dict, Any\n\nfrom fastmcp import FastMCP\n\n# Reuse internal pipeline\nfrom langgraph_app import build_graph\nfrom hybrid_search import search_routed_multi\n\n\nmcp = FastMCP(\"rag-service\")\n_graph = None\n\n_get_graph():\n    global _graph\n    if _graph is None:\n        _graph = build_graph()\n    return _graph\n\n\n@mcp.tool()"}
{"id":43,"text":"answer(repo: Literal[\"project\", \"project\"], question: str) -> Dict[str, Any]:\n    \"\"\"Answer a codebase question using local LangGraph (retrieval+generation). Returns text + citations.\"\"\"\n    g = _get_graph()\n    cfg = {\"configurable\": {\"thread_id\": f\"http-{repo}\"}}\n    state = {\n        \"question\": question,\n        \"documents\": [],\n        \"generation\": \"\",\n        \"iteration\": 0,\n        \"confidence\": 0.0,\n        \"repo\": repo,\n    }\n    res = g.invoke(state, cfg)\n    docs = res.get(\"documents\", [])[:5]\n    citations = [f\"{d['file_path']}:{d['start_line']}-{d['end_line']}\" for d in docs]\n    return {\n        \"answer\": res.get(\"generation\", \"\"),\n        \"citations\": citations,\n        \"repo\": res.get(\"repo\", repo),\n        \"confidence\": float(res.get(\"confidence\", 0.0) or 0.0),\n    }\n\n\n@mcp.tool()"}
{"id":44,"text":"search(repo: Literal[\"project\", \"project\"], question: str, top_k: int = 10) -> Dict[str, Any]:\n    \"\"\"Retrieve relevant code locations without generation.\"\"\"\n    docs = search_routed_multi(question, repo_override=repo, m=4, final_k=top_k)\n    results = [\n        {\n            \"file_path\": d.get(\"file_path\", \"\"),\n            \"start_line\": d.get(\"start_line\", 0),\n            \"end_line\": d.get(\"end_line\", 0),\n            \"language\": d.get(\"language\", \"\"),\n            \"rerank_score\": float(d.get(\"rerank_score\", 0.0) or 0.0),\n            \"repo\": d.get(\"repo\", repo),\n        }\n        for d in docs\n    ]\n    return {\"results\": results, \"repo\": repo, \"count\": len(results)}\n\n\nif __name__ == \"__main__\":\n    # Serve over HTTP for remote MCP (platform evals). Use env overrides for host/port/path.\n    host = os.getenv(\"MCP_HTTP_HOST\", \"0.0.0.0\")\n    port = int(os.getenv(\"MCP_HTTP_PORT\", \"8013\"))\n    path = os.getenv(\"MCP_HTTP_PATH\", \"/mcp\")\n    mcp.run(transport=\"http\", host=host, port=port, path=path)"}
{"id":45,"text":"# Python auto-imports sitecustomize at startup if present in sys.path.\n# We use it to block legacy Chat Completions usage at runtime.\ntry:\n    import openai  # type: ignore\n\n    def _blocked(*_args, **_kwargs):  # noqa: D401\n        raise RuntimeError(\n            \"Legacy Chat Completions API is disabled. Use Responses API via env_model.generate_text().\\n\"\n            \"Docs: https://openai.com/index/new-tools-and-features-in-the-responses-api/\"\n        )\n\n    # Block classic patterns if present on this installed version\n    if hasattr(openai, \"ChatCompletion\"):\n        try:\n            openai.ChatCompletion.create = staticmethod(_blocked)  # type: ignore[attr-defined]\n        except Exception:\n            pass\n    # Some older clients expose nested chat.completions\n    if hasattr(openai, \"chat\"):\n        chat = getattr(openai, \"chat\")\n        if hasattr(chat, \"completions\"):\n            try:\n                chat.completions.create = _blocked  # type: ignore[attr-defined]\n            except Exception:\n                pass\nexcept Exception:\n    # If openai not installed yet, do nothing.\n    pass"}
{"id":46,"text":"import os, operator\nfrom typing import List, Dict, TypedDict, Annotated\nfrom pathlib import Path\nfrom dotenv import load_dotenv, find_dotenv\nfrom langgraph.graph import END, StateGraph\nfrom langgraph.checkpoint.redis import RedisSaver\nfrom hybrid_search import search_routed_multi as hybrid_search_routed_multi\nfrom env_model import generate_text\n\n# Load environment from repo root .env without hard-coded paths\ntry:\n    # Load any existing env first\n    load_dotenv(override=False)\n    repo_root = Path(__file__).resolve().parent\n    env_path = repo_root / \".env\"\n    if env_path.exists():\n        load_dotenv(dotenv_path=env_path, override=False)\n    else:\n        alt = find_dotenv(usecwd=True)\n        if alt:\n            load_dotenv(dotenv_path=alt, override=False)\nexcept Exception:\n    pass\nRAGState(TypedDict):\n    question: str\n    documents: Annotated[List[Dict], operator.add]\n    generation: str\n    iteration: int\n    confidence: float\n    repo: str\n\n# Responses API shim used via generate_text()"}
{"id":47,"text":"should_use_multi_query(question: str) -> bool:\n    q = (question or '').lower().strip()\n    if len(q.split()) <= 3:\n        return False\n    for w in (\"how\", \"why\", \"explain\", \"compare\", \"tradeoff\"):\n        if w in q:\n            return True\n    return False\nretrieve_node(state: RAGState) -> Dict:\n    q = state['question']\n    repo = state.get('repo') if isinstance(state, dict) else None\n    mq = int(os.getenv('MQ_REWRITES','2')) if should_use_multi_query(q) else 1\n    docs = hybrid_search_routed_multi(q, repo_override=repo, m=mq, final_k=20)\n    conf = float(sum(d.get('rerank_score',0.0) for d in docs)/max(1,len(docs)))\n    # Propagate the routed repo into state so downstream nodes build correct headers\n    repo_used = (repo or (docs[0].get('repo') if docs else os.getenv('REPO','project')))\n    return {'documents': docs, 'confidence': conf, 'iteration': state.get('iteration',0)+1, 'repo': repo_used}"}
{"id":48,"text":"route_after_retrieval(state:RAGState)->str:\n    conf = float(state.get(\"confidence\", 0.0) or 0.0)\n    it = int(state.get(\"iteration\", 0) or 0)\n    docs = state.get(\"documents\", []) or []\n    scores = sorted([float(d.get(\"rerank_score\",0.0) or 0.0) for d in docs], reverse=True)\n    top1 = scores[0] if scores else 0.0\n    avg5 = (sum(scores[:5])/min(5, len(scores))) if scores else 0.0\n    # Allow env overrides so teams can tighten gates without code changes\n    try:\n        CONF_TOP1 = float(os.getenv('CONF_TOP1', '0.62'))\n        CONF_AVG5 = float(os.getenv('CONF_AVG5', '0.55'))\n        CONF_ANY = float(os.getenv('CONF_ANY', '0.55'))\n    except Exception:\n        CONF_TOP1, CONF_AVG5, CONF_ANY = 0.62, 0.55, 0.55\n    if top1 >= CONF_TOP1 or avg5 >= CONF_AVG5 or conf >= CONF_ANY:\n        return \"generate\"\n    if it >= 3:\n        return \"fallback\"\n    return \"rewrite_query\""}
{"id":49,"text":"rewrite_query(state: RAGState) -> Dict:\n    q = state['question']\n    sys = \"You rewrite developer questions into search-optimized queries without changing meaning.\"\n    user = f\"Rewrite this for code search (expand CamelCase, include API nouns), one line.\\n\\n{q}\"\n    newq, _ = generate_text(user_input=user, system_instructions=sys, reasoning_effort=None)\n    newq = (newq or '').strip()\n    return {'question': newq}"}
{"id":50,"text":"generate_node(state: RAGState) -> Dict:\n    q = state['question']; ctx = state['documents'][:5]\n    citations = \"\\n\".join([f\"- {d['file_path']}:{d['start_line']}-{d['end_line']}\" for d in ctx])\n    context_text = \"\\n\\n\".join([d.get('code','') for d in ctx])\n    sys = 'You answer strictly from the provided code context. Always cite file paths and line ranges you used.'\n    user = f\"Question:\\n{q}\\n\\nContext:\\n{context_text}\\n\\nCitations (paths and line ranges):\\n{citations}\\n\\nAnswer:\"\n    content, _ = generate_text(user_input=user, system_instructions=sys, reasoning_effort=None)\n    content = content or ''\n    # Lightweight verifier: if confidence low, try multi-query retrieval and regenerate once\n    conf = float(state.get('confidence', 0.0) or 0.0)\n    if conf < 0.55:\n        repo = state.get('repo') or os.getenv('REPO','project')\n        alt_docs = hybrid_search_routed_multi(q, repo_override=repo, m=4, final_k=10)\n        if alt_docs:\n            ctx2 = alt_docs[:5]\n            citations2 = \"\\n\".join([f\"- {d['file_path']}:{d['start_line']}-{d['end_line']}\" for d in ctx2])\n            context_text2 = \"\\n\\n\".join([d.get('code','') for d in ctx2])\n            user2 = f\"Question:\\n{q}\\n\\nContext:\\n{context_text2}\\n\\nCitations (paths and line ranges):\\n{citations2}\\n\\nAnswer:\"\n            content2, _ = generate_text(user_input=user2, system_instructions=sys, reasoning_effort=None)\n            content = (content2 or content or '')\n    repo_hdr = state.get('repo') or (ctx[0].get('repo') if ctx else None) or os.getenv('REPO','project')\n    header = f\"[repo: {repo_hdr}]\"\n    return {'generation': header + \"\\n\" + content}"}
{"id":51,"text":"fallback_node(state: RAGState) -> Dict:\n    repo_hdr = state.get('repo') or (state.get('documents')[0].get('repo') if state.get('documents') else None) or os.getenv('REPO','project')\n    header = f\"[repo: {repo_hdr}]\"\n    msg = \"I don't have high confidence from local code. Try refining the question or expanding the context.\"\n    return {'generation': header + \"\\n\" + msg}"}
{"id":52,"text":"build_graph():\n    builder = StateGraph(RAGState)\n    builder.add_node('retrieve', retrieve_node)\n    builder.add_node('rewrite_query', rewrite_query)\n    builder.add_node('generate', generate_node)\n    builder.add_node('fallback', fallback_node)\n    builder.set_entry_point('retrieve')\n    builder.add_conditional_edges('retrieve', route_after_retrieval, {\n        'generate': 'generate', 'rewrite_query': 'rewrite_query', 'fallback': 'fallback'\n    })\n    builder.add_edge('rewrite_query', 'retrieve')\n    builder.add_edge('generate', END)\n    builder.add_edge('fallback', END)\n    DB_URI = os.getenv('REDIS_URL','redis://127.0.0.1:6379/0')\n    # Prefer Redis checkpointer, but do not fail hard if unavailable\n    try:\n        checkpointer = RedisSaver(redis_url=DB_URI)\n        graph = builder.compile(checkpointer=checkpointer)\n    except Exception:\n        graph = builder.compile()\n    return graph\n\nif __name__ == '__main__':\n    import sys\n    q = ' '.join(sys.argv[1:]) if len(sys.argv)>1 else 'Where is OAuth token validated?'\n    graph = build_graph(); cfg = {'configurable': {'thread_id': 'dev'}}\n    res = graph.invoke({'question': q, 'documents': [], 'generation':'', 'iteration':0, 'confidence':0.0}, cfg)\n    print(res['generation'])"}
{"id":53,"text":"import os, json, time\nfrom typing import List, Dict\nfrom dotenv import load_dotenv\nfrom hybrid_search import search_routed, search_routed_multi\n\nload_dotenv()\n\nGOLDEN_PATH = os.getenv('GOLDEN_PATH', 'golden.json')\nUSE_MULTI = os.getenv('EVAL_MULTI','1') == '1'\nFINAL_K = int(os.getenv('EVAL_FINAL_K','5'))\n\n\"\"\"\nGolden file format (golden.json):\n[\n  {\"q\": \"Where is ProviderSetupWizard rendered?\", \"repo\": \"project\", \"expect_paths\": [\"core/admin_ui/src/components/ProviderSetupWizard.tsx\"]},\n  {\"q\": \"Where do we mask PHI in events?\", \"repo\": \"project\", \"expect_paths\": [\"app/...\"]}\n]\n\"\"\"\nhit(paths: List[str], expect: List[str]) -> bool:\n    return any(any(exp in p for p in paths) for exp in expect)"}
{"id":54,"text":"main():\n    if not os.path.exists(GOLDEN_PATH):\n        print('No golden file found at', GOLDEN_PATH)\n        return\n    gold = json.load(open(GOLDEN_PATH))\n    total = len(gold); hits_top1 = 0; hits_topk = 0\n    t0 = time.time()\n    for i, row in enumerate(gold, 1):\n        q = row['q']; repo = row.get('repo') or os.getenv('REPO','project')\n        expect = row.get('expect_paths') or []\n        if USE_MULTI:\n            docs = search_routed_multi(q, repo_override=repo, m=4, final_k=FINAL_K)\n        else:\n            docs = search_routed(q, repo_override=repo, final_k=FINAL_K)\n        paths = [d.get('file_path','') for d in docs]\n        if paths:\n            if hit(paths[:1], expect): hits_top1 += 1\n            if hit(paths, expect): hits_topk += 1\n        print(f\"[{i}/{total}] repo={repo} q={q}\\n  top1={paths[:1]}\\n  top{FINAL_K} hit={hit(paths, expect)}\")\n    dt = time.time() - t0\n    print(json.dumps({\n        'total': total,\n        'top1': hits_top1,\n        'topk': hits_topk,\n        'final_k': FINAL_K,\n        'use_multi': USE_MULTI,\n        'secs': round(dt,2)\n    }, indent=2))\n\nif __name__ == '__main__':\n    main()"}
{"id":55,"text":"import os, re, hashlib\nfrom typing import Dict, List, Optional\n\n# Optional import: tree_sitter_languages may be unavailable on newer Python versions.\ntry:\n    from tree_sitter_languages import get_parser as _ts_get_parser\nexcept Exception:\n    _ts_get_parser = None\n\nLANG_MAP = {\n    \".py\": \"python\", \".js\": \"javascript\", \".jsx\": \"javascript\",\n    \".ts\": \"typescript\", \".tsx\": \"typescript\",\n    \".go\": \"go\", \".java\": \"java\", \".rs\": \"rust\",\n    \".c\": \"c\", \".h\": \"c\", \".cpp\": \"cpp\", \".cc\": \"cpp\", \".hpp\": \"cpp\",\n}\n\n# Optional overlap to provide additional local context when chunk boundaries split logical units\nOVERLAP_LINES = 20\n\nFUNC_NODES = {\n    \"python\": {\"function_definition\", \"class_definition\"},\n    \"javascript\": {\"function_declaration\", \"class_declaration\", \"method_definition\", \"arrow_function\"},\n    \"typescript\": {\"function_declaration\", \"class_declaration\", \"method_signature\", \"method_definition\", \"arrow_function\"},\n    \"go\": {\"function_declaration\", \"method_declaration\"},\n    \"java\": {\"class_declaration\", \"method_declaration\"},\n    \"rust\": {\"function_item\", \"impl_item\"},\n    \"c\": {\"function_definition\"},\n    \"cpp\": {\"function_definition\", \"class_specifier\"},\n}\n\nIMPORT_NODES = {\n    \"python\": {\"import_statement\", \"import_from_statement\"},\n    \"javascript\": {\"import_declaration\"},\n    \"typescript\": {\"import_declaration\"},\n    \"go\": {\"import_declaration\"},\n    \"java\": {\"import_declaration\"},\n    \"rust\": {\"use_declaration\"},\n    \"c\": set(), \"cpp\": set(),\n}"}
{"id":56,"text":"lang_from_path(path:str)->Optional[str]:\n    _, ext = os.path.splitext(path)\n    return LANG_MAP.get(ext.lower())\nnonws_len(s:str)->int:\n    return len(re.sub(r\"\\s+\", \"\", s))\nextract_imports(src:str, lang:str)->List[str]:\n    try:\n        if _ts_get_parser is None:\n            raise RuntimeError(\"tree_sitter_languages not available\")\n        parser = _ts_get_parser(lang)\n        tree = parser.parse(bytes(src, \"utf-8\"))\n        imports = []\n        def walk(n):\n            if n.type in IMPORT_NODES.get(lang, set()):\n                imports.append(src[n.start_byte:n.end_byte])\n            for c in n.children:\n                walk(c)\n        walk(tree.root_node)\n        return imports\n    except Exception:\n        if lang == \"python\":\n            return re.findall(r\"^(?:from\\s+[^\\n]+|import\\s+[^\\n]+)$\", src, flags=re.M)\n        if lang in {\"javascript\",\"typescript\"}:\n            return re.findall(r\"^import\\s+[^\\n]+;$\", src, flags=re.M)\n        return []"}
{"id":57,"text":"greedy_fallback(src:str, fpath:str, lang:str, target:int)->List[Dict]:\n    sep = r\"(?:\\nclass\\s+|\\ndef\\s+)\" if lang==\"python\" else r\"(?:\\nclass\\s+|\\nfunction\\s+)\"\n    parts = re.split(sep, src)\n    if len(parts) < 2:\n        out, cur, acc = [], [], 0\n        for line in src.splitlines(True):\n            cur.append(line); acc += nonws_len(line)\n            if acc >= target:\n                out.append(\"\".join(cur)); cur, acc = [], 0\n        if cur: out.append(\"\".join(cur))\n        return [{\n            \"id\": hashlib.md5((fpath+str(i)+s[:80]).encode()).hexdigest()[:12],\n            \"file_path\": fpath, \"language\": lang, \"type\":\"blob\",\"name\":None,\n            \"start_line\": 1, \"end_line\": s.count(\"\\n\")+1, \"imports\": extract_imports(src, lang), \"code\": s\n        } for i,s in enumerate(out)]\n    else:\n        rejoined, buf, acc = [], [], 0\n        for p in parts:\n            if acc + nonws_len(p) > target and buf:\n                s = \"\".join(buf); rejoined.append(s); buf, acc = [], 0\n            buf.append(p); acc += nonws_len(p)\n        if buf: rejoined.append(\"\".join(buf))\n        return [{\n            \"id\": hashlib.md5((fpath+str(i)+s[:80]).encode()).hexdigest()[:12],\n            \"file_path\": fpath, \"language\": lang, \"type\":\"section\",\"name\":None,\n            \"start_line\": 1, \"end_line\": s.count(\"\\n\")+1, \"imports\": extract_imports(s, lang), \"code\": s\n        } for i,s in enumerate(rejoined)]"}
{"id":58,"text":"collect_files(roots:List[str])->List[str]:\n    import fnmatch\n    out=[]\n    # Hardcoded skip dirs for safety\n    skip_dirs = {\".git\",\"node_modules\",\".venv\",\"venv\",\"dist\",\"build\",\"__pycache__\",\".next\",\".turbo\",\".parcel-cache\",\".pytest_cache\",\"vendor\",\"third_party\",\".bundle\",\"Pods\"}\n\n    # Try to load exclusion patterns from file\n    exclude_patterns = []\n    for root in roots:\n        # Check for exclude_globs.txt in data/ directory\n        parent_dir = os.path.dirname(root) if os.path.isfile(root) else root\n        exclude_file = os.path.join(parent_dir, 'data', 'exclude_globs.txt')\n        if os.path.exists(exclude_file):\n            try:\n                with open(exclude_file, 'r') as f:\n                    patterns = [line.strip() for line in f if line.strip() and not line.startswith('#')]\n                    exclude_patterns.extend(patterns)\n            except Exception:\n                pass\n\n    for root in roots:\n        for dp, dns, fns in os.walk(root):\n            # Skip directories that match our exclusion rules\n            dns[:] = [d for d in dns if d not in skip_dirs and not d.startswith('.venv') and not d.startswith('venv')]\n\n            for fn in fns:\n                p = os.path.join(dp, fn)\n\n                # Check if file matches any exclusion pattern\n                skip = False\n                for pattern in exclude_patterns:\n                    if fnmatch.fnmatch(p, pattern) or fnmatch.fnmatch(os.path.relpath(p, root), pattern):\n                        skip = True\n                        break\n\n                if not skip and lang_from_path(p):\n                    out.append(p)\n    return out"}
{"id":59,"text":"_guess_name(lang:str, text:str)->Optional[str]:\n    if lang==\"python\":\n        m = re.search(r\"^(?:def|class)\\s+([A-Za-z_][A-Za-z0-9_]*)\", text, flags=re.M)\n        return m.group(1) if m else None\n    if lang in {\"javascript\",\"typescript\"}:\n        m = re.search(r\"^(?:function|class)\\s+([A-Za-z_$][A-Za-z0-9_$]*)\", text, flags=re.M)\n        return m.group(1) if m else None\n    return None"}
{"id":60,"text":"chunk_code(src:str, fpath:str, lang:str, target:int=900)->List[Dict]:\n    \"\"\"AST-aware chunking around functions/classes; falls back if no nodes.\"\"\"\n    try:\n        if _ts_get_parser is None:\n            raise RuntimeError(\"tree_sitter_languages not available\")\n        parser = _ts_get_parser(lang)\n        tree = parser.parse(bytes(src, \"utf-8\"))\n        wanted = FUNC_NODES.get(lang, set())\n        nodes = []\n        stack = [tree.root_node]\n        while stack:\n            n = stack.pop()\n            if n.type in wanted:\n                nodes.append(n)\n            stack.extend(n.children)\n        if not nodes:\n            return greedy_fallback(src, fpath, lang, target)\n        chunks: List[Dict] = []\n        all_lines = src.splitlines()\n        for i, n in enumerate(nodes):\n            text = src[n.start_byte:n.end_byte]\n            if nonws_len(text) > target:\n                for j, sub in enumerate(greedy_fallback(text, fpath, lang, target)):\n                    sub[\"id\"] = hashlib.md5((fpath+f\"/{i}:{j}\"+sub[\"code\"][:80]).encode()).hexdigest()[:12]\n                    sub[\"start_line\"] = n.start_point[0]+1\n                    sub[\"end_line\"] = sub[\"start_line\"] + sub[\"code\"].count(\"\\n\")\n                    chunks.append(sub)\n            else:\n                name = _guess_name(lang, text)\n                start_line = n.start_point[0] + 1\n                end_line = n.end_point[0] + 1\n                actual_start = max(1, start_line - OVERLAP_LINES) if OVERLAP_LINES > 0 else start_line\n                # Slice lines with 1-based indexing\n                chunk_text = \"\\n\".join(all_lines[actual_start-1:end_line])\n                chunks.append({\n                    \"id\": hashlib.md5((fpath+str(i)+text[:80]).encode()).hexdigest()[:12],\n                    \"file_path\": fpath,\n                    \"language\": lang,\n                    \"type\": \"unit\",\n                    \"name\": name,\n                    \"start_line\": actual_start,\n                    \"end_line\": end_line,\n                    \"imports\": extract_imports(src, lang),\n                    \"code\": chunk_text,\n                })\n        return chunks\n    except Exception:\n        return greedy_fallback(src, fpath, lang, target)"}
{"id":61,"text":"import os\nimport json\n\n# MLX backend (default for Apple Silicon)\nENRICH_BACKEND = os.getenv(\"ENRICH_BACKEND\", \"mlx\").lower()  # mlx | ollama\nMLX_MODEL = os.getenv(\"ENRICH_MODEL\", \"mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit\")\nOLLAMA_URL = os.getenv(\"OLLAMA_URL\", \"http://127.0.0.1:11434/api/generate\")\nOLLAMA_MODEL = os.getenv(\"ENRICH_MODEL_OLLAMA\", \"qwen3-coder:30b\")\n\nSYSTEM = (\n    \"You are a senior code analyst. Extract: 1) concise summary of purpose, \"\n    \"2) key APIs/classes/functions referenced, 3) inputs/outputs/side-effects, \"\n    \"4) 8-15 retrieval keywords (snake_case). Keep under 120 tokens.\"\n)\n\nPROMPT_TMPL = (\n    \"<system>\" + SYSTEM + \"</system>\\n\"\n    \"<analyze file='{file}' lang='{lang}'>\\n{code}\\n</analyze>\\n\"\n    \"<format>JSON with keys: summary, keywords</format>\"\n)\n\n# Lazy-load MLX model (only once)\n_mlx_model = None\n_mlx_tokenizer = None"}
{"id":62,"text":"_get_mlx_model():\n    global _mlx_model, _mlx_tokenizer\n    if _mlx_model is None:\n        from mlx_lm import load\n        _mlx_model, _mlx_tokenizer = load(MLX_MODEL)\n    return _mlx_model, _mlx_tokenizer"}
{"id":63,"text":"enrich(file_path: str, lang: str, code: str) -> dict:\n    prompt = PROMPT_TMPL.format(file=file_path, lang=lang, code=(code or '')[:4000])\n\n    if ENRICH_BACKEND == \"mlx\":\n        # MLX backend (Apple Silicon, fast)\n        try:\n            from mlx_lm import generate\n            model, tokenizer = _get_mlx_model()\n            txt = generate(model, tokenizer, prompt=prompt, max_tokens=150, verbose=False)\n        except Exception as e:\n            return {\"summary\": f\"MLX error: {str(e)[:100]}\", \"keywords\": []}\n    else:\n        # Ollama backend (fallback)\n        import requests\n        try:\n            resp = requests.post(\n                OLLAMA_URL,\n                json={\n                    \"model\": OLLAMA_MODEL,\n                    \"prompt\": prompt,\n                    \"stream\": False,\n                    \"options\": {\"temperature\": 0.1, \"num_ctx\": 4096},\n                },\n                timeout=10,\n            )\n            resp.raise_for_status()\n            txt = resp.json().get(\"response\", \"{}\")\n        except Exception as e:\n            return {\"summary\": f\"Ollama error: {str(e)[:100]}\", \"keywords\": []}\n\n    # Parse JSON response\n    try:\n        data = json.loads(txt)\n        if isinstance(data, dict):\n            return {\"summary\": data.get(\"summary\", \"\"), \"keywords\": data.get(\"keywords\", [])}\n    except Exception:\n        pass\n    return {\"summary\": txt[:300], \"keywords\": []}"}
{"id":64,"text":"import os\nfrom dotenv import load_dotenv\n\nload_dotenv('env/project.env')\nfrom serve_rag import app\n\nif __name__ == '__main__':\n    import uvicorn\n    os.environ['COLLECTION_NAME'] = os.environ.get('COLLECTION_NAME', f\"{os.environ['REPO']}_{os.environ.get('COLLECTION_SUFFIX','default')}\")\n    port = int(os.environ.get('PORT', '8012'))\n    uvicorn.run(app, host='127.0.0.1', port=port)"}
{"id":65,"text":"#!/usr/bin/env python3\n\"\"\"\nInteractive CLI chat interface for RAG service.\nUses LangGraph with Redis checkpoints for conversation memory.\n\nUsage:\n    export REPO=project\n    export THREAD_ID=my-session-1\n    python chat_cli.py\n\nCommands:\n    /repo <name>    - Switch repository (project or project)\n    /save           - Save conversation checkpoint\n    /clear          - Clear conversation history\n    /help           - Show commands\n    /exit, /quit    - Exit chat\n\"\"\"\nimport os\nimport sys\nfrom pathlib import Path\ntry:\n    from dotenv import load_dotenv\nexcept Exception:\n    # Graceful fallback if python-dotenv is not installed yet\n    def load_dotenv(*args, **kwargs):\n        return False\n\n# Load environment\nload_dotenv(Path(__file__).parent / \".env\")\n\nfrom langgraph_app import build_graph\nfrom rich.console import Console\nfrom rich.markdown import Markdown\nfrom rich.panel import Panel\nfrom rich.prompt import Prompt\n\nconsole = Console()\n\n# Configuration\nREPO = os.getenv('REPO', 'project')\nTHREAD_ID = os.getenv('THREAD_ID', 'cli-chat')"}
{"id":66,"text":"ChatCLI:\n    \"\"\"Interactive CLI chat with RAG.\"\"\"\n\n    def __init__(self, repo: str = 'project', thread_id: str = 'cli-chat'):\n        self.repo = repo\n        self.thread_id = thread_id\n        self.graph = None\n        self._init_graph()\n\n    def _init_graph(self):\n        \"\"\"Initialize LangGraph with Redis checkpoints.\"\"\"\n        try:\n            self.graph = build_graph()\n            console.print(f\"[green]âœ“[/green] Graph initialized with Redis checkpoints\")\n        except Exception as e:\n            console.print(f\"[red]âœ—[/red] Failed to initialize graph: {e}\")\n            sys.exit(1)\n\n    def _get_config(self):\n        \"\"\"Get config for current thread.\"\"\"\n        return {\"configurable\": {\"thread_id\": self.thread_id}}\n\n    def _format_answer(self, generation: str) -> str:\n        \"\"\"Format answer, removing repo header if present.\"\"\"\n        lines = generation.split('\\n')\n        # Remove [repo: ...] header if present\n        if lines and lines[0].startswith('[repo:'):\n            return '\\n'.join(lines[1:]).strip()\n        return generation\n\n    def ask(self, question: str) -> dict:\n        \"\"\"Ask a question and get answer.\"\"\"\n        try:\n            state = {\n                \"question\": question,\n                \"documents\": [],\n                \"generation\": \"\",\n                \"iteration\": 0,\n                \"confidence\": 0.0,\n                \"repo\": self.repo\n            }\n\n            result = self.graph.invoke(state, self._get_config())\n            return result\n        except Exception as e:\n            console.print(f\"[red]Error:[/red] {e}\")\n            return {\"generation\": f\"Error: {e}\", \"documents\": [], \"confidence\": 0.0}\n\n    def switch_repo(self, new_repo: str):\n        \"\"\"Switch to a different repository.\"\"\"\n        if new_repo not in ['project', 'project']:\n            console.print(f\"[red]âœ—[/red] Invalid repo. Use 'project' or 'project'\")\n            return\n\n        self.repo = new_repo\n        console.print(f\"[green]âœ“[/green] Switched to repo: [bold]{new_repo}[/bold]\")\n\n    def show_help(self):\n        \"\"\"Show available commands.\"\"\"\n        help_text = \"\"\"\n## Commands\n\n- `/repo <name>` - Switch repository (project or project)\n- `/save` - Save conversation checkpoint\n- `/clear` - Clear conversation history\n- `/help` - Show this help\n- `/exit`, `/quit` - Exit chat\n\n## Examples\n\nAsk a question:\n```\n> Where is OAuth token validated?\n```\n\nSwitch repo:\n```\n> /repo project\n> How do we handle inbound faxes?\n```\n        \"\"\"\n        console.print(Markdown(help_text))\n\n    def show_welcome(self):\n        \"\"\"Show welcome message.\"\"\"\n        welcome = f\"\"\"\n# ðŸ¤– RAG CLI Chat\n\nConnected to: [bold cyan]{self.repo}[/bold cyan]\nThread ID: [bold]{self.thread_id}[/bold]\n\nType your question or use `/help` for commands.\n        \"\"\"\n        console.print(Panel(Markdown(welcome), border_style=\"cyan\"))\n\n    def run(self):\n        \"\"\"Main chat loop.\"\"\"\n        self.show_welcome()\n\n        while True:\n            try:\n                # Get user input\n                user_input = Prompt.ask(\n                    f\"\\n[bold cyan]{self.repo}[/bold cyan] >\",\n                    default=\"\"\n                )\n\n                if not user_input.strip():\n                    continue\n\n                # Handle commands\n                if user_input.startswith('/'):\n                    cmd = user_input.lower().split()[0]\n\n                    if cmd in ['/exit', '/quit']:\n                        console.print(\"[yellow]Goodbye![/yellow]\")\n                        break\n\n                    elif cmd == '/help':\n                        self.show_help()\n                        continue\n\n                    elif cmd == '/repo':\n                        parts = user_input.split(maxsplit=1)\n                        if len(parts) > 1:\n                            self.switch_repo(parts[1].strip())\n                        else:\n                            console.print(\"[red]Usage:[/red] /repo <project|project>\")\n                        continue\n\n                    elif cmd == '/save':\n                        console.print(f\"[green]âœ“[/green] Checkpoint saved (thread: {self.thread_id})\")\n                        continue\n\n                    elif cmd == '/clear':\n                        # Create new thread ID to start fresh\n                        import time\n                        self.thread_id = f\"cli-chat-{int(time.time())}\"\n                        console.print(f\"[green]âœ“[/green] Cleared history (new thread: {self.thread_id})\")\n                        continue\n\n                    else:\n                        console.print(f\"[red]Unknown command:[/red] {cmd}\")\n                        console.print(\"Type [bold]/help[/bold] for available commands\")\n                        continue\n\n                # Ask question\n                console.print(\"[dim]Thinking...[/dim]\")\n                result = self.ask(user_input)\n\n                # Show answer\n                answer = self._format_answer(result.get('generation', ''))\n                confidence = result.get('confidence', 0.0)\n                docs = result.get('documents', [])\n\n                # Display answer in panel\n                console.print(\"\\n\")\n                console.print(Panel(\n                    Markdown(answer),\n                    title=f\"Answer (confidence: {confidence:.2f})\",\n                    border_style=\"green\" if confidence > 0.6 else \"yellow\"\n                ))\n\n                # Show top citations\n                if docs:\n                    console.print(\"\\n[dim]Top sources:[/dim]\")\n                    for i, doc in enumerate(docs[:3], 1):\n                        fp = doc.get('file_path', 'unknown')\n                        start = doc.get('start_line', 0)\n                        end = doc.get('end_line', 0)\n                        score = doc.get('rerank_score', 0.0)\n                        console.print(f\"  [dim]{i}.[/dim] {fp}:{start}-{end} [dim](score: {score:.3f})[/dim]\")\n\n            except KeyboardInterrupt:\n                console.print(\"\\n[yellow]Use /exit to quit[/yellow]\")\n                continue\n            except EOFError:\n                console.print(\"\\n[yellow]Goodbye![/yellow]\")\n                break\n            except Exception as e:\n                console.print(f\"[red]Error:[/red] {e}\")\n                continue"}
{"id":67,"text":"main():\n    \"\"\"Entry point.\"\"\"\n    # Check dependencies\n    try:\n        from rich.console import Console\n        from rich.markdown import Markdown\n        from rich.panel import Panel\n        from rich.prompt import Prompt\n    except ImportError:\n        print(\"Error: Missing 'rich' library. Install with: pip install rich\")\n        sys.exit(1)\n\n    # Get config from environment\n    repo = os.getenv('REPO', 'project')\n    thread_id = os.getenv('THREAD_ID', 'cli-chat')\n\n    # Create and run chat\n    chat = ChatCLI(repo=repo, thread_id=thread_id)\n    chat.run()\n\n\nif __name__ == '__main__':\n    main()"}
{"id":68,"text":"# coding: utf-8\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http.exceptions import UnexpectedResponse\n\n_orig_recreate = QdrantClient.recreate_collection\n\n_extract_args(*args, **kwargs):\n    name = kwargs.get(\"collection_name\")\n    vectors_config = kwargs.get(\"vectors_config\")\n    if name is None and args:\n        name = args[0]\n    if vectors_config is None and len(args) > 1:\n        vectors_config = args[1]\n    return name, vectors_config"}
{"id":69,"text":"recreate_collection_safe(self, *args, **kwargs):\n    try:\n        return _orig_recreate(self, *args, **kwargs)\n    except UnexpectedResponse as e:\n        # Some servers return 404 on delete step inside recreate\n        if getattr(e, \"status_code\", None) == 404:\n            name, vectors_config = _extract_args(*args, **kwargs)\n            return self.create_collection(collection_name=name, vectors_config=vectors_config)\n        raise\n    except Exception:\n        # Very defensive fallback: try delete (ignore errors), then create\n        name, vectors_config = _extract_args(*args, **kwargs)\n        try:\n            try:\n                self.delete_collection(name)\n            except Exception:\n                pass\n            return self.create_collection(collection_name=name, vectors_config=vectors_config)\n        except Exception:\n            raise\n\n\nQdrantClient.recreate_collection = recreate_collection_safe"}
{"id":70,"text":"import os\nimport json\nimport hashlib\nfrom typing import List, Dict\nfrom pathlib import Path\nfrom dotenv import load_dotenv, find_dotenv\nfrom config_loader import get_repo_paths, out_dir\nfrom ast_chunker import lang_from_path, collect_files, chunk_code\nimport bm25s\nfrom bm25s.tokenization import Tokenizer\nfrom Stemmer import Stemmer\nfrom qdrant_client import QdrantClient, models\nimport uuid\nfrom openai import OpenAI\nfrom embed_cache import EmbeddingCache\nimport tiktoken\nfrom sentence_transformers import SentenceTransformer\nimport fnmatch, pathlib\nimport qdrant_recreate_fallback  # make recreate_collection 404-safe\n\n# --- global safe filters (avoid indexing junk) ---\nfrom filtering import _prune_dirs_in_place, _should_index_file, PRUNE_DIRS\nimport os\nfrom pathlib import Path\n\n# Patch os.walk to prune noisy dirs and skip junk file types\n_os_walk = os.walk"}
{"id":71,"text":"_filtered_os_walk(top, *args, **kwargs):\n    for root, dirs, files in _os_walk(top, *args, **kwargs):\n        _prune_dirs_in_place(dirs)\n        files[:] = [f for f in files if _should_index_file(f)]\n        yield root, dirs, files\nos.walk = _filtered_os_walk\n\n# Patch Path.rglob as well (if code uses it)\n_Path_rglob = Path.rglob"}
{"id":72,"text":"_filtered_rglob(self, pattern):\n    for p in _Path_rglob(self, pattern):\n        # skip if any pruned dir appears in the path\n        if any(part in PRUNE_DIRS for part in p.parts):\n            continue\n        if not _should_index_file(p.name):\n            continue\n        yield p\nPath.rglob = _filtered_rglob\n# --- end filters ---\n\n\n# Load local env and also repo-root .env if present (no hard-coded paths)\ntry:\n    load_dotenv(override=False)\n    repo_root = Path(__file__).resolve().parent\n    env_path = repo_root / \".env\"\n    if env_path.exists():\n        load_dotenv(dotenv_path=env_path, override=False)\n    else:\n        alt = find_dotenv(usecwd=True)\n        if alt:\n            load_dotenv(dotenv_path=alt, override=False)\nexcept Exception:\n    pass\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\nif OPENAI_API_KEY and OPENAI_API_KEY.strip().upper() in {\"SK-REPLACE\", \"REPLACE\"}:\n    OPENAI_API_KEY = None\nQDRANT_URL = os.getenv('QDRANT_URL','http://127.0.0.1:6333')\n# Repo scoping\nREPO = os.getenv('REPO', 'project').strip()\n# Resolve repo paths and outdir from config (repos.json or env)\ntry:\n    BASES = get_repo_paths(REPO)\nexcept Exception:\n    # Fallback to current directory when no config present (best-effort)\n    BASES = [str(Path(__file__).resolve().parent)]\nOUTDIR = out_dir(REPO)\n# Allow explicit collection override (for versioned collections per embedding config)\nCOLLECTION = os.getenv('COLLECTION_NAME', f'code_chunks_{REPO}')\n\n\n# Centralized file indexing gate (extensions, excludes, heuristics)\nSOURCE_EXTS = {\n    \".py\", \".rb\", \".ts\", \".tsx\", \".js\", \".jsx\", \".go\", \".rs\", \".java\",\n    \".cs\", \".c\", \".h\", \".cpp\", \".hpp\", \".m\", \".mm\", \".kt\", \".kts\", \".swift\",\n    \".sql\", \".yml\", \".yaml\", \".toml\", \".ini\", \".json\", \".md\"\n}\nEXCLUDE_GLOBS_FILE = \"data/exclude_globs.txt\""}
{"id":73,"text":"_load_exclude_globs() -> list[str]:\n    p = pathlib.Path(EXCLUDE_GLOBS_FILE)\n    if not p.exists():\n        return []\n    return [ln.strip() for ln in p.read_text().splitlines() if ln.strip() and not ln.startswith(\"#\")]\n\n_EXCLUDE_GLOBS = _load_exclude_globs()\nshould_index_file(path: str) -> bool:\n    p = pathlib.Path(path)\n    # 1) fast deny: extension must look like source\n    if p.suffix.lower() not in SOURCE_EXTS:\n        return False\n    # 2) glob excludes (vendor, caches, images, minified, etc.)\n    as_posix = p.as_posix()\n    for pat in _EXCLUDE_GLOBS:\n        if fnmatch.fnmatch(as_posix, pat):\n            return False\n    # 3) quick heuristic to skip huge/minified one-liners\n    try:\n        text = p.read_text(errors=\"ignore\")\n        if len(text) > 2_000_000:  # ~2MB\n            return False\n        # suspect minified if average line length is enormous\n        lines = text.splitlines()\n        if lines:\n            avg = sum(len(x) for x in lines) / max(1, len(lines))\n            if avg > 2500:\n                return False\n    except Exception:\n        return False\n    return True\n\n\n# --- Repo-aware layer tagging ---"}
{"id":74,"text":"detect_layer(fp: str) -> str:\n    f = (fp or '').lower()\n    if REPO == 'project':\n        if '/core/admin_ui/' in f or '/site/' in f or '/docs-site/' in f:\n            return 'ui'\n        if '/plugins/' in f or '/core/plugins/' in f or 'notification' in f or 'pushover' in f or 'apprise' in f:\n            return 'plugin'\n        if '/core/api/' in f or '/core/' in f or '/server' in f:\n            return 'kernel'\n        if '/docs/' in f or '/internal_docs/' in f:\n            return 'docs'\n        if '/tests/' in f or '/test_' in f:\n            return 'tests'\n        if '/infra/' in f or '/deploy/' in f or '/scripts/' in f:\n            return 'infra'\n        return 'kernel'\n    else:\n        if '/admin_ui/' in f or '/site/' in f or '/docs-site/' in f:\n            return 'ui'\n        if 'provider' in f or 'providers' in f or 'integration' in f or 'webhook' in f or 'adapter' in f:\n            return 'integration'\n        if '/api/' in f or '/backends/' in f or '/server' in f:\n            return 'server'\n        if '/sdks/' in f or '/python_mcp/' in f or '/node_mcp/' in f or '/plugin-dev-kit/' in f:\n            return 'sdk'\n        if '/docs/' in f or '/internal_docs/' in f:\n            return 'docs'\n        if '/asterisk/' in f or '/config/' in f or '/infra/' in f:\n            return 'infra'\n        return 'server'\n\nVENDOR_MARKERS = (\n    \"/vendor/\",\"/third_party/\",\"/external/\",\"/deps/\",\"/node_modules/\",\n    \"/Pods/\",\"/Godeps/\",\"/.bundle/\",\"/bundle/\"\n)"}
{"id":75,"text":"detect_origin(fp: str) -> str:\n    low = (fp or '').lower()\n    for m in VENDOR_MARKERS:\n        if m in low:\n            return 'vendor'\n    try:\n        with open(fp, 'r', encoding='utf-8', errors='ignore') as f:\n            head = ''.join([next(f) for _ in range(12)])\n        if any(k in head.lower() for k in (\n            'apache license','mit license','bsd license','mozilla public license'\n        )):\n            return 'vendor'\n    except Exception:\n        pass\n    return 'first_party'\nos.makedirs(OUTDIR, exist_ok=True)\n_clip_for_openai(text: str, enc, max_tokens: int = 8000) -> str:\n    toks = enc.encode(text)\n    if len(toks) <= max_tokens:\n        return text\n    return enc.decode(toks[:max_tokens])\nembed_texts(client: OpenAI, texts: List[str], batch: int = 64) -> List[List[float]]:\n    # Legacy non-cached embedder (kept for compatibility if needed)\n    embs = []\n    enc = tiktoken.get_encoding('cl100k_base')\n    for i in range(0, len(texts), batch):\n        sub = [_clip_for_openai(t, enc) for t in texts[i:i+batch]]\n        r = client.embeddings.create(model='text-embedding-3-large', input=sub)\n        for d in r.data:\n            embs.append(d.embedding)\n    return embs"}
{"id":76,"text":"embed_texts_local(texts: List[str], model_name: str = 'BAAI/bge-small-en-v1.5', batch: int = 128) -> List[List[float]]:\n    model = SentenceTransformer(model_name)\n    out = []\n    for i in range(0, len(texts), batch):\n        sub = texts[i:i+batch]\n        v = model.encode(sub, normalize_embeddings=True, show_progress_bar=False)\n        out.extend(v.tolist())\n    return out\n_renorm_truncate(vecs: List[List[float]], dim: int) -> List[List[float]]:\n    out: List[List[float]] = []\n    import math as _m\n    for v in vecs:\n        w = v[:dim] if dim and dim < len(v) else v\n        # renormalize\n        n = _m.sqrt(sum(x*x for x in w)) or 1.0\n        out.append([x / n for x in w])\n    return out\nembed_texts_mxbai(texts: List[str], dim: int = 512, batch: int = 128) -> List[List[float]]:\n    model = SentenceTransformer('mixedbread-ai/mxbai-embed-large-v1')\n    out: List[List[float]] = []\n    for i in range(0, len(texts), batch):\n        sub = texts[i:i+batch]\n        v = model.encode(sub, normalize_embeddings=True, show_progress_bar=False)\n        out.extend(v.tolist())\n    return _renorm_truncate(out, dim)"}
{"id":77,"text":"embed_texts_voyage(texts: List[str], batch: int = 128, output_dimension: int = 512) -> List[List[float]]:\n    import voyageai\n    client = voyageai.Client(api_key=os.getenv('VOYAGE_API_KEY'))\n    out: List[List[float]] = []\n    for i in range(0, len(texts), batch):\n        sub = texts[i:i+batch]\n        r = client.embed(sub, model='voyage-code-3', input_type='document', output_dimension=output_dimension)\n        out.extend(r.embeddings)\n    return out"}
{"id":78,"text":"main() -> None:\n    files = collect_files(BASES)\n    print(f'Discovered {len(files)} source files.')\n    all_chunks: List[Dict] = []\n    for fp in files:\n        if not should_index_file(fp):\n            continue\n        lang = lang_from_path(fp)\n        if not lang:\n            continue\n        try:\n            with open(fp, 'r', encoding='utf-8', errors='ignore') as f:\n                src = f.read()\n        except Exception:\n            continue\n        ch = chunk_code(src, fp, lang, target=900)\n        all_chunks.extend(ch)\n\n    seen, chunks = set(), []\n    for c in all_chunks:\n        c['repo'] = REPO\n        try:\n            c['layer'] = detect_layer(c.get('file_path',''))\n        except Exception:\n            c['layer'] = 'server'\n        try:\n            c['origin'] = detect_origin(c.get('file_path',''))\n        except Exception:\n            c['origin'] = 'first_party'\n        h = hashlib.md5(c['code'].encode()).hexdigest()\n        if h in seen:\n            continue\n        seen.add(h)\n        c['hash'] = h\n        chunks.append(c)\n    print(f'Prepared {len(chunks)} chunks.')\n\n    # Optional enrichment using a local code LLM via Ollama\n    ENRICH = (os.getenv('ENRICH_CODE_CHUNKS', 'false') or 'false').lower() == 'true'\n    if ENRICH:\n        try:\n            from metadata_enricher import enrich  # type: ignore\n        except Exception:\n            enrich = None\n        if enrich is not None:\n            for c in chunks:\n                try:\n                    meta = enrich(c.get('file_path',''), c.get('language',''), c.get('code',''))\n                    c['summary'] = meta.get('summary','')\n                    c['keywords'] = meta.get('keywords', [])\n                except Exception:\n                    c['summary'] = ''\n                    c['keywords'] = []\n\n    # BM25S index\n    corpus: List[str] = []\n    for c in chunks:\n        pre = []\n        if c.get('name'):\n            pre += [c['name']]*2\n        if c.get('imports'):\n            pre += [i[0] or i[1] for i in c['imports'] if isinstance(i, (list, tuple))]\n        body = c['code']\n        corpus.append((' '.join(pre)+'\\n'+body).strip())\n\n    stemmer = Stemmer('english')\n    tokenizer = Tokenizer(stemmer=stemmer, stopwords='en')\n    corpus_tokens = tokenizer.tokenize(corpus)\n    retriever = bm25s.BM25(method='lucene', k1=1.2, b=0.65)\n    retriever.index(corpus_tokens)\n    os.makedirs(os.path.join(OUTDIR, 'bm25_index'), exist_ok=True)\n    # Workaround: ensure JSON-serializable vocab keys\n    try:\n        retriever.vocab_dict = {str(k): v for k, v in retriever.vocab_dict.items()}\n    except Exception:\n        pass\n    retriever.save(os.path.join(OUTDIR, 'bm25_index'), corpus=corpus)\n    tokenizer.save_vocab(save_dir=os.path.join(OUTDIR, 'bm25_index'))\n    tokenizer.save_stopwords(save_dir=os.path.join(OUTDIR, 'bm25_index'))\n    with open(os.path.join(OUTDIR, 'bm25_index', 'corpus.txt'), 'w', encoding='utf-8') as f:\n        for doc in corpus:\n            f.write(doc.replace('\\n','\\\\n')+'\\n')\n    # Persist a stable mapping from BM25 doc index -> chunk id\n    # Persist mapping from BM25 doc index -> chunk id (string)\n    chunk_ids = [str(c['id']) for c in chunks]\n    with open(os.path.join(OUTDIR, 'bm25_index', 'chunk_ids.txt'), 'w', encoding='utf-8') as f:\n        for cid in chunk_ids:\n            f.write(cid+'\\n')\n    # Also write a JSON map for convenience\n    import json as _json\n    _json.dump({str(i): cid for i, cid in enumerate(chunk_ids)}, open(os.path.join(OUTDIR,'bm25_index','bm25_map.json'),'w'))\n    with open(os.path.join(OUTDIR,'chunks.jsonl'),'w',encoding='utf-8') as f:\n        for c in chunks:\n            f.write(json.dumps(c, ensure_ascii=False)+'\\n')\n    print('BM25 index saved.')\n\n    # Optionally skip dense embeddings/Qdrant for fast local-only BM25 indexing\n    if (os.getenv('SKIP_DENSE','0') or '0').strip() == '1':\n        print('Skipping dense embeddings and Qdrant upsert (SKIP_DENSE=1).')\n        return\n\n    client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None\n    # Choose text for embedding (optionally enriched)\n    texts = []\n    for c in chunks:\n        if c.get('summary') or c.get('keywords'):\n            kw = ' '.join(c.get('keywords', []))\n            texts.append(f\"{c.get('file_path','') }\\n{c.get('summary','')}\\n{kw}\\n{c.get('code','')}\")\n        else:\n            texts.append(c['code'])\n    embs: List[List[float]] = []\n    et = (os.getenv('EMBEDDING_TYPE','openai') or 'openai').lower()\n    if et == 'voyage':\n        try:\n            embs = embed_texts_voyage(texts, batch=64, output_dimension=int(os.getenv('VOYAGE_EMBED_DIM','512')))\n        except Exception as e:\n            print(f\"Voyage embedding failed ({e}); falling back to local embeddings.\")\n            embs = []\n        if not embs:\n            embs = embed_texts_local(texts)\n    elif et == 'mxbai':\n        try:\n            dim = int(os.getenv('EMBEDDING_DIM', '512'))\n            embs = embed_texts_mxbai(texts, dim=dim)\n        except Exception as e:\n            print(f\"MXBAI embedding failed ({e}); falling back to local embeddings.\")\n            embs = embed_texts_local(texts)\n    elif et == 'local':\n        embs = embed_texts_local(texts)\n    else:\n        if client is not None:\n            try:\n                cache = EmbeddingCache(OUTDIR)\n                hashes = [c['hash'] for c in chunks]\n                embs = cache.embed_texts(client, texts, hashes, model='text-embedding-3-large', batch=64)\n                cache.save()\n            except Exception as e:\n                print(f'Embedding via OpenAI failed ({e}); falling back to local embeddings.')\n        if not embs:\n            embs = embed_texts_local(texts)\n    point_ids: List[str] = []\n    try:\n        q = QdrantClient(url=QDRANT_URL)\n        q.recreate_collection(\n            collection_name=COLLECTION,\n            vectors_config={'dense': models.VectorParams(size=len(embs[0]), distance=models.Distance.COSINE)}\n        )\n        points = []\n        for c, v in zip(chunks, embs):\n            # Derive a stable UUID from the chunk id string to satisfy Qdrant (expects int or UUID)\n            cid = str(c['id'])\n            pid = str(uuid.uuid5(uuid.NAMESPACE_DNS, cid))\n            # Create slim payload without code (code is stored locally in chunks.jsonl)\n            slim_payload = {\n                'id': c.get('id'),\n                'file_path': c.get('file_path'),\n                'start_line': c.get('start_line'),\n                'end_line': c.get('end_line'),\n                'layer': c.get('layer'),\n                'repo': c.get('repo'),\n                'origin': c.get('origin'),\n                'hash': c.get('hash'),\n                'language': c.get('language')\n            }\n            # Remove None values to keep payload minimal\n            slim_payload = {k: v for k, v in slim_payload.items() if v is not None}\n            points.append(models.PointStruct(id=pid, vector={'dense': v}, payload=slim_payload))\n            point_ids.append(pid)\n            if len(points) == 64:\n                q.upsert(COLLECTION, points=points)\n                points = []\n        if points:\n            q.upsert(COLLECTION, points=points)\n        # Persist point id mapping aligned to BM25 corpus order\n        import json as _json\n        _json.dump({str(i): pid for i, pid in enumerate(point_ids)}, open(os.path.join(OUTDIR,'bm25_index','bm25_point_ids.json'),'w'))\n        print(f'Indexed {len(chunks)} chunks to Qdrant (embeddings: {len(embs[0])} dims).')\n    except Exception as e:\n        # Allow offline usage (BM25-only search) when Qdrant is unavailable\n        print(f\"Qdrant unavailable or failed to index ({e}); continuing with BM25-only index. Dense retrieval will be disabled.\")\n\nif __name__ == '__main__':\n    main()"}
{"id":79,"text":"import os, json\nimport tiktoken"}
{"id":80,"text":"EmbeddingCache:\n    def __init__(self, outdir: str):\n        os.makedirs(outdir, exist_ok=True)\n        self.path = os.path.join(outdir, \"embed_cache.jsonl\")\n        self.cache = {}\n        if os.path.exists(self.path):\n            with open(self.path, \"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    try:\n                        o = json.loads(line)\n                        self.cache[o[\"hash\"]] = o[\"vec\"]\n                    except Exception:\n                        pass\n\n    def get(self, h: str):\n        return self.cache.get(h)\n\n    def put(self, h: str, v):\n        self.cache[h] = v\n\n    def save(self):\n        with open(self.path, \"w\", encoding=\"utf-8\") as f:\n            for h, v in self.cache.items():\n                f.write(json.dumps({\"hash\": h, \"vec\": v}) + \"\\n\")\n\n    def embed_texts(self, client, texts, hashes, model=\"text-embedding-3-large\", batch=64):\n        embs = [None] * len(texts)\n        to_embed, idx_map = [], []\n        for i, (t, h) in enumerate(zip(texts, hashes)):\n            v = self.get(h)\n            if v is None:\n                idx_map.append(i)\n                to_embed.append(t)\n            else:\n                embs[i] = v\n        enc = tiktoken.get_encoding('cl100k_base')\n        def _clip_for_openai(text: str, max_tokens: int = 8000) -> str:\n            toks = enc.encode(text)\n            if len(toks) <= max_tokens:\n                return text\n            return enc.decode(toks[:max_tokens])\n        for i in range(0, len(to_embed), batch):\n            sub = [_clip_for_openai(t) for t in to_embed[i:i+batch]]\n            r = client.embeddings.create(model=model, input=sub)\n            for j, d in enumerate(r.data):\n                orig = idx_map[i + j]\n                vec = d.embedding\n                embs[orig] = vec\n                self.put(hashes[orig], vec)\n        return embs"}
{"id":81,"text":"#!/usr/bin/env python3\n\"\"\"\nMCP server exposing RAG tools for Codex/Claude integration.\nImplements Model Context Protocol via stdio.\n\nTools (sanitized names for OpenAI tool spec):\n  - rag_answer(repo, question) â†’ full LangGraph answer + citations\n  - rag_search(repo, question) â†’ retrieval-only (for debugging)\nCompatibility: accepts legacy names \"rag.answer\" and \"rag.search\" on tools/call.\n\"\"\"\nimport sys\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, Any, List\n\n# Ensure we can import from the same directory\nsys.path.insert(0, str(Path(__file__).parent))\n\nfrom langgraph_app import build_graph\nfrom hybrid_search import search_routed_multi\nimport urllib.request, urllib.error, urllib.parse\nimport json as _json"}
{"id":82,"text":"MCPServer:\n    \"\"\"Minimal MCP server over stdio.\"\"\"\n\n    def __init__(self):\n        self.graph = None\n        self._init_graph()\n\n    def _init_graph(self):\n        \"\"\"Lazy-load the LangGraph.\"\"\"\n        try:\n            self.graph = build_graph()\n        except Exception as e:\n            self._error(f\"Failed to initialize graph: {e}\")\n\n    def _error(self, msg: str):\n        \"\"\"Write error to stderr (MCP uses stdout for protocol).\"\"\"\n        print(f\"ERROR: {msg}\", file=sys.stderr)\n\n    def _log(self, msg: str):\n        \"\"\"Write log to stderr.\"\"\"\n        print(f\"LOG: {msg}\", file=sys.stderr)\n\n    def handle_rag_answer(self, repo: str, question: str) -> Dict[str, Any]:\n        \"\"\"\n        Execute full LangGraph pipeline: retrieval â†’ generation â†’ answer.\n        Returns: {answer: str, citations: List[str], repo: str}\n        \"\"\"\n        if not self.graph:\n            self._init_graph()\n\n        if not self.graph:\n            return {\n                \"error\": \"Graph not initialized\",\n                \"answer\": \"\",\n                \"citations\": [],\n                \"repo\": repo or \"unknown\"\n            }\n\n        try:\n            cfg = {\"configurable\": {\"thread_id\": f\"mcp-{repo or 'default'}\"}}\n            state = {\n                \"question\": question,\n                \"documents\": [],\n                \"generation\": \"\",\n                \"iteration\": 0,\n                \"confidence\": 0.0,\n                \"repo\": repo\n            }\n\n            result = self.graph.invoke(state, cfg)\n\n            # Extract citations from documents\n            docs = result.get(\"documents\", [])[:5]\n            citations = [\n                f\"{d['file_path']}:{d['start_line']}-{d['end_line']}\"\n                for d in docs\n            ]\n\n            return {\n                \"answer\": result.get(\"generation\", \"\"),\n                \"citations\": citations,\n                \"repo\": result.get(\"repo\", repo or \"unknown\"),\n                \"confidence\": float(result.get(\"confidence\", 0.0))\n            }\n        except Exception as e:\n            self._error(f\"rag.answer error: {e}\")\n            return {\n                \"error\": str(e),\n                \"answer\": \"\",\n                \"citations\": [],\n                \"repo\": repo or \"unknown\"\n            }\n\n    def handle_rag_search(self, repo: str, question: str, top_k: int = 10) -> Dict[str, Any]:\n        \"\"\"\n        Retrieval-only path for debugging.\n        Returns: {results: List[Dict], repo: str, count: int}\n        \"\"\"\n        try:\n            docs = search_routed_multi(\n                question,\n                repo_override=repo,\n                m=4,\n                final_k=top_k\n            )\n\n            # Return slim results (no code bodies for MCP transport)\n            results = [\n                {\n                    \"file_path\": d.get(\"file_path\", \"\"),\n                    \"start_line\": d.get(\"start_line\", 0),\n                    \"end_line\": d.get(\"end_line\", 0),\n                    \"language\": d.get(\"language\", \"\"),\n                    \"rerank_score\": float(d.get(\"rerank_score\", 0.0)),\n                    \"repo\": d.get(\"repo\", repo or \"unknown\")\n                }\n                for d in docs\n            ]\n\n            return {\n                \"results\": results,\n                \"repo\": repo or (results[0][\"repo\"] if results else \"unknown\"),\n                \"count\": len(results)\n            }\n        except Exception as e:\n            self._error(f\"rag.search error: {e}\")\n            return {\n                \"error\": str(e),\n                \"results\": [],\n                \"repo\": repo or \"unknown\",\n                \"count\": 0\n            }\n\n    # --- Netlify helpers ---\n    def _netlify_api(self, path: str, method: str = \"GET\", data: dict | None = None) -> dict:\n        api_key = os.getenv(\"NETLIFY_API_KEY\")\n        if not api_key:\n            raise RuntimeError(\"NETLIFY_API_KEY not set in environment\")\n        url = f\"https://api.netlify.com/api/v1{path}\"\n        req = urllib.request.Request(url, method=method)\n        req.add_header(\"Authorization\", f\"Bearer {api_key}\")\n        req.add_header(\"Content-Type\", \"application/json\")\n        body = None\n        if data is not None:\n            body = _json.dumps(data).encode(\"utf-8\")\n        try:\n            with urllib.request.urlopen(req, data=body, timeout=30) as resp:\n                raw = resp.read().decode(\"utf-8\")\n                return _json.loads(raw) if raw else {}\n        except urllib.error.HTTPError as he:\n            err_body = he.read().decode(\"utf-8\", errors=\"ignore\")\n            raise RuntimeError(f\"Netlify HTTP {he.code}: {err_body}\")\n\n    def _netlify_find_site_by_domain(self, domain: str) -> dict | None:\n        sites = self._netlify_api(\"/sites\", method=\"GET\")\n        if isinstance(sites, list):\n            domain_low = (domain or \"\").strip().lower()\n            for s in sites:\n                for key in (\"custom_domain\", \"url\", \"ssl_url\"):\n                    val = (s.get(key) or \"\").lower()\n                    if val and domain_low in val:\n                        return s\n        return None\n\n    def handle_netlify_deploy(self, domain: str) -> Dict[str, Any]:\n        targets: list[str]\n        if domain == \"both\":\n            targets = [\"project.net\", \"project.dev\"]\n        else:\n            targets = [domain]\n        results = []\n        for d in targets:\n            site = self._netlify_find_site_by_domain(d)\n            if not site:\n                results.append({\"domain\": d, \"status\": \"not_found\"})\n                continue\n            site_id = site.get(\"id\")\n            if not site_id:\n                results.append({\"domain\": d, \"status\": \"no_site_id\"})\n                continue\n            try:\n                build = self._netlify_api(f\"/sites/{site_id}/builds\", method=\"POST\", data={})\n                results.append({\n                    \"domain\": d,\n                    \"status\": \"triggered\",\n                    \"site_id\": site_id,\n                    \"build_id\": build.get(\"id\"),\n                })\n            except Exception as e:\n                results.append({\"domain\": d, \"status\": \"error\", \"error\": str(e)})\n        return {\"results\": results}\n\n    # --- Web tools (allowlisted) ---\n    _WEB_ALLOWED = {\"openai.com\", \"platform.openai.com\", \"github.com\", \"openai.github.io\"}\n\n    def _is_allowed_url(self, url: str) -> bool:\n        try:\n            u = urllib.parse.urlparse(url)\n            host = (u.netloc or \"\").lower()\n            # allow subdomains of allowed hosts\n            return any(host == h or host.endswith(\".\" + h) for h in self._WEB_ALLOWED)\n        except Exception:\n            return False\n\n    def handle_web_get(self, url: str, max_bytes: int = 20000) -> Dict[str, Any]:\n        if not (url or \"\").startswith(\"http\"):\n            return {\"error\": \"url must start with http(s)\"}\n        if not self._is_allowed_url(url):\n            return {\"error\": \"host not allowlisted\"}\n        req = urllib.request.Request(url, method=\"GET\", headers={\"User-Agent\": \"project-rag-mcp/1.0\"})\n        try:\n            with urllib.request.urlopen(req, timeout=20) as resp:\n                raw = resp.read(max_bytes + 1)\n                clipped = raw[:max_bytes]\n                return {\n                    \"url\": url,\n                    \"status\": resp.status,\n                    \"length\": len(raw),\n                    \"clipped\": len(raw) > len(clipped),\n                    \"content_preview\": clipped.decode(\"utf-8\", errors=\"ignore\")\n                }\n        except urllib.error.HTTPError as he:\n            body = he.read().decode(\"utf-8\", errors=\"ignore\")\n            return {\"url\": url, \"status\": he.code, \"error\": body[:1000]}\n        except Exception as e:\n            return {\"url\": url, \"error\": str(e)}\n\n    def handle_request(self, request: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        Handle MCP tool call request.\n\n        Request format:\n        {\n          \"jsonrpc\": \"2.0\",\n          \"id\": <request_id>,\n          \"method\": \"tools/call\",\n          \"params\": {\n            \"name\": \"rag.answer\" | \"rag.search\",\n            \"arguments\": {\n              \"repo\": \"project\" | \"project\",\n              \"question\": \"...\",\n              \"top_k\": 10  # optional, search only\n            }\n          }\n        }\n        \"\"\"\n        method = request.get(\"method\")\n        req_id = request.get(\"id\")\n\n        if method == \"tools/list\":\n            # Return available tools\n            return {\n                \"jsonrpc\": \"2.0\",\n                \"id\": req_id,\n                \"result\": {\n                    \"tools\": [\n                        {\n                            \"name\": \"rag_answer\",\n                            \"description\": \"Get RAG answer with citations for a question in a specific repo (project|project)\",\n                            \"inputSchema\": {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                    \"repo\": {\n                                        \"type\": \"string\",\n                                        \"description\": \"Repository name: 'project' or 'project'\",\n                                        \"enum\": [\"project\", \"project\"]\n                                    },\n                                    \"question\": {\n                                        \"type\": \"string\",\n                                        \"description\": \"Developer question to answer from codebase\"\n                                    }\n                                },\n                                \"required\": [\"repo\", \"question\"]\n                            }\n                        },\n                        {\n                            \"name\": \"rag_search\",\n                            \"description\": \"Retrieval-only search (debugging) - returns relevant code locations without generation\",\n                            \"inputSchema\": {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                    \"repo\": {\n                                        \"type\": \"string\",\n                                        \"description\": \"Repository name: 'project' or 'project'\",\n                                        \"enum\": [\"project\", \"project\"]\n                                    },\n                                    \"question\": {\n                                        \"type\": \"string\",\n                                        \"description\": \"Search query for code retrieval\"\n                                    },\n                                    \"top_k\": {\n                                        \"type\": \"integer\",\n                                        \"description\": \"Number of results to return (default: 10)\",\n                                        \"default\": 10\n                                    }\n                                },\n                                \"required\": [\"repo\", \"question\"]\n                            }\n                        },\n                        {\n                            \"name\": \"netlify_deploy\",\n                            \"description\": \"Trigger a Netlify build for project.net, project.dev, or both (uses NETLIFY_API_KEY)\",\n                            \"inputSchema\": {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                    \"domain\": {\n                                        \"type\": \"string\",\n                                        \"description\": \"Target domain\",\n                                        \"enum\": [\"project.net\", \"project.dev\", \"both\"],\n                                        \"default\": \"both\"\n                                    }\n                                }\n                            }\n                        },\n                        {\n                            \"name\": \"web_get\",\n                            \"description\": \"HTTP GET (allowlisted hosts only: openai.com, platform.openai.com, github.com, openai.github.io)\",\n                            \"inputSchema\": {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                    \"url\": {\"type\": \"string\", \"description\": \"Absolute URL to fetch\"},\n                                    \"max_bytes\": {\"type\": \"integer\", \"description\": \"Max bytes to return\", \"default\": 20000}\n                                },\n                                \"required\": [\"url\"]\n                            }\n                        }\n                    ]\n                }\n            }\n\n        elif method == \"tools/call\":\n            params = request.get(\"params\", {})\n            tool_name = params.get(\"name\")\n            args = params.get(\"arguments\", {})\n\n            # Backward-compat: accept legacy dotted names\n            if tool_name in (\"rag.answer\", \"rag_answer\"):\n                result = self.handle_rag_answer(\n                    repo=args.get(\"repo\"),\n                    question=args.get(\"question\", \"\")\n                )\n                return {\n                    \"jsonrpc\": \"2.0\",\n                    \"id\": req_id,\n                    \"result\": {\"content\": [{\"type\": \"text\", \"text\": json.dumps(result, indent=2)}]}\n                }\n\n            elif tool_name in (\"rag.search\", \"rag_search\"):\n                result = self.handle_rag_search(\n                    repo=args.get(\"repo\"),\n                    question=args.get(\"question\", \"\"),\n                    top_k=args.get(\"top_k\", 10)\n                )\n                return {\n                    \"jsonrpc\": \"2.0\",\n                    \"id\": req_id,\n                    \"result\": {\"content\": [{\"type\": \"text\", \"text\": json.dumps(result, indent=2)}]}\n                }\n\n            elif tool_name in (\"netlify.deploy\", \"netlify_deploy\"):\n                domain = args.get(\"domain\", \"both\")\n                result = self.handle_netlify_deploy(domain)\n                return {\n                    \"jsonrpc\": \"2.0\",\n                    \"id\": req_id,\n                    \"result\": {\"content\": [{\"type\": \"text\", \"text\": json.dumps(result, indent=2)}]}\n                }\n\n            elif tool_name in (\"web.get\", \"web_get\"):\n                url = args.get(\"url\", \"\")\n                max_bytes = args.get(\"max_bytes\", 20000)\n                result = self.handle_web_get(url, max_bytes=max_bytes)\n                return {\n                    \"jsonrpc\": \"2.0\",\n                    \"id\": req_id,\n                    \"result\": {\"content\": [{\"type\": \"text\", \"text\": json.dumps(result, indent=2)}]}\n                }\n\n            else:\n                return {\n                    \"jsonrpc\": \"2.0\",\n                    \"id\": req_id,\n                    \"error\": {\n                        \"code\": -32601,\n                        \"message\": f\"Unknown tool: {tool_name}\"\n                    }\n                }\n\n        elif method == \"initialize\":\n            return {\n                \"jsonrpc\": \"2.0\",\n                \"id\": req_id,\n                \"result\": {\n                    \"protocolVersion\": \"2024-11-05\",\n                    \"capabilities\": {\n                        \"tools\": {}\n                    },\n                    \"serverInfo\": {\n                        \"name\": \"project-rag-mcp\",\n                        \"version\": \"1.0.0\"\n                    }\n                }\n            }\n\n        else:\n            return {\n                \"jsonrpc\": \"2.0\",\n                \"id\": req_id,\n                \"error\": {\n                    \"code\": -32601,\n                    \"message\": f\"Method not found: {method}\"\n                }\n            }\n\n    def run(self):\n        \"\"\"Main stdio loop.\"\"\"\n        self._log(\"MCP server starting (stdio mode)...\")\n\n        for line in sys.stdin:\n            line = line.strip()\n            if not line:\n                continue\n\n            try:\n                request = json.loads(line)\n                response = self.handle_request(request)\n                print(json.dumps(response), flush=True)\n            except json.JSONDecodeError as e:\n                self._error(f\"Invalid JSON: {e}\")\n                print(json.dumps({\n                    \"jsonrpc\": \"2.0\",\n                    \"id\": None,\n                    \"error\": {\n                        \"code\": -32700,\n                        \"message\": \"Parse error\"\n                    }\n                }), flush=True)\n            except Exception as e:\n                self._error(f\"Unexpected error: {e}\")\n                print(json.dumps({\n                    \"jsonrpc\": \"2.0\",\n                    \"id\": None,\n                    \"error\": {\n                        \"code\": -32603,\n                        \"message\": f\"Internal error: {e}\"\n                    }\n                }), flush=True)\n\n\nif __name__ == \"__main__\":\n    server = MCPServer()\n    server.run()"}
{"id":83,"text":"import express from 'express';\nimport fetch from 'node-fetch';\n\nconst app = express();\nconst PORT = process.env.PORT || 8014;\nconst RAG_API_URL = process.env.RAG_API_URL || 'http://127.0.0.1:8012';\n\napp.get('/health', (req, res) => {\n  res.json({ status: 'ok', proxy: true, target: RAG_API_URL });\n});\n\n// JSON answer proxy\napp.get('/mcp/answer', async (req, res) => {\n  try {\n    const { q, repo, token } = req.query;\n    const u = new URL('/answer', RAG_API_URL);\n    if (q) u.searchParams.set('q', q);\n    if (repo) u.searchParams.set('repo', repo);\n    const headers = token ? { Authorization: `Bearer ${token}` } : {};\n    const r = await fetch(u.toString(), { headers });\n    const data = await r.json();\n    res.json(data);\n  } catch (e) {\n    res.status(500).json({ error: String(e) });\n  }\n});\n\n// JSON search proxy\napp.get('/mcp/search', async (req, res) => {\n  try {\n    const { q, repo, top_k, token } = req.query;\n    const u = new URL('/search', RAG_API_URL);\n    if (q) u.searchParams.set('q', q);\n    if (repo) u.searchParams.set('repo', repo);\n    if (top_k) u.searchParams.set('top_k', String(top_k));"}
{"id":84,"text":"const headers = token ? { Authorization: `Bearer ${token}` } : {};\n    const r = await fetch(u.toString(), { headers });\n    const data = await r.json();\n    res.json(data);\n  } catch (e) {\n    res.status(500).json({ error: String(e) });\n  }\n});\n\n// SSE proxy for streaming answer\napp.get('/mcp/answer_stream', async (req, res) => {\n  try {\n    const { q, repo, token } = req.query;\n    const u = new URL('/answer_stream', RAG_API_URL);\n    if (q) u.searchParams.set('q', q);\n    if (repo) u.searchParams.set('repo', repo);\n    const headers = token ? { Authorization: `Bearer ${token}` } : {};\n\n    const r = await fetch(u.toString(), { headers });\n    res.setHeader('Content-Type', 'text/event-stream; charset=utf-8');\n    res.setHeader('Cache-Control', 'no-cache');\n    res.setHeader('X-Accel-Buffering', 'no');\n\n    if (!r.ok || !r.body) {\n      res.write(`data: [ERROR] upstream ${r.status}\\n\\n`);\n      return res.end();\n    }\n\n    const reader = r.body.getReader();\n    const decoder = new TextDecoder();\n    while (true) {\n      const { done, value } = await reader.read();\n      if (done) break;\n      res.write(decoder.decode(value));\n      // flush"}
{"id":85,"text":"}\n    res.end();\n  } catch (e) {\n    res.setHeader('Content-Type', 'text/event-stream; charset=utf-8');\n    res.write(`data: [ERROR] ${String(e)}\\n\\n`);\n    res.end();\n  }\n});\n\napp.listen(PORT, () => {\n  console.log(`Node proxy listening on :${PORT}, targeting ${RAG_API_URL}`);\n});"}
{"id":86,"text":"#!/usr/bin/env python3\n\"\"\"Quick token test for docs - measure actual usage\"\"\"\nimport os\nos.environ[\"OLLAMA_URL\"] = \"http://127.0.0.1:11434/api\"\nos.environ[\"GEN_MODEL\"] = \"qwen3-coder:30b\"\n\nimport sys\nROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\nsys.path.insert(0, ROOT_DIR)\n\ntry:\n    import tiktoken\n    enc = tiktoken.encoding_for_model(\"gpt-4o\")\n    def count_tokens(text): return len(enc.encode(text))\nexcept:\n    def count_tokens(text): return len(text) // 4\n\n# Test 1: Claude Alone (read full files)\nfrom pathlib import Path\nquestion = \"How are fax jobs created and dispatched\"\nkeywords = [\"fax\", \"jobs\", \"created\", \"dispatched\"]\nrepo_path = os.getenv('project_PATH', '/abs/path/to/project')\n\nfull_content = \"\"\nfor py_file in list(Path(repo_path).rglob('*.py'))[:10]:\n    try:\n        content = py_file.read_text(errors='ignore')\n        if any(kw in content.lower() for kw in keywords):\n            full_content += f\"\\n{'='*70}\\n{content}\\n\"\n    except:\n        pass\n\ntokens_claude_alone = count_tokens(full_content)\nprint(f\"1. Claude Alone: {tokens_claude_alone:,} tokens\")"}
{"id":87,"text":"# Test 2: MCP metadata only (simulate what Claude Code gets)\nmcp_response = \"\"\"{\"results\": [\n  {\"file_path\": \"server.py\", \"start_line\": 120, \"end_line\": 145, \"score\": 0.89},\n  {\"file_path\": \"tasks.py\", \"start_line\": 67, \"end_line\": 89, \"score\": 0.85},\n  {\"file_path\": \"models.py\", \"start_line\": 234, \"end_line\": 267, \"score\": 0.78}\n], \"count\": 3}\"\"\"\n\n# Tool schema (sent with every request)\ntool_schema = \"\"\"{\"tools\": [{\"name\": \"rag_search\", \"description\": \"Search codebase\", \"inputSchema\": {...}}]}\"\"\"\n\ntokens_mcp = count_tokens(mcp_response + tool_schema)\nprint(f\"2. Claude + RAG via MCP: {tokens_mcp:,} tokens\")\n\n# Calculate savings\nsaved = tokens_claude_alone - tokens_mcp\npct = (saved / tokens_claude_alone * 100) if tokens_claude_alone > 0 else 0\nreduction = tokens_claude_alone / tokens_mcp if tokens_mcp > 0 else 0\n\nprint(f\"\\nSavings: {saved:,} tokens ({pct:.1f}%)\")\nprint(f\"Reduction: {reduction:.1f}x\")\n\n# Cost (gpt-4o: $2.50/1M input)\ncost_alone = tokens_claude_alone * (2.50 / 1_000_000)\ncost_mcp = tokens_mcp * (2.50 / 1_000_000)"}
{"id":88,"text":"cost_saved = cost_alone - cost_mcp\n\nprint(f\"\\nPer query: ${cost_saved:.6f} saved\")\nprint(f\"Per 100 queries: ${cost_saved * 100:.2f} saved\")\nprint(f\"Per month (100/day): ${cost_saved * 3000:.2f} saved\")"}
{"id":89,"text":"#!/usr/bin/env python3\nimport os, sys, json, urllib.request, urllib.error\n\nAPI = \"https://api.netlify.com/api/v1\"\napi(path: str, method: str = \"GET\", data: dict | None = None) -> dict:\n    token = os.getenv(\"NETLIFY_API_KEY\")\n    if not token:\n        print(\"NETLIFY_API_KEY not set\", file=sys.stderr)\n        sys.exit(2)\n    url = f\"{API}{path}\"\n    req = urllib.request.Request(url, method=method)\n    req.add_header(\"Authorization\", f\"Bearer {token}\")\n    req.add_header(\"Content-Type\", \"application/json\")\n    body = json.dumps(data).encode(\"utf-8\") if data is not None else None\n    with urllib.request.urlopen(req, data=body, timeout=30) as resp:\n        raw = resp.read().decode(\"utf-8\")\n        return json.loads(raw) if raw else {}\nfind_site(domain: str) -> dict | None:\n    sites = api(\"/sites\", \"GET\")\n    dom = (domain or \"\").strip().lower()\n    if isinstance(sites, list):\n        for s in sites:\n            for key in (\"custom_domain\", \"url\", \"ssl_url\"):\n                val = (s.get(key) or \"\").lower()\n                if val and dom in val:\n                    return s\n    return None"}
{"id":90,"text":"trigger(domain: str) -> dict:\n    s = find_site(domain)\n    if not s:\n        return {\"domain\": domain, \"status\": \"not_found\"}\n    sid = s.get(\"id\")\n    if not sid:\n        return {\"domain\": domain, \"status\": \"no_site_id\"}\n    try:\n        b = api(f\"/sites/{sid}/builds\", \"POST\", {})\n        return {\"domain\": domain, \"status\": \"triggered\", \"site_id\": sid, \"build_id\": b.get(\"id\")}\n    except Exception as e:\n        return {\"domain\": domain, \"status\": \"error\", \"error\": str(e)}"}
{"id":91,"text":"main():\n    if len(sys.argv) < 2:\n        print(\"Usage: netlify_deploy.py [project.net|project.dev|both|list]\", file=sys.stderr)\n        sys.exit(2)\n    cmd = sys.argv[1].strip().lower()\n    if cmd == \"list\":\n        sites = api(\"/sites\", \"GET\")\n        out = []\n        for s in sites if isinstance(sites, list) else []:\n            out.append({\"id\": s.get(\"id\"), \"name\": s.get(\"name\"), \"url\": s.get(\"url\"), \"custom_domain\": s.get(\"custom_domain\")})\n        print(json.dumps(out, indent=2))\n        return\n    domains = [\"project.net\", \"project.dev\"] if cmd == \"both\" else [cmd]\n    results = [trigger(d) for d in domains]\n    print(json.dumps(results, indent=2))\n\nif __name__ == \"__main__\":\n    main()"}
{"id":92,"text":"#!/usr/bin/env python3\nimport os, sys, re\n\nSCAN_ALL = os.getenv(\"SCAN_ALL\", \"0\").lower() in {\"1\",\"true\",\"yes\"}\nROOTS = [os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))]\nif SCAN_ALL:\n    ROOTS += [\n        os.getenv('PROJECT_PATH', '/abs/path/to/project'),\n        os.getenv('project_PATH', '/abs/path/to/project'),\n    ]\n\nBAD_PATTERNS = [\n    r\"\\bChatCompletion\\b\",\n    r\"\\bclient\\.chat\\.completions\\b\",\n    r\"\\bassistants?\\.v1\\b\",\n    r\"\\bgpt-3\\.5\\b\",\n    r\"\\bgpt-4(?!\\.1|o)\\b\",\n    r\"\\bgpt-4o(-mini)?\\b\",\n    r\"\\btext-embedding-ada\\b\",\n    r\"\\btext-embedding-00[23]\\b\",\n]\nALLOWLIST_FILES = {\n    # add filenames you want ignored (e.g., historical docs)\n}\n\nSKIP_DIRS = {\".git\", \".venv\", \"venv\", \"node_modules\", \"dist\", \"build\", \"vendor\", \"third_party\", \"site-packages\", \"__pycache__\"}\n\nscan_file(path: str) -> list[str]:\n    try:\n        with open(path, \"r\", errors=\"ignore\") as f:\n            s = f.read()\n    except Exception:\n        return []\n    hits = []\n    for pat in BAD_PATTERNS:\n        if re.search(pat, s):\n            hits.append(pat)\n    return hits"}
{"id":93,"text":"main() -> int:\n    offenders = []\n    for root in ROOTS:\n        if not os.path.isdir(root):\n            continue\n        for base, dirs, files in os.walk(root):\n            # prune skip dirs\n            dirs[:] = [d for d in dirs if d not in SKIP_DIRS and not d.startswith('.')]\n            for name in files:\n                if name in ALLOWLIST_FILES:\n                    continue\n                # Scan code files only (skip docs like .md)\n                if not any(name.endswith(ext) for ext in (\".py\", \".ts\", \".tsx\", \".js\", \".rb\")):\n                    continue\n                path = os.path.join(base, name)\n                # skip this guard file and sitecustomize self-detection\n                if path.endswith(\"scripts/guard_legacy_api.py\") or path.endswith(\"sitecustomize.py\"):\n                    continue\n                hits = scan_file(path)\n                if hits:\n                    offenders.append((path, hits))\n    if offenders:\n        print(\"\\u274c Legacy APIs/models detected:\")\n        for p, pats in offenders:\n            print(f\"- {p}\")\n            for pat in pats:\n                print(f\"    \\u21b3 {pat}\")\n        print(\"\\nAction: replace Chat Completions with Responses API calls; update model pins (e.g., gpt-4o-mini-latest or a dated pin).\")\n        print(\"Docs:\")\n        print(\"  https://openai.com/index/new-tools-and-features-in-the-responses-api/\")\n        print(\"  https://openai.com/index/introducing-upgrades-to-codex/\")\n        return 2\n    print(\"\\u2713 No legacy APIs/models detected.\")\n    return 0\n\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())"}
{"id":94,"text":"#!/usr/bin/env python3\n\"\"\"Measure MCP tool schema overhead - the part sent on EVERY request\"\"\"\nimport sys, os\nimport json\nROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\nsys.path.insert(0, ROOT_DIR)\n\ntry:\n    import tiktoken\n    enc = tiktoken.encoding_for_model(\"gpt-4o\")\n    def count_tokens(text): return len(enc.encode(text))\nexcept:\n    def count_tokens(text): return len(text) // 4\n\n# Get tool schemas\nfrom mcp_server import MCPServer\nserver = MCPServer()\ntools_req = {'jsonrpc': '2.0', 'id': 1, 'method': 'tools/list', 'params': {}}\ntools_resp = server.handle_request(tools_req)\ntools_json = json.dumps(tools_resp['result']['tools'], indent=2)\n\nschema_tokens = count_tokens(tools_json)\n\nprint(\"=\" * 80)\nprint(\"MCP TOOL SCHEMA OVERHEAD (sent with EVERY Claude Code request)\")\nprint(\"=\" * 80)\nprint(f\"Schema tokens: {schema_tokens:,}\")\nprint(f\"Schema size: {len(tools_json):,} bytes\")\nprint(f\"\\nThis overhead is ADDED to every single request.\")\nprint(f\"Even if MCP response is small, you always pay for the tool schemas.\\n\")"}
{"id":95,"text":"# Show the actual schema\nprint(\"Tool schemas:\")\nfor tool in tools_resp['result']['tools']:\n    print(f\"  - {tool['name']}: {len(json.dumps(tool)):,} bytes\")\n\nwith open('/tmp/mcp_schema.json', 'w') as f:\n    f.write(tools_json)\nprint(f\"\\nFull schema saved to: /tmp/mcp_schema.json\")"}
{"id":96,"text":"#!/usr/bin/env python3\n\"\"\"\nCompare token usage across three approaches:\n1. Claude alone (no RAG) - reads full files\n2. RAG via direct Python calls (hybrid_search.py)\n3. RAG via MCP tools (what Claude Code uses)\n\nThis shows actual token savings from using RAG.\n\"\"\"\n\nimport sys\nimport os\nimport json\nfrom pathlib import Path\n\n# Try tiktoken for precise counts\ntry:\n    import tiktoken\n    HAS_TIKTOKEN = True\n    print(\"âœ“ Using tiktoken for precise token counts\\n\")\nexcept ImportError:\n    HAS_TIKTOKEN = False\n    print(\"âš ï¸  tiktoken not installed - using estimates (1 token â‰ˆ 4 chars)\")\n    print(\"   Install: pip install tiktoken\\n\")\n\ncount_tokens(text: str, model: str = \"gpt-4o\") -> int:\n    \"\"\"Count tokens precisely or estimate\"\"\"\n    if HAS_TIKTOKEN:\n        try:\n            encoding = tiktoken.encoding_for_model(model)\n            return len(encoding.encode(text))\n        except:\n            pass\n    return len(text) // 4\n\n\n# ============================================================\n# Approach 1: Claude Alone (Traditional - NO RAG)\n# ============================================================"}
{"id":97,"text":"measure_claude_alone(question: str, repo: str):\n    \"\"\"\n    Simulate what Claude would do WITHOUT RAG:\n    - Extract keywords from question\n    - Grep files for those keywords\n    - Read 5-10 full files\n    \"\"\"\n    repo_paths = {\n        'project': os.getenv('PROJECT_PATH', '/abs/path/to/project'),\n        'faxbot': os.getenv('FAXBOT_PATH', '/abs/path/to/faxbot')\n    }\n\n    repo_path = repo_paths.get(repo)\n    if not repo_path or not os.path.exists(repo_path):\n        return {'error': f'Repo not found: {repo}'}\n\n    # Extract keywords (what Claude would search for)\n    keywords = [w.lower() for w in question.split() if len(w) > 3][:5]\n\n    # Find matching files\n    matched_files = []\n    combined_text = \"\"\n\n    for py_file in Path(repo_path).rglob('*.py'):\n        # Skip vendor/node_modules\n        if any(skip in str(py_file) for skip in ['node_modules', '.venv', 'vendor', '.git']):\n            continue\n\n        try:\n            content = py_file.read_text(errors='ignore')\n\n            # If keywords match, Claude would read this ENTIRE file\n            if any(kw in content.lower() for kw in keywords):\n                matched_files.append(str(py_file))\n                combined_text += f\"\\n{'='*70}\\n{py_file}\\n{'='*70}\\n{content}\\n\"\n\n                if len(matched_files) >= 10:  # Limit to 10 files\n                    break\n        except:\n            pass\n\n    tokens = count_tokens(combined_text)\n\n    return {\n        'approach': 'Claude Alone (no RAG)',\n        'files_read': len(matched_files),\n        'chars': len(combined_text),\n        'tokens': tokens,\n        'files': matched_files[:5]  # Show first 5\n    }\n\n\n# ============================================================\n# Approach 2: RAG via Direct Python\n# ============================================================"}
{"id":98,"text":"measure_rag_python(question: str, repo: str, top_k: int = 10):\n    \"\"\"Use hybrid_search.py directly (local Python calls)\"\"\"\n    try:\n        from hybrid_search import search_routed_multi\n\n        results = search_routed_multi(question, repo_override=repo, final_k=top_k)\n\n        # Combine retrieved chunks\n        combined_text = \"\"\n        for r in results:\n            combined_text += f\"{r['file_path']}:{r['start_line']}-{r['end_line']}\\n\"\n            combined_text += r.get('code', '') + \"\\n\\n\"\n\n        tokens = count_tokens(combined_text)\n\n        return {\n            'approach': 'RAG (direct Python)',\n            'chunks': len(results),\n            'chars': len(combined_text),\n            'tokens': tokens,\n            'files_touched': len(set(r['file_path'] for r in results)),\n            'top_scores': [r['rerank_score'] for r in results[:3]]\n        }\n    except Exception as e:\n        return {'error': str(e)}\n\n\n# ============================================================\n# Approach 3: RAG via MCP (What Claude Code Uses)\n# ============================================================"}
{"id":99,"text":"measure_rag_mcp(question: str, repo: str, top_k: int = 10):\n    \"\"\"\n    Simulate MCP tool call (what Claude Code actually uses).\n    This calls the same backend as direct Python but through MCP layer.\n    \"\"\"\n    try:\n        from mcp_server import MCPServer\n\n        # Call rag_search tool\n        req = {\n            'jsonrpc': '2.0',\n            'id': 1,\n            'method': 'tools/call',\n            'params': {\n                'name': 'rag_search',\n                'arguments': {\n                    'repo': repo,\n                    'question': question,\n                    'top_k': top_k\n                }\n            }\n        }\n\n        server = MCPServer()\n        resp = server.handle_request(req)\n\n        # Extract results\n        result_text = resp['result']['content'][0]['text']\n        result_data = json.loads(result_text)\n\n        # MCP returns file paths + line ranges (no full code in the response)\n        # But we need to count what gets sent to Claude\n        combined_text = result_text  # This is what Claude receives\n\n        tokens = count_tokens(combined_text)\n\n        return {\n            'approach': 'RAG (via MCP tools)',\n            'chunks': result_data.get('count', 0),\n            'chars': len(combined_text),\n            'tokens': tokens,\n            'mcp_result_size': len(result_text)\n        }\n    except Exception as e:\n        return {'error': str(e)}\n\n\n# ============================================================\n# Run Comparison\n# ============================================================"}
{"id":100,"text":"run_test(question: str, repo: str):\n    \"\"\"Run all three approaches and compare\"\"\"\n    print(f\"\\n{'='*70}\")\n    print(f\"TEST: {question}\")\n    print(f\"REPO: {repo}\")\n    print(f\"{'='*70}\\n\")\n\n    # Method 1: Claude alone\n    print(\"â³ Measuring: Claude Alone (traditional grep + read files)...\")\n    claude_alone = measure_claude_alone(question, repo)\n\n    # Method 2: RAG Python\n    print(\"â³ Measuring: RAG via Direct Python...\")\n    rag_python = measure_rag_python(question, repo, top_k=10)\n\n    # Method 3: RAG MCP\n    print(\"â³ Measuring: RAG via MCP tools...\")\n    rag_mcp = measure_rag_mcp(question, repo, top_k=10)\n\n    # Print results\n    print(f\"\\n{'='*70}\")\n    print(\"RESULTS:\")\n    print(f\"{'='*70}\\n\")\n\n    # Claude Alone\n    if 'error' not in claude_alone:\n        print(f\"1ï¸âƒ£  CLAUDE ALONE (no RAG):\")\n        print(f\"   Files read: {claude_alone['files_read']}\")\n        print(f\"   Total tokens: {claude_alone['tokens']:,}\")\n        print(f\"   Characters: {claude_alone['chars']:,}\")\n\n    # RAG Python\n    if 'error' not in rag_python:\n        print(f\"\\n2ï¸âƒ£  RAG (Direct Python):\")\n        print(f\"   Chunks retrieved: {rag_python['chunks']}\")\n        print(f\"   Files touched: {rag_python['files_touched']}\")\n        print(f\"   Total tokens: {rag_python['tokens']:,}\")\n        print(f\"   Top scores: {[f'{s:.3f}' for s in rag_python.get('top_scores', [])]}\")\n\n    # RAG MCP\n    if 'error' not in rag_mcp:\n        print(f\"\\n3ï¸âƒ£  RAG (via MCP - what Claude Code uses):\")\n        print(f\"   Chunks retrieved: {rag_mcp['chunks']}\")\n        print(f\"   Total tokens: {rag_mcp['tokens']:,}\")\n\n    # Calculate savings\n    if all('error' not in r for r in [claude_alone, rag_python, rag_mcp]):\n        alone_tokens = claude_alone['tokens']\n        python_tokens = rag_python['tokens']\n        mcp_tokens = rag_mcp['tokens']\n\n        print(f\"\\n{'='*70}\")\n        print(\"ðŸ’° TOKEN SAVINGS:\")\n        print(f\"{'='*70}\")\n\n        # Python vs Alone\n        saved_python = alone_tokens - python_tokens\n        pct_python = (saved_python / alone_tokens * 100) if alone_tokens > 0 else 0\n\n        print(f\"\\nRAG Python vs Claude Alone:\")\n        print(f\"   Tokens saved: {saved_python:,}\")\n        print(f\"   Percentage: {pct_python:.1f}%\")\n        print(f\"   Reduction: {alone_tokens / max(python_tokens, 1):.1f}x smaller\")\n\n        # MCP vs Alone\n        saved_mcp = alone_tokens - mcp_tokens\n        pct_mcp = (saved_mcp / alone_tokens * 100) if alone_tokens > 0 else 0\n\n        print(f\"\\nRAG MCP vs Claude Alone:\")\n        print(f\"   Tokens saved: {saved_mcp:,}\")\n        print(f\"   Percentage: {pct_mcp:.1f}%\")\n        print(f\"   Reduction: {alone_tokens / max(mcp_tokens, 1):.1f}x smaller\")\n\n        # Cost estimate (gpt-4o: $2.50/1M input tokens)\n        cost_per_token = 2.50 / 1_000_000\n\n        print(f\"\\nðŸ’µ COST SAVINGS (gpt-4o @ $2.50/1M input tokens):\")\n        print(f\"   Per query (Python): ${saved_python * cost_per_token:.6f}\")\n        print(f\"   Per 1000 queries (Python): ${saved_python * cost_per_token * 1000:.2f}\")\n        print(f\"   Per query (MCP): ${saved_mcp * cost_per_token:.6f}\")\n        print(f\"   Per 1000 queries (MCP): ${saved_mcp * cost_per_token * 1000:.2f}\")\n\n    return {\n        'question': question,\n        'repo': repo,\n        'claude_alone': claude_alone.get('tokens', 0),\n        'rag_python': rag_python.get('tokens', 0),\n        'rag_mcp': rag_mcp.get('tokens', 0)\n    }\n\n\nif __name__ == '__main__':\n    # Test cases\n    tests = [\n        (\"Where is OAuth token validated\", \"project\"),\n        (\"How are fax jobs created and dispatched\", \"project\"),\n        (\"EventStream component event types in dropdown\", \"project\"),\n    ]\n\n    results = []\n\n    for question, repo in tests:\n        try:\n            result = run_test(question, repo)\n            results.append(result)\n        except Exception as e:\n            print(f\"\\nâŒ Error: {e}\")\n\n    # Overall summary\n    if results:\n        print(f\"\\n\\n{'='*70}\")\n        print(\"ðŸ“Š OVERALL SUMMARY\")\n        print(f\"{'='*70}\\n\")\n\n        total_alone = sum(r['claude_alone'] for r in results)\n        total_python = sum(r['rag_python'] for r in results)\n        total_mcp = sum(r['rag_mcp'] for r in results)\n\n        print(f\"Total queries: {len(results)}\")\n        print(f\"\\nClaude Alone: {total_alone:,} tokens\")\n        print(f\"RAG Python: {total_python:,} tokens\")\n        print(f\"RAG MCP: {total_mcp:,} tokens\")\n\n        if total_alone > 0:\n            print(f\"\\nAverage reduction (Python): {total_alone / max(total_python, 1):.1f}x\")\n            print(f\"Average reduction (MCP): {total_alone / max(total_mcp, 1):.1f}x\")\n\n            saved_python = total_alone - total_python\n            saved_mcp = total_alone - total_mcp\n\n            print(f\"\\nTotal saved (Python): {saved_python:,} tokens ({saved_python/total_alone*100:.1f}%)\")\n            print(f\"Total saved (MCP): {saved_mcp:,} tokens ({saved_mcp/total_alone*100:.1f}%)\")"}
{"id":101,"text":"#!/usr/bin/env python3\nimport sys, json, re\n\n\"\"\"\nUsage:\n  python scripts/eval_gate_guard.py <answers.jsonl>\n\nWhere each line is a JSON object containing:\n  {\"q\": \"...\", \"repo\": \"project\", \"answer\": \"...\"}\nThis fails if the answer lacks a [repo: ...] header or no file path-like citation.\n\"\"\"\n\nHEADER_RE = re.compile(r\"^\\[repo:\\s*(project|project)\\]\", re.I | re.M)\nPATH_RE = re.compile(r\"[A-Za-z0-9_\\-./]+?\\.[A-Za-z0-9_]+:\\d+-\\d+\")\n\nok(answer: str) -> bool:\n    if not HEADER_RE.search(answer or \"\"):\n        return False\n    if not PATH_RE.search(answer or \"\"):\n        return False\n    return True"}
{"id":102,"text":"main():\n    if len(sys.argv) < 2:\n        print(\"usage: python scripts/eval_gate_guard.py <answers.jsonl>\")\n        sys.exit(2)\n    bad = 0\n    with open(sys.argv[1], \"r\", errors=\"ignore\") as f:\n        for i, line in enumerate(f, 1):\n            line = line.strip()\n            if not line:\n                continue\n            try:\n                obj = json.loads(line)\n            except Exception:\n                print(f\"line {i}: not json\")\n                bad += 1\n                continue\n            ans = obj.get(\"answer\", \"\")\n            if not ok(ans):\n                print(f\"line {i}: FAIL (missing repo header or file citation)\")\n                bad += 1\n    if bad:\n        print(f\"\\u274c guard failed: {bad} bad answer(s)\")\n        sys.exit(3)\n    print(\"\\u2713 guard passed\")\n    sys.exit(0)\n\n\nif __name__ == \"__main__\":\n    main()"}
{"id":103,"text":"import json\nimport os\nimport re\nfrom collections import Counter, defaultdict\nfrom pathlib import Path\nshould_skip_directory(path):\n    \"\"\"Skip vendor/dependency directories\"\"\"\n    skip_patterns = [\n        'node_modules', '.venv', 'venv', '__pycache__', \n        '.git', 'dist', 'build', 'vendor', 'tmp',\n        'test', 'tests', 'spec', 'specs',  # test files\n        'migrations', 'db/migrate',  # migrations\n        'locale', 'locales', 'i18n',  # translations\n        '.bundle', 'coverage', '.pytest_cache'\n    ]\n    return any(skip in path for skip in skip_patterns)"}
{"id":104,"text":"extract_semantic_terms(file_path, code):\n    \"\"\"Extract meaningful business/domain terms\"\"\"\n    terms = set()\n    \n    # 1. Extract from file/directory names (most semantic!)\n    path_parts = file_path.split('/')\n    for part in path_parts:\n        # Clean up: UserController.rb -> user, controller\n        cleaned = re.sub(r'[._-]', ' ', part)\n        words = re.findall(r'[A-Z][a-z]+|[a-z]+', cleaned)\n        terms.update(w.lower() for w in words if len(w) > 3)\n    \n    # 2. Extract class names (PascalCase)\n    class_names = re.findall(r'\\bclass ([A-Z][a-zA-Z0-9_]+)', code)\n    for name in class_names:\n        # Split camelCase: AIStudioComponent -> ai, studio, component\n        words = re.findall(r'[A-Z][a-z]+|[A-Z]+(?=[A-Z]|$)', name)\n        terms.update(w.lower() for w in words if len(w) > 2)\n    \n    # 3. Extract function names (meaningful ones only)\n    func_names = re.findall(r'\\b(?:def|function|const)\\s+([a-z][a-zA-Z0-9_]+)', code)\n    for name in func_names:\n        # Only keep multi-word functions: validate_oauth not just get\n        if '_' in name:\n            words = name.split('_')\n            terms.update(w for w in words if len(w) > 3)\n    \n    # 4. Extract from comments (gold mine!)\n    comments = re.findall(r'(?:#|//|/\\*|\\*)\\s*(.+)', code)\n    for comment in comments:\n        # Extract capitalized words (likely domain terms)\n        words = re.findall(r'\\b[A-Z][a-z]{2,}\\b', comment)\n        terms.update(w.lower() for w in words)\n    \n    # 5. Extract string literals (API endpoints, routes, etc)\n    strings = re.findall(r'[\"\\']([^\"\\']{5,50})[\"\\']', code)\n    for s in strings:\n        if '/' in s:  # likely a route\n            parts = s.split('/')\n            terms.update(p.lower() for p in parts if p.isalpha() and len(p) > 3)\n    \n    # Filter out programming keywords\n    stop_words = {\n        'return', 'function', 'class', 'const', 'import', 'export',\n        'from', 'self', 'this', 'super', 'none', 'null', 'true', 'false',\n        'async', 'await', 'yield', 'raise', 'assert', 'break', 'continue',\n        'string', 'number', 'boolean', 'object', 'array', 'type', 'interface',\n        'params', 'args', 'kwargs', 'options', 'config', 'props', 'state'\n    }\n    \n    return {t for t in terms if t not in stop_words and t.isalpha()}"}
{"id":105,"text":"analyze_repo_semantic(repo_path, repo_name):\n    \"\"\"Find meaningful business domain terms\"\"\"\n    term_counts = Counter()\n    term_files = defaultdict(set)\n    directory_terms = Counter()\n    \n    total_files = 0\n    \n    for root, dirs, files in os.walk(repo_path):\n        # Skip vendor directories\n        if should_skip_directory(root):\n            continue\n        \n        # Remove skippable dirs from traversal\n        dirs[:] = [d for d in dirs if not should_skip_directory(os.path.join(root, d))]\n        \n        # Analyze directory name itself\n        dir_name = os.path.basename(root)\n        if dir_name and len(dir_name) > 3:\n            directory_terms[dir_name.lower()] += 1\n        \n        for file in files:\n            # Only source code files\n            if not any(file.endswith(ext) for ext in ['.py', '.js', '.ts', '.tsx', '.rb', '.yml', '.java']):\n                continue\n            \n            file_path = os.path.join(root, file)\n            rel_path = os.path.relpath(file_path, repo_path)\n            \n            try:\n                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n                    code = f.read()\n                    \n                terms = extract_semantic_terms(rel_path, code)\n                \n                for term in terms:\n                    term_counts[term] += 1\n                    term_files[term].add(rel_path)\n                \n                total_files += 1\n            except:\n                continue\n    \n    # Calculate relevance scores\n    scored_terms = []\n    for term, count in term_counts.items():\n        file_count = len(term_files[term])\n        \n        # Score formula:\n        # - Appears in multiple files (2-20% of codebase) = domain term\n        # - Too rare (1 file) = noise\n        # - Too common (>20% files) = generic utility\n        if file_count >= 2 and file_count <= total_files * 0.2:\n            # Boost if term appears in directory names (very semantic)\n            dir_boost = 2.0 if term in directory_terms else 1.0\n            \n            # Calculate domain specificity score\n            score = (count * file_count * dir_boost) / (total_files + 1)\n            \n            scored_terms.append({\n                'term': term,\n                'score': score,\n                'files': file_count,\n                'mentions': count,\n                'in_directories': term in directory_terms,\n                'sample_files': list(term_files[term])[:3]\n            })\n    \n    # Sort by score\n    scored_terms.sort(key=lambda x: x['score'], reverse=True)\n    \n    return scored_terms, total_files, directory_terms\n\nif __name__ == '__main__':\n    repos = {\n        'project': os.getenv('PROJECT_PATH', '/abs/path/to/project'),\n        'faxbot': os.getenv('FAXBOT_PATH', '/abs/path/to/faxbot')\n    }\n    \n    all_results = {}\n    \n    for repo_name, repo_path in repos.items():\n        print(f'\\n{\"=\"*80}')\n        print(f'SEMANTIC ANALYSIS: {repo_name}')\n        print(f'{\"=\"*80}')\n        \n        terms, total_files, directories = analyze_repo_semantic(repo_path, repo_name)\n        all_results[repo_name] = terms[:50]\n        \n        print(f'\\nAnalyzed {total_files} files')\n        print(f'Found {len(terms)} meaningful domain terms')\n        print(f'\\nTop 30 Business/Domain Keywords:\\n')\n        \n        for i, t in enumerate(terms[:30], 1):\n            dir_marker = 'ðŸ“' if t['in_directories'] else '  '\n            print(f'{i:2}. {dir_marker} {t[\"term\"]:20} | Score: {t[\"score\"]:8.1f} | {t[\"files\"]:3} files | {t[\"mentions\"]:4} mentions')\n        \n        # Show sample context\n        print(f'\\nðŸ“„ Sample file locations for top terms:')\n        for t in terms[:5]:\n            print(f'\\n  {t[\"term\"]}:')\n            for f in t['sample_files']:\n                print(f'    - {f}')\n    \n    # Cross-analysis\n    print(f'\\n{\"=\"*80}')\n    print('CROSS-REPO COMPARISON')\n    print(f'{\"=\"*80}')\n    \n    viv_terms = {t['term'] for t in all_results['project'][:30]}\n    fax_terms = {t['term'] for t in all_results['project'][:30]}\n    \n    shared = viv_terms & fax_terms\n    viv_only = viv_terms - fax_terms\n    fax_only = fax_terms - viv_terms\n    \n    print(f'\\nðŸ”„ Shared terms ({len(shared)}):')\n    if shared:\n        print(f'   {\", \".join(sorted(shared)[:10])}')\n    \n    print(f'\\nðŸ’Š PROJECT-specific ({len(viv_only)}):')\n    print(f'   {\", \".join(sorted(list(viv_only)[:15]))}')\n    \n    print(f'\\nðŸ“  PROJECT-specific ({len(fax_only)}):')\n    print(f'   {\", \".join(sorted(list(fax_only)[:15]))}')\n    \n    # Generate suggested queries\n    print(f'\\n{\"=\"*80}')\n    print('SUGGESTED EVAL QUERIES (based on actual terms)')\n    print(f'{\"=\"*80}')\n    \n    for repo_name, terms in all_results.items():\n        print(f'\\n{repo_name.upper()}:')\n        top_terms = terms[:10]\n        \n        # Generate natural queries\n        queries = []\n        for t in top_terms[:5]:\n            queries.append(f'  - \"Where is {t[\"term\"]} implemented?\"')\n            if t['in_directories']:\n                queries.append(f'  - \"How does {t[\"term\"]} work?\"')\n        \n        for q in queries[:8]:\n            print(q)\n    \n    # Save\n    with open('semantic_keywords.json', 'w') as f:\n        json.dump(all_results, f, indent=2)\n    \n    print(f'\\nâœ“ Saved to semantic_keywords.json')"}
{"id":106,"text":"#!/usr/bin/env python3\n\"\"\"\nCompare token usage across FOUR approaches:\n\n1. Claude Alone (no RAG) - reads full files via grep\n2. RAG CLI Standalone - RAG answers directly (no Claude)\n3. Claude + RAG Direct - Claude gets full code chunks from RAG\n4. Claude + RAG via MCP - Claude gets MCP metadata responses\n\nShows actual tokens sent to LLM in each scenario.\n\"\"\"\n\nimport sys\nimport os\nROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\nsys.path.insert(0, ROOT_DIR)\nimport json\nfrom pathlib import Path\n\n# Try tiktoken for precise counts\ntry:\n    import tiktoken\n    HAS_TIKTOKEN = True\nexcept ImportError:\n    HAS_TIKTOKEN = False\n    print(\"âš ï¸  Install tiktoken for precise counts: pip install tiktoken\\n\")"}
{"id":107,"text":"count_tokens(text: str, model: str = \"gpt-4o\") -> int:\n    \"\"\"Count tokens precisely or estimate\"\"\"\n    if HAS_TIKTOKEN:\n        try:\n            encoding = tiktoken.encoding_for_model(model)\n            return len(encoding.encode(text))\n        except:\n            pass\n    return len(text) // 4\n\n\n# ============================================================\n# Approach 1: Claude Alone (Traditional - NO RAG)\n# ============================================================"}
{"id":108,"text":"approach1_claude_alone(question: str, repo: str):\n    \"\"\"\n    Claude without RAG:\n    - Extract keywords\n    - Grep files\n    - Read 5-10 FULL files\n    \"\"\"\n    repo_paths = {\n        'project': os.getenv('PROJECT_PATH', '/abs/path/to/project'),\n        'project': os.getenv('project_PATH', '/abs/path/to/project')\n    }\n\n    repo_path = repo_paths.get(repo)\n    if not repo_path or not os.path.exists(repo_path):\n        return {'error': f'Repo not found: {repo_path}'}\n\n    # Keywords from question\n    keywords = [w.lower() for w in question.split() if len(w) > 3][:5]\n\n    # Find files\n    matched_files = []\n    full_content = \"\"\n\n    for py_file in Path(repo_path).rglob('*.py'):\n        if any(skip in str(py_file) for skip in ['node_modules', '.venv', 'vendor', '.git', '__pycache__']):\n            continue\n\n        try:\n            content = py_file.read_text(errors='ignore')\n            if any(kw in content.lower() for kw in keywords):\n                matched_files.append(str(py_file))\n                full_content += f\"\\n{'='*70}\\nFile: {py_file}\\n{'='*70}\\n{content}\\n\"\n\n                if len(matched_files) >= 10:\n                    break\n        except:\n            pass\n\n    tokens = count_tokens(full_content)\n\n    return {\n        'method': '1. Claude Alone (no RAG)',\n        'description': 'Reads full files matching keywords',\n        'files_read': len(matched_files),\n        'tokens': tokens,\n        'sample_files': [Path(f).name for f in matched_files[:3]]\n    }\n\n\n# ============================================================\n# Approach 2: RAG CLI Standalone (no Claude)\n# ============================================================"}
{"id":109,"text":"approach2_rag_standalone(question: str, repo: str):\n    \"\"\"\n    RAG CLI standalone - full answer generation without Claude.\n    Counts the generated answer + citations.\n    \"\"\"\n    try:\n        from langgraph_app import build_graph\n\n        # Build graph and run (with required thread_id config)\n        graph = build_graph()\n        result = graph.invoke(\n            {\n                \"question\": question,\n                \"repo\": repo,\n            },\n            config={\"configurable\": {\"thread_id\": \"test-comparison\"}}\n        )\n\n        # What gets generated\n        answer_text = result.get('answer', '')\n        citations_text = '\\n'.join([\n            f\"{c.get('file_path', '')}:{c.get('start_line', '')}-{c.get('end_line', '')}\"\n            for c in result.get('citations', [])\n        ])\n\n        full_output = f\"Answer:\\n{answer_text}\\n\\nCitations:\\n{citations_text}\"\n        tokens = count_tokens(full_output)\n\n        return {\n            'method': '2. RAG CLI Standalone',\n            'description': 'RAG generates answer directly (no Claude)',\n            'tokens': tokens,\n            'answer_length': len(answer_text),\n            'citations_count': len(result.get('citations', []))\n        }\n    except Exception as e:\n        return {'error': str(e)}\n\n\n# ============================================================\n# Approach 3: Claude + RAG Direct (full chunks)\n# ============================================================"}
{"id":110,"text":"approach3_claude_plus_rag_direct(question: str, repo: str, top_k: int = 10):\n    \"\"\"\n    Claude gets full code chunks from RAG.\n    This is what would happen if Claude called hybrid_search directly.\n    \"\"\"\n    try:\n        from hybrid_search import search_routed_multi\n\n        results = search_routed_multi(question, repo_override=repo, final_k=top_k)\n\n        # Build what gets sent to Claude\n        context = \"Retrieved code chunks:\\n\\n\"\n        for r in results:\n            context += f\"File: {r['file_path']}:{r['start_line']}-{r['end_line']}\\n\"\n            context += f\"Score: {r['rerank_score']:.3f}\\n\"\n            context += f\"Code:\\n{r.get('code', '')}\\n\\n\"\n\n        tokens = count_tokens(context)\n\n        return {\n            'method': '3. Claude + RAG Direct',\n            'description': 'Claude gets full code chunks from RAG',\n            'chunks': len(results),\n            'tokens': tokens,\n            'files_touched': len(set(r['file_path'] for r in results))\n        }\n    except Exception as e:\n        return {'error': str(e)}\n\n\n# ============================================================\n# Approach 4: Claude + RAG via MCP (metadata only)\n# ============================================================"}
{"id":111,"text":"approach4_claude_plus_rag_mcp(question: str, repo: str, top_k: int = 10):\n    \"\"\"\n    Claude gets MCP tool response (metadata, no full code).\n    This is what I (Claude Code) actually receive.\n\n    IMPORTANT: MCP tool schemas are sent with EVERY request!\n    \"\"\"\n    try:\n        from mcp_server import MCPServer\n\n        server = MCPServer()\n\n        # Get tool schemas (sent with every request)\n        tools_req = {'jsonrpc': '2.0', 'id': 1, 'method': 'tools/list', 'params': {}}\n        tools_resp = server.handle_request(tools_req)\n        tools_json = json.dumps(tools_resp['result']['tools'])\n        schema_tokens = count_tokens(tools_json)\n\n        # Get the actual search response\n        search_req = {\n            'jsonrpc': '2.0',\n            'id': 1,\n            'method': 'tools/call',\n            'params': {\n                'name': 'rag_search',\n                'arguments': {\n                    'repo': repo,\n                    'question': question,\n                    'top_k': top_k\n                }\n            }\n        }\n\n        search_resp = server.handle_request(search_req)\n\n        # The MCP response is what Claude receives\n        mcp_response = search_resp['result']['content'][0]['text']\n        response_tokens = count_tokens(mcp_response)\n\n        # Total = schemas + response\n        total_tokens = schema_tokens + response_tokens\n\n        # Parse to get metadata\n        result_data = json.loads(mcp_response)\n\n        return {\n            'method': '4. Claude + RAG via MCP',\n            'description': 'Claude gets MCP metadata (paths + scores only) + tool schemas',\n            'chunks': result_data.get('count', 0),\n            'tokens': total_tokens,\n            'schema_tokens': schema_tokens,\n            'response_tokens': response_tokens,\n            'breakdown': f'{schema_tokens} (schemas) + {response_tokens} (response)'\n        }\n    except Exception as e:\n        return {'error': str(e)}\n\n\n# ============================================================\n# Run Comparison\n# ============================================================"}
{"id":112,"text":"run_comparison(question: str, repo: str):\n    \"\"\"Run all four approaches and compare\"\"\"\n    print(f\"\\n{'='*75}\")\n    print(f\"QUESTION: {question}\")\n    print(f\"REPO: {repo}\")\n    print(f\"{'='*75}\\n\")\n\n    results = []\n\n    # Run each approach\n    approaches = [\n        (\"Claude Alone\", approach1_claude_alone),\n        (\"RAG CLI Standalone\", approach2_rag_standalone),\n        (\"Claude + RAG Direct\", approach3_claude_plus_rag_direct),\n        (\"Claude + RAG via MCP\", approach4_claude_plus_rag_mcp),\n    ]\n\n    for name, func in approaches:\n        print(f\"â³ Testing: {name}...\")\n        result = func(question, repo)\n        results.append(result)\n\n    # Print results\n    print(f\"\\n{'='*75}\")\n    print(\"RESULTS (tokens sent to LLM):\")\n    print(f\"{'='*75}\\n\")\n\n    for i, result in enumerate(results, 1):\n        if 'error' in result:\n            print(f\"{i}. {result.get('method', 'Unknown')}: ERROR - {result['error']}\")\n            continue\n\n        print(f\"{i}. {result['method']}\")\n        print(f\"   {result['description']}\")\n        print(f\"   Tokens: {result['tokens']:,}\")\n\n        # Show method-specific details\n        if 'files_read' in result:\n            print(f\"   Files read: {result['files_read']}\")\n            if result.get('sample_files'):\n                print(f\"   Sample: {', '.join(result['sample_files'])}\")\n\n        if 'chunks' in result:\n            print(f\"   Chunks: {result['chunks']}\")\n\n        if 'files_touched' in result:\n            print(f\"   Files: {result['files_touched']}\")\n\n        if 'citations_count' in result:\n            print(f\"   Citations: {result['citations_count']}\")\n\n        if 'breakdown' in result:\n            print(f\"   Breakdown: {result['breakdown']}\")\n\n        print()\n\n    # Calculate savings\n    valid_results = [r for r in results if 'error' not in r and 'tokens' in r]\n\n    if len(valid_results) >= 2:\n        baseline = valid_results[0]['tokens']  # Claude alone\n\n        print(f\"{'='*75}\")\n        print(\"ðŸ’° SAVINGS vs Claude Alone:\")\n        print(f\"{'='*75}\\n\")\n\n        for result in valid_results[1:]:\n            tokens = result['tokens']\n            saved = baseline - tokens\n            pct = (saved / baseline * 100) if baseline > 0 else 0\n            reduction = baseline / tokens if tokens > 0 else 0\n\n            print(f\"{result['method']}:\")\n            print(f\"   Tokens saved: {saved:,}\")\n            print(f\"   Percentage: {pct:.1f}%\")\n            print(f\"   Reduction: {reduction:.1f}x\")\n\n            # Cost (gpt-4o: $2.50/1M input)\n            cost_saved = saved * (2.50 / 1_000_000)\n            print(f\"   $ saved/query: ${cost_saved:.6f}\")\n            print(f\"   $ saved/1000: ${cost_saved * 1000:.2f}\\n\")\n\n    return results\n\n\n# ============================================================\n# Main\n# ============================================================\n\nif __name__ == '__main__':\n    if not HAS_TIKTOKEN:\n        print(\"Installing tiktoken for accurate counts...\")\n        os.system(\"pip install -q tiktoken\")\n        try:\n            import tiktoken\n            HAS_TIKTOKEN = True\n            print(\"âœ“ tiktoken installed\\n\")\n        except:\n            print(\"âš ï¸  Using estimates (1 token â‰ˆ 4 chars)\\n\")\n\n    # Test cases\n    tests = [\n        (\"Where is OAuth token validated\", \"project\"),\n        (\"How are fax jobs created and dispatched\", \"project\"),\n    ]\n\n    all_results = []\n\n    for question, repo in tests:\n        try:\n            results = run_comparison(question, repo)\n            all_results.append({\n                'question': question,\n                'repo': repo,\n                'results': results\n            })\n        except Exception as e:\n            print(f\"\\nâŒ Error: {e}\\n\")\n\n    # Overall summary\n    if all_results:\n        print(f\"\\n{'='*75}\")\n        print(\"ðŸ“Š SUMMARY\")\n        print(f\"{'='*75}\\n\")\n\n        print(f\"Total queries tested: {len(all_results)}\\n\")\n\n        # Average by method\n        methods = ['Claude Alone', 'RAG CLI Standalone', 'Claude + RAG Direct', 'Claude + RAG via MCP']\n\n        for method in methods:\n            tokens = []\n            for test in all_results:\n                for r in test['results']:\n                    if r.get('method', '').startswith(method.split()[0]) and 'tokens' in r:\n                        tokens.append(r['tokens'])\n\n            if tokens:\n                avg = sum(tokens) / len(tokens)\n                print(f\"{method}: {avg:,.0f} avg tokens\")\n\n        print(f\"\\nðŸŽ¯ Recommendation:\")\n        print(f\"   Use MCP tools for maximum token efficiency\")\n        print(f\"   Use RAG CLI for standalone Q&A without Claude\")\n        print(f\"   Use Direct calls for custom integrations\")"}
{"id":113,"text":"#!/usr/bin/env python3\n\"\"\"\nInteractive quick setup to:\n  1) Add the current working directory as a repo (repos.json)\n  2) Optionally index it\n  3) Ensure venv + deps\n  4) Optionally start infra (Qdrant/Redis via docker compose)\n  5) Register MCP servers with Codex CLI and Claude Code\n\nRun this from the ROOT of the repo you want to index:\n  python /path/to/rag-service/scripts/quick_setup.py\n\nNotes:\n  - Never writes secrets without confirmation\n  - Creates timestamped backups of modified config files\n  - Uses Rich spinners/progress so users always see activity\n\"\"\"\nimport os\nimport sys\nimport json\nimport time\nimport platform\nimport shutil\nimport subprocess\nfrom pathlib import Path\n\ntry:\n    from rich.console import Console\n    from rich.panel import Panel\n    from rich.prompt import Confirm, Prompt\n    from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn\nexcept Exception:\n    print(\"This setup requires 'rich'. Install with: pip install rich\", file=sys.stderr)\n    sys.exit(1)\n\nconsole = Console()"}
{"id":114,"text":"write_repos_json(rag_root: Path, name: str, code_path: Path) -> Path:\n    p = os.getenv('REPOS_FILE') or str(rag_root / 'repos.json')\n    repos_path = Path(p)\n    cfg = {'default_repo': name, 'repos': []}\n    if repos_path.exists():\n        try:\n            cfg = json.loads(repos_path.read_text())\n            if not isinstance(cfg, dict):\n                cfg = {'default_repo': name, 'repos': []}\n        except Exception:\n            cfg = {'default_repo': name, 'repos': []}\n    # Update or append\n    repos = cfg.get('repos') or []\n    found = False\n    for r in repos:\n        if (r.get('name') or '').strip().lower() == name.lower():\n            r['path'] = str(code_path)\n            found = True\n            break\n    if not found:\n        repos.append({'name': name, 'path': str(code_path), 'keywords': [], 'path_boosts': [], 'layer_bonuses': {}})\n    cfg['repos'] = repos\n    # Ask to set default\n    if Confirm.ask(f\"Make [bold]{name}[/bold] the default repo?\", default=True):\n        cfg['default_repo'] = name\n    repos_path.write_text(json.dumps(cfg, indent=2))\n    return repos_path"}
{"id":115,"text":"_venv_python(repo_root: Path) -> Path:\n    if platform.system().lower().startswith('win'):\n        return repo_root / '.venv' / 'Scripts' / 'python.exe'\n    return repo_root / '.venv' / 'bin' / 'python'"}
{"id":116,"text":"ensure_venv_and_deps(rag_root: Path, progress: Progress, task_id) -> bool:\n    \"\"\"Create .venv and install deps if needed.\"\"\"\n    py = _venv_python(rag_root)\n    # Create venv if missing\n    if not py.exists():\n        progress.update(task_id, description='Creating virtualenv (.venv)')\n        try:\n            subprocess.check_call([sys.executable, '-m', 'venv', str(rag_root / '.venv')])\n        except subprocess.CalledProcessError as e:\n            console.print(f\"[red]Failed to create venv:[/red] {e}\")\n            return False\n    # Install deps\n    progress.update(task_id, description='Installing dependencies')\n    try:\n        reqs = [str(rag_root / 'requirements-rag.txt'), str(rag_root / 'requirements.txt')]\n        for req in reqs:\n            if Path(req).exists():\n                subprocess.check_call([str(py), '-m', 'pip', 'install', '--disable-pip-version-check', '-r', req])\n        # quick sanity imports\n        subprocess.check_call([str(py), '-c', 'import fastapi,qdrant_client,bm25s,langgraph;print(\"ok\")'])\n        return True\n    except subprocess.CalledProcessError as e:\n        console.print(f\"[red]Dependency install failed:[/red] {e}\")\n        return False"}
{"id":117,"text":"start_infra(rag_root: Path, progress: Progress, task_id) -> None:\n    progress.update(task_id, description='Starting Qdrant/Redis (docker compose)')\n    up = rag_root / 'scripts' / 'up.sh'\n    if not up.exists():\n        progress.update(task_id, description='Infra script not found (skipping)')\n        time.sleep(0.3)\n        return\n    try:\n        subprocess.check_call(['bash', str(up)])\n    except Exception as e:\n        console.print(f\"[yellow]Infra start skipped/failed:[/yellow] {e}\")\n    # quick qdrant ping\n    progress.update(task_id, description='Verifying Qdrant/Redis health')\n    try:\n        subprocess.check_call(['bash', '-lc', 'curl -s http://127.0.0.1:6333/collections >/dev/null || true'])\n    except Exception:\n        pass\n\ndetect_codex() -> str | None:\n    path = shutil.which('codex')\n    return path"}
{"id":118,"text":"codex_register(rag_root: Path, progress: Progress, task_id) -> None:\n    path = detect_codex()\n    if not path:\n        progress.update(task_id, description='Codex CLI not found (skip)')\n        time.sleep(0.3)\n        return\n    py = _venv_python(rag_root)\n    server = rag_root / 'mcp_server.py'\n    name = 'rag-service'\n    progress.update(task_id, description='Registering MCP with Codex')\n    try:\n        # remove existing silently\n        subprocess.run(['codex', 'mcp', 'remove', name], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n        subprocess.check_call(['codex', 'mcp', 'add', name, '--', str(py), str(server)])\n    except subprocess.CalledProcessError as e:\n        console.print(f\"[yellow]Codex registration failed:[/yellow] {e}\")"}
{"id":119,"text":"_claude_config_path() -> Path | None:\n    sysname = platform.system().lower()\n    home = Path.home()\n    if 'darwin' in sysname or 'mac' in sysname:\n        return (home / 'Library' / 'Application Support' / 'Claude' / 'claude_desktop_config.json')\n    if 'linux' in sysname:\n        return (home / '.config' / 'Claude' / 'claude_desktop_config.json')\n    if 'windows' in sysname or 'win' in sysname:\n        appdata = os.getenv('APPDATA')\n        if appdata:\n            return Path(appdata) / 'Claude' / 'claude_desktop_config.json'\n    return None"}
{"id":120,"text":"claude_register(rag_root: Path, progress: Progress, task_id) -> None:\n    cfgp = _claude_config_path()\n    if not cfgp:\n        progress.update(task_id, description='Claude config path not found (skip)')\n        time.sleep(0.3)\n        return\n    cfgp.parent.mkdir(parents=True, exist_ok=True)\n    py = _venv_python(rag_root)\n    server = rag_root / 'mcp_server.py'\n    # Load existing\n    data = {}\n    if cfgp.exists():\n        try:\n            data = json.loads(cfgp.read_text())\n        except Exception:\n            data = {}\n        # backup\n        bak = cfgp.with_suffix(cfgp.suffix + f'.bak.{time.strftime(\"%Y%m%d-%H%M%S\")}')\n        bak.write_text(json.dumps(data, indent=2))\n    # Merge entry\n    ms = data.get('mcpServers') or {}\n    ms['rag-service'] = {\n        'command': str(py),\n        'args': [str(server)],\n        'env': {\n            'OPENAI_API_KEY': os.getenv('OPENAI_API_KEY', '')\n        }\n    }\n    data['mcpServers'] = ms\n    progress.update(task_id, description='Writing Claude config')\n    cfgp.write_text(json.dumps(data, indent=2))"}
{"id":121,"text":"main():\n    rag_root = Path(__file__).resolve().parents[1]\n    # Allow explicit path override for code repo\n    forced_path = None\n    forced_name = None\n    argv = sys.argv[1:]\n    for i, a in enumerate(argv):\n        if a.startswith('--path='):\n            forced_path = a.split('=', 1)[1].strip()\n        elif a == '--path' and i+1 < len(argv):\n            forced_path = argv[i+1].strip()\n        elif a.startswith('--name='):\n            forced_name = a.split('=', 1)[1].strip()\n        elif a == '--name' and i+1 < len(argv):\n            forced_name = argv[i+1].strip()\n\n    code_root = Path(forced_path or os.getcwd()).resolve()\n    suggested = (forced_name or code_root.name.lower().replace(' ', '-').replace('_', '-'))\n    title = \"RAG Service â€” Quick Setup\"\n    msg = (\n        f\"Detected current directory:\\n[bold]{code_root}[/bold]\\n\\n\"\n        \"Create or update repos.json to include this path?\\n\"\n    )\n    console.print(Panel(msg, title=title, border_style=\"cyan\"))\n    if not Confirm.ask(\"Add this repo?\", default=True):\n        console.print(\"[yellow]Canceled.[/yellow]\")\n        return\n    name = forced_name or Prompt.ask(\"Repository name\", default=suggested)\n    repos_path = write_repos_json(rag_root, name, code_root)\n    console.print(f\"[green]âœ“[/green] Updated {repos_path}\")\n\n    # Offer to index\n    console.print(Panel(\n        \"Index now? This builds BM25 and embeddings; it may take time and bill your provider if configured.\",\n        title=\"Index Repository\", border_style=\"yellow\"\n    ))\n    do_index = Confirm.ask(\"Start indexing now?\", default=False)\n\n    console.print(Panel(\"Setup environment and agents?\", title=\"Agents & Infra\", border_style=\"cyan\"))\n    do_env = Confirm.ask(\"Ensure virtualenv + dependencies?\", default=True)\n    do_infra = Confirm.ask(\"Start Qdrant/Redis (docker compose)?\", default=True)\n    do_codex = Confirm.ask(\"Register Codex MCP?\", default=True)\n    do_claude = Confirm.ask(\"Register Claude MCP?\", default=True)\n\n    with Progress(\n        SpinnerColumn(style='cyan'),\n        TextColumn(\"{task.description}\"),\n        BarColumn(bar_width=None),\n        TimeElapsedColumn(),\n        transient=True,\n    ) as progress:\n        if do_env:\n            t = progress.add_task(\"Preparing environment\", total=None)\n            ok = ensure_venv_and_deps(rag_root, progress, t)\n            progress.remove_task(t)\n            if not ok:\n                console.print(\"[red]Environment setup failed; continuing without guarantees.[/red]\")\n        if do_infra:\n            t = progress.add_task(\"Starting infra\", total=None)\n            start_infra(rag_root, progress, t)\n            progress.remove_task(t)\n        if do_index:\n            t = progress.add_task(\"Indexing repository\", total=None)\n            env = os.environ.copy()\n            env['REPO'] = name\n            try:\n                subprocess.check_call([str(_venv_python(rag_root)), str(rag_root / 'index_repo.py')], env=env, cwd=str(rag_root))\n                console.print(f\"[green]âœ“[/green] Indexed repo: [bold]{name}[/bold]\")\n            except subprocess.CalledProcessError as e:\n                console.print(f\"[red]Indexing failed:[/red] {e}\")\n            progress.remove_task(t)\n        if do_codex:\n            t = progress.add_task(\"Registering Codex\", total=None)\n            codex_register(rag_root, progress, t)\n            progress.remove_task(t)\n        if do_claude:\n            t = progress.add_task(\"Registering Claude\", total=None)\n            claude_register(rag_root, progress, t)\n            progress.remove_task(t)\n\n    # Friendly next-steps banner\n    console.print(Panel(\n        \"Setup complete. Next steps:\\n\"\n        \" â€¢ Type 'codex' and try: Use rag_search to find OAuth in your repo\\n\"\n        f\" â€¢ Or run API: uvicorn serve_rag:app --host 127.0.0.1 --port 8012\\n\"\n        f\" â€¢ CLI streaming: python chat_cli.py --stream --api-url http://127.0.0.1:8012\\n\",\n        title=\"You're ready!\", border_style=\"green\"\n    ))\n\n\nif __name__ == '__main__':\n    main()"}
{"id":122,"text":"#!/usr/bin/env python3\n\"\"\"\nMeasure actual token savings from RAG vs traditional file reading.\nCompares targeted RAG retrieval against reading full files.\n\"\"\"\n\nimport sys\nimport os\nfrom pathlib import Path\nfrom hybrid_search import search_routed_multi\n\ntry:\n    import tiktoken\n    HAS_TIKTOKEN = True\nexcept ImportError:\n    HAS_TIKTOKEN = False\n    print(\"âš ï¸  tiktoken not installed - using rough estimates (1 token â‰ˆ 4 chars)\")\n    print(\"   Install with: pip install tiktoken\\n\")\n\ncount_tokens(text: str, model: str = \"gpt-4o\") -> int:\n    \"\"\"Count tokens precisely if tiktoken available, else estimate\"\"\"\n    if HAS_TIKTOKEN:\n        try:\n            encoding = tiktoken.encoding_for_model(model)\n            return len(encoding.encode(text))\n        except:\n            pass\n    # Fallback: rough estimate\n    return len(text) // 4"}
{"id":123,"text":"measure_rag_tokens(question: str, repo: str, top_k: int = 10):\n    \"\"\"Measure tokens using RAG hybrid search\"\"\"\n    results = search_routed_multi(question, repo_override=repo, final_k=top_k)\n\n    # Combine all retrieved code\n    combined_text = \"\"\n    for r in results:\n        combined_text += f\"File: {r['file_path']}:{r['start_line']}-{r['end_line']}\\n\"\n        combined_text += r.get('code', '') + \"\\n\\n\"\n\n    tokens = count_tokens(combined_text)\n\n    return {\n        'approach': 'RAG (hybrid search)',\n        'chunks': len(results),\n        'text': combined_text,\n        'tokens': tokens,\n        'files_touched': len(set(r['file_path'] for r in results))\n    }"}
{"id":124,"text":"measure_traditional_tokens(question: str, repo: str, max_files: int = 10):\n    \"\"\"\n    Simulate traditional approach: grep for keywords, read full files.\n    This is what you'd do WITHOUT RAG.\n    \"\"\"\n    repo_paths = {\n        'project': os.getenv('PROJECT_PATH', '/abs/path/to/project'),\n        'faxbot': os.getenv('FAXBOT_PATH', '/abs/path/to/faxbot')\n    }\n\n    repo_path = repo_paths.get(repo)\n    if not repo_path or not os.path.exists(repo_path):\n        return {'approach': 'Traditional', 'error': f'Repo not found: {repo_path}'}\n\n    # Extract keywords from question (simulate what a human would grep for)\n    keywords = [w.lower() for w in question.split() if len(w) > 3][:5]\n\n    # Find files containing keywords\n    combined_text = \"\"\n    matched_files = []\n\n    for py_file in Path(repo_path).rglob('*.py'):\n        if 'node_modules' in str(py_file) or '.venv' in str(py_file):\n            continue\n\n        try:\n            content = py_file.read_text(errors='ignore')\n\n            # If any keyword appears, a human would likely read this whole file\n            if any(kw in content.lower() for kw in keywords):\n                matched_files.append(str(py_file))\n                combined_text += f\"\\n{'='*60}\\nFile: {py_file}\\n{'='*60}\\n\"\n                combined_text += content + \"\\n\"\n\n                if len(matched_files) >= max_files:\n                    break\n        except Exception as e:\n            pass\n\n    tokens = count_tokens(combined_text)\n\n    return {\n        'approach': 'Traditional (grep + read full files)',\n        'files_read': len(matched_files),\n        'text': combined_text,\n        'tokens': tokens\n    }"}
{"id":125,"text":"run_comparison(question: str, repo: str):\n    \"\"\"Run both approaches and compare\"\"\"\n    print(f\"\\n{'='*70}\")\n    print(f\"Question: {question}\")\n    print(f\"Repository: {repo}\")\n    print(f\"{'='*70}\\n\")\n\n    # Measure RAG\n    print(\"â³ Running RAG hybrid search...\")\n    rag = measure_rag_tokens(question, repo, top_k=10)\n\n    # Measure traditional\n    print(\"â³ Simulating traditional grep + file reading...\")\n    trad = measure_traditional_tokens(question, repo, max_files=10)\n\n    # Print results\n    print(f\"\\n{'='*70}\")\n    print(\"ðŸ“Š RESULTS:\")\n    print(f\"{'='*70}\")\n\n    print(f\"\\nðŸ” RAG Approach:\")\n    print(f\"   Chunks retrieved: {rag['chunks']}\")\n    print(f\"   Files touched: {rag['files_touched']}\")\n    print(f\"   Total tokens: {rag['tokens']:,}\")\n\n    print(f\"\\nðŸ“ Traditional Approach (grep + read full files):\")\n    print(f\"   Files read: {trad['files_read']}\")\n    print(f\"   Total tokens: {trad['tokens']:,}\")\n\n    # Calculate savings\n    if trad['tokens'] > 0 and rag['tokens'] > 0:\n        saved = trad['tokens'] - rag['tokens']\n        saved_pct = (saved / trad['tokens']) * 100\n        reduction = trad['tokens'] / rag['tokens']\n\n        print(f\"\\n{'='*70}\")\n        print(\"ðŸ’° TOKEN SAVINGS:\")\n        print(f\"{'='*70}\")\n        print(f\"   Tokens saved: {saved:,} tokens\")\n        print(f\"   Percentage saved: {saved_pct:.1f}%\")\n        print(f\"   Reduction factor: {reduction:.1f}x smaller\")\n\n        # Cost estimate (rough: $15/1M input tokens for gpt-4o)\n        cost_per_token = 15 / 1_000_000\n        cost_saved = saved * cost_per_token\n        print(f\"   Cost saved per query: ${cost_saved:.6f}\")\n        print(f\"   Cost saved per 1000 queries: ${cost_saved * 1000:.2f}\")\n\n    return rag, trad\n\n\nif __name__ == '__main__':\n    # Test queries\n    test_cases = [\n        (\"Where is OAuth token validated\", \"project\"),\n        (\"How are fax jobs created and dispatched\", \"project\"),\n        (\"EventStream component event types\", \"project\"),\n        (\"provider health status implementation\", \"project\"),\n    ]\n\n    results = []\n\n    for question, repo in test_cases:\n        try:\n            rag, trad = run_comparison(question, repo)\n            results.append({\n                'question': question,\n                'repo': repo,\n                'rag_tokens': rag['tokens'],\n                'trad_tokens': trad['tokens'],\n                'savings': trad['tokens'] - rag['tokens']\n            })\n        except Exception as e:\n            print(f\"\\nâŒ Error testing '{question}': {e}\")\n\n    # Summary\n    if results:\n        print(f\"\\n\\n{'='*70}\")\n        print(\"ðŸ“ˆ OVERALL SUMMARY\")\n        print(f\"{'='*70}\")\n\n        total_rag = sum(r['rag_tokens'] for r in results)\n        total_trad = sum(r['trad_tokens'] for r in results)\n        total_saved = total_trad - total_rag\n\n        print(f\"\\nTotal queries tested: {len(results)}\")\n        print(f\"Total RAG tokens: {total_rag:,}\")\n        print(f\"Total traditional tokens: {total_trad:,}\")\n        print(f\"Total saved: {total_saved:,} tokens ({(total_saved/total_trad*100):.1f}%)\")\n        print(f\"Average reduction: {total_trad/total_rag:.1f}x\\n\")"}
{"id":126,"text":"#!/usr/bin/env python3\n\"\"\"\nMake a repos.json from simple CLI args.\n\nUsage examples:\n  python scripts/make_repos_json.py repo-a=/abs/path/a repo-b=/abs/path/b --default repo-a\n\nEnvironment fallbacks:\n  REPO and REPO_PATH if provided (single repo).\n\nBehavior:\n  - Writes repos.json in repo root (or REPOS_FILE location if set)\n  - If repos.json exists, writes a timestamped backup next to it\n\"\"\"\nimport os, sys, json, time\nfrom pathlib import Path\n\nparse_args(argv):\n    pairs = []\n    default_repo = None\n    for arg in argv:\n        if arg == '--help' or arg == '-h':\n            print(__doc__)\n            sys.exit(0)\n        if arg.startswith('--default='):\n            default_repo = arg.split('=',1)[1].strip()\n            continue\n        if arg == '--default':\n            # next token is default\n            # handled in caller for simplicity\n            continue\n        if '=' in arg:\n            name, path = arg.split('=',1)\n            name = name.strip()\n            path = path.strip()\n            if name and path:\n                pairs.append((name, path))\n    # Handle \"--default name\" form\n    if '--default' in argv:\n        i = argv.index('--default')\n        if i+1 < len(argv):\n            default_repo = argv[i+1].strip()\n    return pairs, default_repo"}
{"id":127,"text":"main():\n    args = sys.argv[1:]\n    pairs, default_repo = parse_args(args)\n\n    # Fallback to env for single-repo if no pairs passed\n    if not pairs:\n        env_repo = (os.getenv('REPO') or '').strip()\n        env_path = (os.getenv('REPO_PATH') or '').strip()\n        if env_repo and env_path:\n            pairs = [(env_repo, env_path)]\n            if not default_repo:\n                default_repo = env_repo\n        else:\n            print('No repo arguments provided and REPO/REPO_PATH not set. Example: repo-a=/abs/path/a')\n            sys.exit(2)\n\n    # Build config structure\n    repos = []\n    for name, path in pairs:\n        repos.append({\n            'name': name,\n            'path': str(Path(path).expanduser()),\n            'keywords': [],\n            'path_boosts': [],\n            'layer_bonuses': {}\n        })\n\n    if not default_repo:\n        default_repo = repos[0]['name']\n\n    cfg = {'default_repo': default_repo, 'repos': repos}\n\n    # Output path\n    out = os.getenv('REPOS_FILE') or str(Path(__file__).resolve().parents[1] / 'repos.json')\n    outp = Path(out)\n    outp_parent = outp.parent\n    outp_parent.mkdir(parents=True, exist_ok=True)\n\n    # Backup existing\n    if outp.exists():\n        ts = time.strftime('%Y%m%d-%H%M%S')\n        bak = outp.with_suffix(outp.suffix + f'.bak.{ts}')\n        bak.write_text(outp.read_text())\n        print(f'Backed up existing {outp} -> {bak}')\n\n    outp.write_text(json.dumps(cfg, indent=2))\n    print(f'Wrote {outp} with {len(repos)} repo(s); default_repo={default_repo}')\n\n\nif __name__ == '__main__':\n    main()"}
{"id":128,"text":"import os\nfrom hybrid_search import search_routed_multi\n\nTESTS = [\n    ('project','ai studio','easy'),\n    ('project','TBAC trait system','easy'),\n    ('project','plugin builder','easy'),\n    ('project','webhook verification','easy'),\n    ('project','three lane gateway','medium'),\n    ('project','plugin sandbox isolation','medium'),\n    ('project','provider adapter traits','medium'),\n    ('project','canonical event normalization','medium'),\n    ('project','how does TBAC prevent PHI access','hard'),\n    ('project','what is the general purpose of project','hard'),\n    ('project','how do different providers interact','hard'),\n]\n\nos.environ.setdefault('EMBEDDING_TYPE', 'local')\n\nby_diff = {}\nfor repo, q, d in TESTS:\n    docs = search_routed_multi(q, repo_override=repo, final_k=5)\n    s = (docs or [{}])[0].get('rerank_score', 0.0)\n    by_diff.setdefault(d, []).append(s)\n\nprint('\\n' + '='*80)\nprint('FINAL PERFORMANCE METRICS')\nprint('='*80)\n\nTARGET = {'easy':0.80, 'medium':0.70, 'hard':0.65}\nall_scores = []\nfor d, arr in by_diff.items():\n    avg = sum(arr)/max(1,len(arr))"}
{"id":129,"text":"all_scores.extend(arr)\n    status = 'âœ“' if avg >= TARGET[d] else 'âœ—'\n    print(f\"{status} {d.upper():7} | Avg: {avg:.3f} | Target: {TARGET[d]:.3f}\")\n\noverall = sum(all_scores)/max(1,len(all_scores))\nprint(f\"\\n{'Overall Average:':20} {overall:.3f}\")\nprint('='*80)"}
{"id":130,"text":"import json\nimport os\nfrom collections import Counter, defaultdict\nfrom pathlib import Path\nimport re\nextract_tokens(code):\n    \"\"\"Extract meaningful tokens from code\"\"\"\n    # Remove strings and comments\n    code = re.sub(r'[\"\\'].*?[\"\\']', '', code)\n    code = re.sub(r'#.*?\\n', '', code)\n    code = re.sub(r'//.*?\\n', '', code)\n    \n    # Extract identifiers (camelCase, snake_case, etc)\n    tokens = re.findall(r'\\b[a-zA-Z_][a-zA-Z0-9_]*\\b', code)\n    return [t.lower() for t in tokens if len(t) > 2]"}
{"id":131,"text":"analyze_repo(repo_path):\n    \"\"\"Analyze a repo for discriminative keywords\"\"\"\n    file_tokens = defaultdict(set)  # file -> set of tokens\n    global_counts = Counter()  # token -> total count\n    \n    for root, dirs, files in os.walk(repo_path):\n        # Skip common ignore patterns\n        dirs[:] = [d for d in dirs if d not in {'.git', 'node_modules', '__pycache__', '.venv', 'dist', 'build'}]\n        \n        for file in files:\n            if not any(file.endswith(ext) for ext in ['.py', '.js', '.ts', '.tsx', '.rb', '.java', '.go']):\n                continue\n                \n            file_path = os.path.join(root, file)\n            try:\n                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n                    code = f.read()\n                    tokens = extract_tokens(code)\n                    file_tokens[file_path].update(tokens)\n                    global_counts.update(tokens)\n            except:\n                continue\n    \n    # Calculate TF-IDF style scores\n    num_files = len(file_tokens)\n    doc_freq = Counter()  # how many files contain each token\n    \n    for tokens in file_tokens.values():\n        doc_freq.update(tokens)\n    \n    # Score = term frequency * inverse document frequency\n    keyword_scores = {}\n    for token, total_count in global_counts.items():\n        df = doc_freq[token]\n        idf = num_files / df if df > 0 else 0\n        \n        # High score = appears often but in few files (discriminative)\n        # Low score = appears everywhere (stop word) or rarely (noise)\n        if df > 1 and df < num_files * 0.05:  # in 2+ files but <5% of files\n            keyword_scores[token] = total_count * idf\n    \n    return keyword_scores, doc_freq, num_files"}
{"id":132,"text":"find_discriminative_keywords(repo_path, top_n=50):\n    \"\"\"Find the most discriminative keywords in a repo\"\"\"\n    keyword_scores, doc_freq, num_files = analyze_repo(repo_path)\n    \n    # Sort by score\n    sorted_keywords = sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)\n    \n    results = []\n    for token, score in sorted_keywords[:top_n]:\n        results.append({\n            'keyword': token,\n            'score': round(score, 2),\n            'appears_in_files': doc_freq[token],\n            'file_percentage': round(100 * doc_freq[token] / num_files, 1)\n        })\n    \n    return results\n\nif __name__ == '__main__':\n    repos = {\n        'project': os.getenv('PROJECT_PATH', '/abs/path/to/project'),\n        'project': os.getenv('project_PATH', '/abs/path/to/project')\n    }\n    \n    all_results = {}\n    \n    for repo_name, repo_path in repos.items():\n        print(f'\\n{\"=\"*80}')\n        print(f'ANALYZING: {repo_name}')\n        print(f'{\"=\"*80}')\n        \n        keywords = find_discriminative_keywords(repo_path, top_n=30)\n        all_results[repo_name] = keywords\n        \n        print(f'\\nTop 30 Discriminative Keywords (best for queries):\\n')\n        for i, kw in enumerate(keywords, 1):\n            print(f'{i:2}. {kw[\"keyword\"]:20} | Score: {kw[\"score\"]:8.1f} | In {kw[\"appears_in_files\"]:3} files ({kw[\"file_percentage\"]:4.1f}%)')\n    \n    # Find cross-contamination terms\n    print(f'\\n{\"=\"*80}')\n    print('CROSS-CONTAMINATION ANALYSIS')\n    print(f'{\"=\"*80}')\n    \n    viv_keywords = {k['keyword'] for k in all_results['project'][:30]}\n    fax_keywords = {k['keyword'] for k in all_results['project'][:30]}\n    \n    overlap = viv_keywords & fax_keywords\n    print(f'\\nShared keywords (cause confusion): {len(overlap)}')\n    if overlap:\n        print(f'  {\", \".join(sorted(overlap))}')\n    \n    print(f'\\nPROJECT-only keywords (use these!): {len(viv_keywords - fax_keywords)}')\n    print(f'  {\", \".join(sorted(list(viv_keywords - fax_keywords)[:10]))}')\n    \n    print(f'\\nPROJECT-only keywords (use these!): {len(fax_keywords - viv_keywords)}')\n    print(f'  {\", \".join(sorted(list(fax_keywords - viv_keywords)[:10]))}')\n    \n    # Save to file\n    with open('discriminative_keywords.json', 'w') as f:\n        json.dump(all_results, f, indent=2)\n    \n    print(f'\\nâœ“ Results saved to discriminative_keywords.json')"}
{"id":133,"text":"#!/usr/bin/env python3\n\"\"\"\nComplete, transparent comparison:\n- Qwen 3 vs OpenAI gpt-4o\n- Actual MCP tool schema overhead\n- Real latency measurements\n- Quality comparison\n\"\"\"\nimport sys\nimport os\nimport json\nimport time\nROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\nsys.path.insert(0, ROOT_DIR)\n\ntry:\n    import tiktoken\n    enc = tiktoken.encoding_for_model(\"gpt-4o\")\n    def count_tokens(text): return len(enc.encode(text))\nexcept:\n    def count_tokens(text): return len(text) // 4\n\nprint(\"=\" * 80)\nprint(\"COMPLETE MODEL COMPARISON - TRANSPARENT MEASUREMENTS\")\nprint(\"=\" * 80)\n\n# Test query\nquestion = \"How are fax jobs created and dispatched\"\nrepo = \"project\"\n\nprint(f\"\\nTest query: '{question}'\")\nprint(f\"Repo: {repo}\\n\")\n\n# ==================================================================\n# 1. MEASURE MCP TOOL SCHEMA OVERHEAD (sent on EVERY request)\n# ==================================================================\nprint(\"1. MCP Tool Schema Overhead\")\nprint(\"-\" * 80)\n\nfrom mcp_server import MCPServer"}
{"id":134,"text":"server = MCPServer()\ntools_req = {'jsonrpc': '2.0', 'id': 1, 'method': 'tools/list', 'params': {}}\ntools_resp = server.handle_request(tools_req)\ntools_json = json.dumps(tools_resp['result']['tools'])\nschema_tokens = count_tokens(tools_json)\n\nprint(f\"Tool schemas (sent with EVERY request): {schema_tokens:,} tokens\")\nprint(f\"Schema size: {len(tools_json):,} bytes\\n\")\n\n# ==================================================================\n# 2. MCP SEARCH RESPONSE SIZE\n# ==================================================================\nprint(\"2. MCP Search Response\")\nprint(\"-\" * 80)\n\nsearch_req = {\n    'jsonrpc': '2.0',\n    'id': 1,\n    'method': 'tools/call',\n    'params': {\n        'name': 'rag_search',\n        'arguments': {'repo': repo, 'question': question, 'top_k': 10}\n    }\n}\n\nstart = time.time()\nsearch_resp = server.handle_request(search_req)\nsearch_latency = time.time() - start\n\nmcp_response = search_resp['result']['content'][0]['text']\nresponse_tokens = count_tokens(mcp_response)\ntotal_mcp_tokens = schema_tokens + response_tokens"}
{"id":135,"text":"print(f\"Response tokens: {response_tokens:,}\")\nprint(f\"Total MCP tokens: {total_mcp_tokens:,} ({schema_tokens} schema + {response_tokens} response)\")\nprint(f\"Search latency: {search_latency:.2f}s\\n\")\n\n# ==================================================================\n# 3. QWEN 3 GENERATION\n# ==================================================================\nprint(\"3. Qwen 3 Generation (Local)\")\nprint(\"-\" * 80)\n\nos.environ[\"OLLAMA_URL\"] = \"http://127.0.0.1:11434/api\"\nos.environ[\"GEN_MODEL\"] = \"qwen3-coder:30b\"\n\nfrom env_model import generate_text\n\n# Parse MCP response to get context\nresult_data = json.loads(mcp_response)\ncontext = f\"Retrieved {result_data['count']} code locations:\\n\"\nfor r in result_data['results'][:5]:\n    context += f\"- {r['file_path']}:{r['start_line']}-{r['end_line']} (score: {r['rerank_score']:.3f})\\n\"\n\nprompt = f\"{context}\\n\\nQuestion: {question}\\nAnswer:\"\n\nstart = time.time()\nqwen_answer, _ = generate_text(prompt, model=\"qwen3-coder:30b\")\nqwen_latency = time.time() - start"}
{"id":136,"text":"qwen_output_tokens = count_tokens(qwen_answer)\nqwen_total_tokens = total_mcp_tokens + qwen_output_tokens\n\nprint(f\"Answer length: {len(qwen_answer)} chars\")\nprint(f\"Output tokens: {qwen_output_tokens:,}\")\nprint(f\"Total tokens (MCP + generation): {qwen_total_tokens:,}\")\nprint(f\"Generation latency: {qwen_latency:.2f}s\")\nprint(f\"Cost: $0.00 (local)\")\nprint(f\"\\nAnswer preview: {qwen_answer[:200]}...\\n\")\n\n# ==================================================================\n# 4. OPENAI GPT-4O GENERATION\n# ==================================================================\nprint(\"4. OpenAI gpt-4o Generation (API)\")\nprint(\"-\" * 80)\n\n# Use OpenAI for generation\nfrom openai import OpenAI\nclient = OpenAI()\n\nstart = time.time()\ntry:\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        max_tokens=500\n    )\n    openai_answer = response.choices[0].message.content\n    openai_latency = time.time() - start\n    \n    openai_output_tokens = count_tokens(openai_answer)"}
{"id":137,"text":"openai_total_tokens = total_mcp_tokens + openai_output_tokens\n    \n    # gpt-4o pricing (as of Oct 2025): $2.50/1M input, $10/1M output\n    input_cost = total_mcp_tokens * (2.50 / 1_000_000)\n    output_cost = openai_output_tokens * (10.00 / 1_000_000)\n    total_cost = input_cost + output_cost\n    \n    print(f\"Answer length: {len(openai_answer)} chars\")\n    print(f\"Output tokens: {openai_output_tokens:,}\")\n    print(f\"Total tokens (MCP + generation): {openai_total_tokens:,}\")\n    print(f\"Generation latency: {openai_latency:.2f}s\")\n    print(f\"Cost: ${total_cost:.6f} (${input_cost:.6f} input + ${output_cost:.6f} output)\")\n    print(f\"\\nAnswer preview: {openai_answer[:200]}...\\n\")\nexcept Exception as e:\n    print(f\"ERROR: {e}\\n\")\n    openai_answer = None\n\n# ==================================================================\n# 5. COMPARISON TABLE\n# ==================================================================\nprint(\"=\" * 80)\nprint(\"SUMMARY COMPARISON\")\nprint(\"=\" * 80)\n\nprint(\"\\nTOKEN BREAKDOWN:\")\nprint(f\"  MCP tool schemas:     {schema_tokens:,} tokens (sent on EVERY request)\")"}
{"id":138,"text":"print(f\"  MCP search response:  {response_tokens:,} tokens\")\nprint(f\"  Qwen 3 output:        {qwen_output_tokens:,} tokens\")\nif openai_answer:\n    print(f\"  gpt-4o output:        {openai_output_tokens:,} tokens\")\n\nprint(f\"\\nTOTAL TOKENS:\")\nprint(f\"  Qwen 3:   {qwen_total_tokens:,} tokens\")\nif openai_answer:\n    print(f\"  gpt-4o:   {openai_total_tokens:,} tokens\")\n\nprint(f\"\\nLATENCY:\")\nprint(f\"  MCP search:       {search_latency:.2f}s\")\nprint(f\"  Qwen 3 generate:  {qwen_latency:.2f}s\")\nif openai_answer:\n    print(f\"  gpt-4o generate:  {openai_latency:.2f}s\")\n\nprint(f\"\\nCOST PER QUERY:\")\nprint(f\"  Qwen 3:   $0.00 (local)\")\nif openai_answer:\n    print(f\"  gpt-4o:   ${total_cost:.6f}\")\n\nprint(f\"\\nANSWER QUALITY:\")\nprint(f\"  Qwen 3:   {len(qwen_answer)} chars - {qwen_answer[:100]}...\")\nif openai_answer:\n    print(f\"  gpt-4o:   {len(openai_answer)} chars - {openai_answer[:100]}...\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(f\"SAVED TO: /tmp/full_comparison_results.json\")\nprint(\"=\" * 80)\n\n# Save results\nresults = {\n    \"query\": question,\n    \"repo\": repo,\n    \"mcp\": {\n        \"schema_tokens\": schema_tokens,"}
{"id":139,"text":"\"response_tokens\": response_tokens,\n        \"total_tokens\": total_mcp_tokens,\n        \"latency_s\": search_latency\n    },\n    \"qwen3\": {\n        \"output_tokens\": qwen_output_tokens,\n        \"total_tokens\": qwen_total_tokens,\n        \"latency_s\": qwen_latency,\n        \"cost_usd\": 0.0,\n        \"answer\": qwen_answer\n    }\n}\n\nif openai_answer:\n    results[\"gpt4o\"] = {\n        \"output_tokens\": openai_output_tokens,\n        \"total_tokens\": openai_total_tokens,\n        \"latency_s\": openai_latency,\n        \"cost_usd\": total_cost,\n        \"answer\": openai_answer\n    }\n\nwith open('/tmp/full_comparison_results.json', 'w') as f:\n    json.dump(results, f, indent=2)\n\nprint(\"\\nDone!\")"}
